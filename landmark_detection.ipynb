{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(0, 55, 207); padding: 30px; border-radius: 20px; box-shadow: 0 4px 15px rgba(105, 195, 255, 0.3); color:rgb(187, 201, 248); font-family: 'Times New Roman', serif;\">\n",
    "\n",
    "<h1 style=\"text-align: center; font-size: 38px; color: white; font-weight: bold;\">Creating Numpy Files of Extracted Landmarks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Files Processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "DATA_DIR = Path(r\"YOUR_DATASET\")\n",
    "OUTPUT_DIR = Path(r\"OUTPUT\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_hand_keypoints(results):\n",
    "    # Get left and right hand landmarks\n",
    "    lh = np.array([[res.x, res.y, res.z] \n",
    "                   for res in results.multi_hand_landmarks[0].landmark]) if results.multi_hand_landmarks and len(results.multi_hand_landmarks) > 0 else np.zeros(21*3)\n",
    "    \n",
    "    rh = np.array([[res.x, res.y, res.z] \n",
    "                   for res in results.multi_hand_landmarks[1].landmark]) if results.multi_hand_landmarks and len(results.multi_hand_landmarks) > 1 else np.zeros(21*3)\n",
    "    \n",
    "    return np.concatenate([lh, rh])\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    landmarks_seq = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            keypoints = extract_hand_keypoints(results)\n",
    "            landmarks_seq.append(keypoints)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(landmarks_seq, dtype=np.float32)\n",
    "\n",
    "# Process each video\n",
    "for video_file in tqdm(list(DATA_DIR.rglob(\"*.mp4\"))):\n",
    "    relative_path = video_file.relative_to(DATA_DIR)\n",
    "    output_path = OUTPUT_DIR / relative_path.with_suffix(\".npy\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    landmarks_array = process_video(video_file)\n",
    "    np.save(output_path, landmarks_array)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
