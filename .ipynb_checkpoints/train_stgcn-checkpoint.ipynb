{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447eb207",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyYAML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a06003",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --force-reinstall scikit-video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61c290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/mediapipe_asl/train_label.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "# If it's a tuple (label list, path list), unpack:\n",
    "label_list = labels[0] if isinstance(labels, tuple) else labels\n",
    "num_classes = len(set(label_list))\n",
    "print(f\"âœ… Your dataset contains {num_classes} unique classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59233d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from feeder.feeder import Feeder\n",
    "import yaml\n",
    "\n",
    "# Load config\n",
    "with open('config/st_gcn/mediapipe-asl.yaml', 'r', encoding='utf-8') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Create dataset\n",
    "dataset = Feeder(**cfg['train_feeder_args'])\n",
    "loader = DataLoader(dataset, batch_size=cfg['train_batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"ğŸ§ª Dataset contains {len(dataset)} samples\")\n",
    "print(f\"ğŸ“¦ Each batch contains {cfg['train_batch_size']} samples\")\n",
    "print(f\"ğŸ” Total batches = {len(loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab4607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "740ed4f5",
   "metadata": {},
   "source": [
    "### Simplified STGN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa58c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor start\n",
      "Loading data...\n",
      "Loading model...\n",
      "Starting training for 80 epochs\n",
      "\n",
      "ğŸŒ€ Epoch 1/80\n",
      "[MODEL INIT] Linear input features: 102627\n",
      "  ğŸ” Batch 1/90  |  Loss: 6.3847\n",
      "  ğŸ” Batch 2/90  |  Loss: 56.2589\n",
      "  ğŸ” Batch 3/90  |  Loss: 65.3429\n",
      "  ğŸ” Batch 4/90  |  Loss: 95.7490\n",
      "  ğŸ” Batch 5/90  |  Loss: 74.7355\n",
      "  ğŸ” Batch 6/90  |  Loss: 88.8566\n",
      "  ğŸ” Batch 7/90  |  Loss: 116.0466\n",
      "  ğŸ” Batch 8/90  |  Loss: 124.0181\n",
      "  ğŸ” Batch 9/90  |  Loss: 66.8916\n",
      "  ğŸ” Batch 10/90  |  Loss: 78.7280\n",
      "  ğŸ” Batch 11/90  |  Loss: 116.0327\n",
      "  ğŸ” Batch 12/90  |  Loss: 78.3762\n",
      "  ğŸ” Batch 13/90  |  Loss: 85.5617\n",
      "  ğŸ” Batch 14/90  |  Loss: 97.9196\n",
      "  ğŸ” Batch 15/90  |  Loss: 136.9725\n",
      "  ğŸ” Batch 16/90  |  Loss: 131.2691\n",
      "  ğŸ” Batch 17/90  |  Loss: 124.6676\n",
      "  ğŸ” Batch 18/90  |  Loss: 133.8821\n",
      "  ğŸ” Batch 19/90  |  Loss: 101.9443\n",
      "  ğŸ” Batch 20/90  |  Loss: 83.8803\n",
      "  ğŸ” Batch 21/90  |  Loss: 98.3145\n",
      "  ğŸ” Batch 22/90  |  Loss: 103.5148\n",
      "  ğŸ” Batch 23/90  |  Loss: 120.1026\n",
      "  ğŸ” Batch 24/90  |  Loss: 94.6147\n",
      "  ğŸ” Batch 25/90  |  Loss: 76.7716\n",
      "  ğŸ” Batch 26/90  |  Loss: 68.1526\n",
      "  ğŸ” Batch 27/90  |  Loss: 70.1446\n",
      "  ğŸ” Batch 28/90  |  Loss: 125.3097\n",
      "  ğŸ” Batch 29/90  |  Loss: 89.4748\n",
      "  ğŸ” Batch 30/90  |  Loss: 46.9831\n",
      "  ğŸ” Batch 31/90  |  Loss: 73.7710\n",
      "  ğŸ” Batch 32/90  |  Loss: 76.1359\n",
      "  ğŸ” Batch 33/90  |  Loss: 79.0847\n",
      "  ğŸ” Batch 34/90  |  Loss: 69.6339\n",
      "  ğŸ” Batch 35/90  |  Loss: 68.1792\n",
      "  ğŸ” Batch 36/90  |  Loss: 77.8325\n",
      "  ğŸ” Batch 37/90  |  Loss: 112.3386\n",
      "  ğŸ” Batch 38/90  |  Loss: 73.6290\n",
      "  ğŸ” Batch 39/90  |  Loss: 123.8555\n",
      "  ğŸ” Batch 40/90  |  Loss: 69.1409\n",
      "  ğŸ” Batch 41/90  |  Loss: 85.6891\n",
      "  ğŸ” Batch 42/90  |  Loss: 64.3340\n",
      "  ğŸ” Batch 43/90  |  Loss: 53.2170\n",
      "  ğŸ” Batch 44/90  |  Loss: 53.5393\n",
      "  ğŸ” Batch 45/90  |  Loss: 65.0503\n",
      "  ğŸ” Batch 46/90  |  Loss: 54.8119\n",
      "  ğŸ” Batch 47/90  |  Loss: 70.7426\n",
      "  ğŸ” Batch 48/90  |  Loss: 57.8138\n",
      "  ğŸ” Batch 49/90  |  Loss: 81.6613\n",
      "  ğŸ” Batch 50/90  |  Loss: 70.5949\n",
      "  ğŸ” Batch 51/90  |  Loss: 79.4429\n",
      "  ğŸ” Batch 52/90  |  Loss: 76.2078\n",
      "  ğŸ” Batch 53/90  |  Loss: 57.2064\n",
      "  ğŸ” Batch 54/90  |  Loss: 99.5830\n",
      "  ğŸ” Batch 55/90  |  Loss: 102.8874\n",
      "  ğŸ” Batch 56/90  |  Loss: 88.1837\n",
      "  ğŸ” Batch 57/90  |  Loss: 95.8688\n",
      "  ğŸ” Batch 58/90  |  Loss: 57.4107\n",
      "  ğŸ” Batch 59/90  |  Loss: 72.6043\n",
      "  ğŸ” Batch 60/90  |  Loss: 104.7506\n",
      "  ğŸ” Batch 61/90  |  Loss: 38.7378\n",
      "  ğŸ” Batch 62/90  |  Loss: 68.1220\n",
      "  ğŸ” Batch 63/90  |  Loss: 46.0439\n",
      "  ğŸ” Batch 64/90  |  Loss: 59.6645\n",
      "  ğŸ” Batch 65/90  |  Loss: 55.8813\n",
      "  ğŸ” Batch 66/90  |  Loss: 45.7701\n",
      "  ğŸ” Batch 67/90  |  Loss: 72.4014\n",
      "  ğŸ” Batch 68/90  |  Loss: 64.7411\n",
      "  ğŸ” Batch 69/90  |  Loss: 81.1237\n",
      "  ğŸ” Batch 70/90  |  Loss: 62.3674\n",
      "  ğŸ” Batch 71/90  |  Loss: 57.9331\n",
      "  ğŸ” Batch 72/90  |  Loss: 73.4564\n",
      "  ğŸ” Batch 73/90  |  Loss: 48.8637\n",
      "  ğŸ” Batch 74/90  |  Loss: 43.3717\n",
      "  ğŸ” Batch 75/90  |  Loss: 53.0554\n",
      "  ğŸ” Batch 76/90  |  Loss: 93.4109\n",
      "  ğŸ” Batch 77/90  |  Loss: 44.1256\n",
      "  ğŸ” Batch 78/90  |  Loss: 50.2644\n",
      "  ğŸ” Batch 79/90  |  Loss: 49.4861\n",
      "  ğŸ” Batch 80/90  |  Loss: 35.2910\n",
      "  ğŸ” Batch 81/90  |  Loss: 37.4941\n",
      "  ğŸ” Batch 82/90  |  Loss: 42.5647\n",
      "  ğŸ” Batch 83/90  |  Loss: 55.6263\n",
      "  ğŸ” Batch 84/90  |  Loss: 44.4325\n",
      "  ğŸ” Batch 85/90  |  Loss: 56.1223\n",
      "  ğŸ” Batch 86/90  |  Loss: 43.0294\n",
      "  ğŸ” Batch 87/90  |  Loss: 52.2384\n",
      "  ğŸ” Batch 88/90  |  Loss: 38.9033\n",
      "  ğŸ” Batch 89/90  |  Loss: 58.2979\n",
      "  ğŸ” Batch 90/90  |  Loss: 49.6383\n",
      "\n",
      "ğŸŒ€ Epoch 2/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 60.0483\n",
      "  ğŸ” Batch 2/90  |  Loss: 36.2958\n",
      "  ğŸ” Batch 3/90  |  Loss: 42.6767\n",
      "  ğŸ” Batch 4/90  |  Loss: 22.9888\n",
      "  ğŸ” Batch 5/90  |  Loss: 54.2416\n",
      "  ğŸ” Batch 6/90  |  Loss: 39.2966\n",
      "  ğŸ” Batch 7/90  |  Loss: 23.8498\n",
      "  ğŸ” Batch 8/90  |  Loss: 45.4693\n",
      "  ğŸ” Batch 9/90  |  Loss: 39.3298\n",
      "  ğŸ” Batch 10/90  |  Loss: 24.7966\n",
      "  ğŸ” Batch 11/90  |  Loss: 32.1818\n",
      "  ğŸ” Batch 12/90  |  Loss: 27.0415\n",
      "  ğŸ” Batch 13/90  |  Loss: 55.9643\n",
      "  ğŸ” Batch 14/90  |  Loss: 33.2580\n",
      "  ğŸ” Batch 15/90  |  Loss: 31.4984\n",
      "  ğŸ” Batch 16/90  |  Loss: 36.2609\n",
      "  ğŸ” Batch 17/90  |  Loss: 36.8565\n",
      "  ğŸ” Batch 18/90  |  Loss: 40.6841\n",
      "  ğŸ” Batch 19/90  |  Loss: 33.4845\n",
      "  ğŸ” Batch 20/90  |  Loss: 19.5013\n",
      "  ğŸ” Batch 21/90  |  Loss: 51.0622\n",
      "  ğŸ” Batch 22/90  |  Loss: 41.0960\n",
      "  ğŸ” Batch 23/90  |  Loss: 50.0084\n",
      "  ğŸ” Batch 24/90  |  Loss: 28.5418\n",
      "  ğŸ” Batch 25/90  |  Loss: 33.7576\n",
      "  ğŸ” Batch 26/90  |  Loss: 59.4284\n",
      "  ğŸ” Batch 27/90  |  Loss: 37.0285\n",
      "  ğŸ” Batch 28/90  |  Loss: 36.5147\n",
      "  ğŸ” Batch 29/90  |  Loss: 37.7556\n",
      "  ğŸ” Batch 30/90  |  Loss: 30.6520\n",
      "  ğŸ” Batch 31/90  |  Loss: 39.5661\n",
      "  ğŸ” Batch 32/90  |  Loss: 48.7996\n",
      "  ğŸ” Batch 33/90  |  Loss: 59.3661\n",
      "  ğŸ” Batch 34/90  |  Loss: 30.2593\n",
      "  ğŸ” Batch 35/90  |  Loss: 41.5667\n",
      "  ğŸ” Batch 36/90  |  Loss: 42.7747\n",
      "  ğŸ” Batch 37/90  |  Loss: 43.0172\n",
      "  ğŸ” Batch 38/90  |  Loss: 23.1697\n",
      "  ğŸ” Batch 39/90  |  Loss: 52.7437\n",
      "  ğŸ” Batch 40/90  |  Loss: 36.5896\n",
      "  ğŸ” Batch 41/90  |  Loss: 42.8876\n",
      "  ğŸ” Batch 42/90  |  Loss: 46.6993\n",
      "  ğŸ” Batch 43/90  |  Loss: 57.1209\n",
      "  ğŸ” Batch 44/90  |  Loss: 51.7680\n",
      "  ğŸ” Batch 45/90  |  Loss: 38.2619\n",
      "  ğŸ” Batch 46/90  |  Loss: 37.9600\n",
      "  ğŸ” Batch 47/90  |  Loss: 60.6696\n",
      "  ğŸ” Batch 48/90  |  Loss: 27.2564\n",
      "  ğŸ” Batch 49/90  |  Loss: 33.0466\n",
      "  ğŸ” Batch 50/90  |  Loss: 48.9007\n",
      "  ğŸ” Batch 51/90  |  Loss: 61.0328\n",
      "  ğŸ” Batch 52/90  |  Loss: 44.4983\n",
      "  ğŸ” Batch 53/90  |  Loss: 37.4730\n",
      "  ğŸ” Batch 54/90  |  Loss: 34.7043\n",
      "  ğŸ” Batch 55/90  |  Loss: 35.8334\n",
      "  ğŸ” Batch 56/90  |  Loss: 30.8918\n",
      "  ğŸ” Batch 57/90  |  Loss: 45.4868\n",
      "  ğŸ” Batch 58/90  |  Loss: 35.1819\n",
      "  ğŸ” Batch 59/90  |  Loss: 51.3103\n",
      "  ğŸ” Batch 60/90  |  Loss: 33.6631\n",
      "  ğŸ” Batch 61/90  |  Loss: 38.5162\n",
      "  ğŸ” Batch 62/90  |  Loss: 42.9213\n",
      "  ğŸ” Batch 63/90  |  Loss: 57.6734\n",
      "  ğŸ” Batch 64/90  |  Loss: 39.0222\n",
      "  ğŸ” Batch 65/90  |  Loss: 35.8350\n",
      "  ğŸ” Batch 66/90  |  Loss: 31.9857\n",
      "  ğŸ” Batch 67/90  |  Loss: 26.7189\n",
      "  ğŸ” Batch 68/90  |  Loss: 31.0867\n",
      "  ğŸ” Batch 69/90  |  Loss: 24.3404\n",
      "  ğŸ” Batch 70/90  |  Loss: 35.3675\n",
      "  ğŸ” Batch 71/90  |  Loss: 44.8145\n",
      "  ğŸ” Batch 72/90  |  Loss: 29.4866\n",
      "  ğŸ” Batch 73/90  |  Loss: 56.2352\n",
      "  ğŸ” Batch 74/90  |  Loss: 30.4318\n",
      "  ğŸ” Batch 75/90  |  Loss: 33.5565\n",
      "  ğŸ” Batch 76/90  |  Loss: 22.0162\n",
      "  ğŸ” Batch 77/90  |  Loss: 29.8924\n",
      "  ğŸ” Batch 78/90  |  Loss: 49.6475\n",
      "  ğŸ” Batch 79/90  |  Loss: 18.8999\n",
      "  ğŸ” Batch 80/90  |  Loss: 36.8093\n",
      "  ğŸ” Batch 81/90  |  Loss: 31.6368\n",
      "  ğŸ” Batch 82/90  |  Loss: 48.1923\n",
      "  ğŸ” Batch 83/90  |  Loss: 27.2375\n",
      "  ğŸ” Batch 84/90  |  Loss: 68.8554\n",
      "  ğŸ” Batch 85/90  |  Loss: 32.8157\n",
      "  ğŸ” Batch 86/90  |  Loss: 39.3605\n",
      "  ğŸ” Batch 87/90  |  Loss: 53.6897\n",
      "  ğŸ” Batch 88/90  |  Loss: 50.2013\n",
      "  ğŸ” Batch 89/90  |  Loss: 36.1662\n",
      "  ğŸ” Batch 90/90  |  Loss: 71.5385\n",
      "\n",
      "ğŸŒ€ Epoch 3/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 45.9652\n",
      "  ğŸ” Batch 2/90  |  Loss: 38.4066\n",
      "  ğŸ” Batch 3/90  |  Loss: 32.3397\n",
      "  ğŸ” Batch 4/90  |  Loss: 38.9247\n",
      "  ğŸ” Batch 5/90  |  Loss: 63.5019\n",
      "  ğŸ” Batch 6/90  |  Loss: 40.3762\n",
      "  ğŸ” Batch 7/90  |  Loss: 17.4308\n",
      "  ğŸ” Batch 8/90  |  Loss: 51.2879\n",
      "  ğŸ” Batch 9/90  |  Loss: 28.5705\n",
      "  ğŸ” Batch 10/90  |  Loss: 33.4522\n",
      "  ğŸ” Batch 11/90  |  Loss: 24.1137\n",
      "  ğŸ” Batch 12/90  |  Loss: 25.1805\n",
      "  ğŸ” Batch 13/90  |  Loss: 40.5646\n",
      "  ğŸ” Batch 14/90  |  Loss: 46.6518\n",
      "  ğŸ” Batch 15/90  |  Loss: 27.8459\n",
      "  ğŸ” Batch 16/90  |  Loss: 44.8646\n",
      "  ğŸ” Batch 17/90  |  Loss: 42.1767\n",
      "  ğŸ” Batch 18/90  |  Loss: 44.6491\n",
      "  ğŸ” Batch 19/90  |  Loss: 38.8164\n",
      "  ğŸ” Batch 20/90  |  Loss: 43.8373\n",
      "  ğŸ” Batch 21/90  |  Loss: 29.0263\n",
      "  ğŸ” Batch 22/90  |  Loss: 53.0528\n",
      "  ğŸ” Batch 23/90  |  Loss: 52.7735\n",
      "  ğŸ” Batch 24/90  |  Loss: 34.5112\n",
      "  ğŸ” Batch 25/90  |  Loss: 52.6752\n",
      "  ğŸ” Batch 26/90  |  Loss: 61.3980\n",
      "  ğŸ” Batch 27/90  |  Loss: 44.2585\n",
      "  ğŸ” Batch 28/90  |  Loss: 50.6451\n",
      "  ğŸ” Batch 29/90  |  Loss: 35.2551\n",
      "  ğŸ” Batch 30/90  |  Loss: 31.2977\n",
      "  ğŸ” Batch 31/90  |  Loss: 43.8032\n",
      "  ğŸ” Batch 32/90  |  Loss: 53.5086\n",
      "  ğŸ” Batch 33/90  |  Loss: 41.9387\n",
      "  ğŸ” Batch 34/90  |  Loss: 40.7440\n",
      "  ğŸ” Batch 35/90  |  Loss: 44.5153\n",
      "  ğŸ” Batch 36/90  |  Loss: 42.0482\n",
      "  ğŸ” Batch 37/90  |  Loss: 33.9009\n",
      "  ğŸ” Batch 38/90  |  Loss: 48.4503\n",
      "  ğŸ” Batch 39/90  |  Loss: 39.6926\n",
      "  ğŸ” Batch 40/90  |  Loss: 43.4839\n",
      "  ğŸ” Batch 41/90  |  Loss: 41.0635\n",
      "  ğŸ” Batch 42/90  |  Loss: 33.8616\n",
      "  ğŸ” Batch 43/90  |  Loss: 23.1730\n",
      "  ğŸ” Batch 44/90  |  Loss: 15.8150\n",
      "  ğŸ” Batch 45/90  |  Loss: 50.0167\n",
      "  ğŸ” Batch 46/90  |  Loss: 65.2949\n",
      "  ğŸ” Batch 47/90  |  Loss: 39.5828\n",
      "  ğŸ” Batch 48/90  |  Loss: 55.5728\n",
      "  ğŸ” Batch 49/90  |  Loss: 27.7213\n",
      "  ğŸ” Batch 50/90  |  Loss: 35.0200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 51/90  |  Loss: 26.1541\n",
      "  ğŸ” Batch 52/90  |  Loss: 41.6015\n",
      "  ğŸ” Batch 53/90  |  Loss: 20.0602\n",
      "  ğŸ” Batch 54/90  |  Loss: 32.7555\n",
      "  ğŸ” Batch 55/90  |  Loss: 56.4931\n",
      "  ğŸ” Batch 56/90  |  Loss: 48.3358\n",
      "  ğŸ” Batch 57/90  |  Loss: 36.7938\n",
      "  ğŸ” Batch 58/90  |  Loss: 30.9270\n",
      "  ğŸ” Batch 59/90  |  Loss: 25.3308\n",
      "  ğŸ” Batch 60/90  |  Loss: 9.2314\n",
      "  ğŸ” Batch 61/90  |  Loss: 29.3538\n",
      "  ğŸ” Batch 62/90  |  Loss: 61.3245\n",
      "  ğŸ” Batch 63/90  |  Loss: 46.2730\n",
      "  ğŸ” Batch 64/90  |  Loss: 59.8913\n",
      "  ğŸ” Batch 65/90  |  Loss: 46.5222\n",
      "  ğŸ” Batch 66/90  |  Loss: 73.9245\n",
      "  ğŸ” Batch 67/90  |  Loss: 51.4826\n",
      "  ğŸ” Batch 68/90  |  Loss: 49.9511\n",
      "  ğŸ” Batch 69/90  |  Loss: 37.3795\n",
      "  ğŸ” Batch 70/90  |  Loss: 62.7362\n",
      "  ğŸ” Batch 71/90  |  Loss: 22.1856\n",
      "  ğŸ” Batch 72/90  |  Loss: 27.6570\n",
      "  ğŸ” Batch 73/90  |  Loss: 25.8884\n",
      "  ğŸ” Batch 74/90  |  Loss: 41.6294\n",
      "  ğŸ” Batch 75/90  |  Loss: 41.9400\n",
      "  ğŸ” Batch 76/90  |  Loss: 58.7232\n",
      "  ğŸ” Batch 77/90  |  Loss: 59.6149\n",
      "  ğŸ” Batch 78/90  |  Loss: 36.6585\n",
      "  ğŸ” Batch 79/90  |  Loss: 42.3132\n",
      "  ğŸ” Batch 80/90  |  Loss: 44.6247\n",
      "  ğŸ” Batch 81/90  |  Loss: 32.0918\n",
      "  ğŸ” Batch 82/90  |  Loss: 36.5063\n",
      "  ğŸ” Batch 83/90  |  Loss: 38.8304\n",
      "  ğŸ” Batch 84/90  |  Loss: 35.9966\n",
      "  ğŸ” Batch 85/90  |  Loss: 56.3665\n",
      "  ğŸ” Batch 86/90  |  Loss: 28.4821\n",
      "  ğŸ” Batch 87/90  |  Loss: 30.9566\n",
      "  ğŸ” Batch 88/90  |  Loss: 56.5389\n",
      "  ğŸ” Batch 89/90  |  Loss: 39.7818\n",
      "  ğŸ” Batch 90/90  |  Loss: 37.6538\n",
      "\n",
      "ğŸŒ€ Epoch 4/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 37.7756\n",
      "  ğŸ” Batch 2/90  |  Loss: 29.6254\n",
      "  ğŸ” Batch 3/90  |  Loss: 48.0056\n",
      "  ğŸ” Batch 4/90  |  Loss: 34.5252\n",
      "  ğŸ” Batch 5/90  |  Loss: 46.5181\n",
      "  ğŸ” Batch 6/90  |  Loss: 48.4758\n",
      "  ğŸ” Batch 7/90  |  Loss: 53.4780\n",
      "  ğŸ” Batch 8/90  |  Loss: 51.5695\n",
      "  ğŸ” Batch 9/90  |  Loss: 14.3090\n",
      "  ğŸ” Batch 10/90  |  Loss: 46.4958\n",
      "  ğŸ” Batch 11/90  |  Loss: 57.4919\n",
      "  ğŸ” Batch 12/90  |  Loss: 16.3057\n",
      "  ğŸ” Batch 13/90  |  Loss: 22.5154\n",
      "  ğŸ” Batch 14/90  |  Loss: 30.8855\n",
      "  ğŸ” Batch 15/90  |  Loss: 43.2178\n",
      "  ğŸ” Batch 16/90  |  Loss: 31.2035\n",
      "  ğŸ” Batch 17/90  |  Loss: 53.6664\n",
      "  ğŸ” Batch 18/90  |  Loss: 33.7032\n",
      "  ğŸ” Batch 19/90  |  Loss: 23.7226\n",
      "  ğŸ” Batch 20/90  |  Loss: 30.2816\n",
      "  ğŸ” Batch 21/90  |  Loss: 36.2867\n",
      "  ğŸ” Batch 22/90  |  Loss: 51.4579\n",
      "  ğŸ” Batch 23/90  |  Loss: 42.0349\n",
      "  ğŸ” Batch 24/90  |  Loss: 21.9588\n",
      "  ğŸ” Batch 25/90  |  Loss: 30.7189\n",
      "  ğŸ” Batch 26/90  |  Loss: 42.7873\n",
      "  ğŸ” Batch 27/90  |  Loss: 33.3750\n",
      "  ğŸ” Batch 28/90  |  Loss: 38.7045\n",
      "  ğŸ” Batch 29/90  |  Loss: 37.0564\n",
      "  ğŸ” Batch 30/90  |  Loss: 48.1948\n",
      "  ğŸ” Batch 31/90  |  Loss: 33.5647\n",
      "  ğŸ” Batch 32/90  |  Loss: 33.6223\n",
      "  ğŸ” Batch 33/90  |  Loss: 44.3744\n",
      "  ğŸ” Batch 34/90  |  Loss: 42.4630\n",
      "  ğŸ” Batch 35/90  |  Loss: 32.0025\n",
      "  ğŸ” Batch 36/90  |  Loss: 36.1990\n",
      "  ğŸ” Batch 37/90  |  Loss: 25.7652\n",
      "  ğŸ” Batch 38/90  |  Loss: 43.0355\n",
      "  ğŸ” Batch 39/90  |  Loss: 20.2131\n",
      "  ğŸ” Batch 40/90  |  Loss: 34.9146\n",
      "  ğŸ” Batch 41/90  |  Loss: 15.9913\n",
      "  ğŸ” Batch 42/90  |  Loss: 35.1833\n",
      "  ğŸ” Batch 43/90  |  Loss: 54.0498\n",
      "  ğŸ” Batch 44/90  |  Loss: 51.8056\n",
      "  ğŸ” Batch 45/90  |  Loss: 33.6556\n",
      "  ğŸ” Batch 46/90  |  Loss: 18.9975\n",
      "  ğŸ” Batch 47/90  |  Loss: 27.0374\n",
      "  ğŸ” Batch 48/90  |  Loss: 29.7588\n",
      "  ğŸ” Batch 49/90  |  Loss: 30.7022\n",
      "  ğŸ” Batch 50/90  |  Loss: 26.2201\n",
      "  ğŸ” Batch 51/90  |  Loss: 26.5729\n",
      "  ğŸ” Batch 52/90  |  Loss: 18.8779\n",
      "  ğŸ” Batch 53/90  |  Loss: 52.9809\n",
      "  ğŸ” Batch 54/90  |  Loss: 22.2096\n",
      "  ğŸ” Batch 55/90  |  Loss: 21.7679\n",
      "  ğŸ” Batch 56/90  |  Loss: 57.8705\n",
      "  ğŸ” Batch 57/90  |  Loss: 37.5316\n",
      "  ğŸ” Batch 58/90  |  Loss: 51.6938\n",
      "  ğŸ” Batch 59/90  |  Loss: 36.5038\n",
      "  ğŸ” Batch 60/90  |  Loss: 32.9831\n",
      "  ğŸ” Batch 61/90  |  Loss: 32.8934\n",
      "  ğŸ” Batch 62/90  |  Loss: 45.9625\n",
      "  ğŸ” Batch 63/90  |  Loss: 29.9859\n",
      "  ğŸ” Batch 64/90  |  Loss: 16.0344\n",
      "  ğŸ” Batch 65/90  |  Loss: 15.7288\n",
      "  ğŸ” Batch 66/90  |  Loss: 31.7177\n",
      "  ğŸ” Batch 67/90  |  Loss: 40.4976\n",
      "  ğŸ” Batch 68/90  |  Loss: 38.9348\n",
      "  ğŸ” Batch 69/90  |  Loss: 31.1762\n",
      "  ğŸ” Batch 70/90  |  Loss: 32.6743\n",
      "  ğŸ” Batch 71/90  |  Loss: 24.5222\n",
      "  ğŸ” Batch 72/90  |  Loss: 28.3906\n",
      "  ğŸ” Batch 73/90  |  Loss: 33.6978\n",
      "  ğŸ” Batch 74/90  |  Loss: 60.7748\n",
      "  ğŸ” Batch 75/90  |  Loss: 42.5770\n",
      "  ğŸ” Batch 76/90  |  Loss: 53.3458\n",
      "  ğŸ” Batch 77/90  |  Loss: 47.6740\n",
      "  ğŸ” Batch 78/90  |  Loss: 41.9178\n",
      "  ğŸ” Batch 79/90  |  Loss: 43.3477\n",
      "  ğŸ” Batch 80/90  |  Loss: 35.6539\n",
      "  ğŸ” Batch 81/90  |  Loss: 40.2450\n",
      "  ğŸ” Batch 82/90  |  Loss: 53.2374\n",
      "  ğŸ” Batch 83/90  |  Loss: 41.6785\n",
      "  ğŸ” Batch 84/90  |  Loss: 25.4240\n",
      "  ğŸ” Batch 85/90  |  Loss: 69.9240\n",
      "  ğŸ” Batch 86/90  |  Loss: 44.6572\n",
      "  ğŸ” Batch 87/90  |  Loss: 18.1944\n",
      "  ğŸ” Batch 88/90  |  Loss: 14.5487\n",
      "  ğŸ” Batch 89/90  |  Loss: 54.6532\n",
      "  ğŸ” Batch 90/90  |  Loss: 64.2075\n",
      "\n",
      "ğŸŒ€ Epoch 5/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 31.6079\n",
      "  ğŸ” Batch 2/90  |  Loss: 33.4182\n",
      "  ğŸ” Batch 3/90  |  Loss: 47.4892\n",
      "  ğŸ” Batch 4/90  |  Loss: 41.2671\n",
      "  ğŸ” Batch 5/90  |  Loss: 37.8840\n",
      "  ğŸ” Batch 6/90  |  Loss: 28.2899\n",
      "  ğŸ” Batch 7/90  |  Loss: 43.7882\n",
      "  ğŸ” Batch 8/90  |  Loss: 25.0536\n",
      "  ğŸ” Batch 9/90  |  Loss: 58.3396\n",
      "  ğŸ” Batch 10/90  |  Loss: 43.0585\n",
      "  ğŸ” Batch 11/90  |  Loss: 28.1595\n",
      "  ğŸ” Batch 12/90  |  Loss: 25.1563\n",
      "  ğŸ” Batch 13/90  |  Loss: 32.6438\n",
      "  ğŸ” Batch 14/90  |  Loss: 31.4506\n",
      "  ğŸ” Batch 15/90  |  Loss: 25.5169\n",
      "  ğŸ” Batch 16/90  |  Loss: 52.9298\n",
      "  ğŸ” Batch 17/90  |  Loss: 36.5661\n",
      "  ğŸ” Batch 18/90  |  Loss: 15.2783\n",
      "  ğŸ” Batch 19/90  |  Loss: 37.7133\n",
      "  ğŸ” Batch 20/90  |  Loss: 17.0056\n",
      "  ğŸ” Batch 21/90  |  Loss: 33.1886\n",
      "  ğŸ” Batch 22/90  |  Loss: 26.6322\n",
      "  ğŸ” Batch 23/90  |  Loss: 24.7693\n",
      "  ğŸ” Batch 24/90  |  Loss: 36.5324\n",
      "  ğŸ” Batch 25/90  |  Loss: 26.2603\n",
      "  ğŸ” Batch 26/90  |  Loss: 25.3127\n",
      "  ğŸ” Batch 27/90  |  Loss: 62.2301\n",
      "  ğŸ” Batch 28/90  |  Loss: 34.2112\n",
      "  ğŸ” Batch 29/90  |  Loss: 55.0775\n",
      "  ğŸ” Batch 30/90  |  Loss: 50.2003\n",
      "  ğŸ” Batch 31/90  |  Loss: 90.0419\n",
      "  ğŸ” Batch 32/90  |  Loss: 47.9878\n",
      "  ğŸ” Batch 33/90  |  Loss: 21.5868\n",
      "  ğŸ” Batch 34/90  |  Loss: 40.8188\n",
      "  ğŸ” Batch 35/90  |  Loss: 80.1868\n",
      "  ğŸ” Batch 36/90  |  Loss: 29.3779\n",
      "  ğŸ” Batch 37/90  |  Loss: 22.2874\n",
      "  ğŸ” Batch 38/90  |  Loss: 30.2442\n",
      "  ğŸ” Batch 39/90  |  Loss: 42.4298\n",
      "  ğŸ” Batch 40/90  |  Loss: 30.6265\n",
      "  ğŸ” Batch 41/90  |  Loss: 10.5235\n",
      "  ğŸ” Batch 42/90  |  Loss: 34.0104\n",
      "  ğŸ” Batch 43/90  |  Loss: 16.8895\n",
      "  ğŸ” Batch 44/90  |  Loss: 34.0829\n",
      "  ğŸ” Batch 45/90  |  Loss: 22.2381\n",
      "  ğŸ” Batch 46/90  |  Loss: 23.2956\n",
      "  ğŸ” Batch 47/90  |  Loss: 44.2551\n",
      "  ğŸ” Batch 48/90  |  Loss: 51.9883\n",
      "  ğŸ” Batch 49/90  |  Loss: 35.6316\n",
      "  ğŸ” Batch 50/90  |  Loss: 50.8095\n",
      "  ğŸ” Batch 51/90  |  Loss: 38.3819\n",
      "  ğŸ” Batch 52/90  |  Loss: 33.7918\n",
      "  ğŸ” Batch 53/90  |  Loss: 29.3085\n",
      "  ğŸ” Batch 54/90  |  Loss: 39.7430\n",
      "  ğŸ” Batch 55/90  |  Loss: 37.3684\n",
      "  ğŸ” Batch 56/90  |  Loss: 29.7483\n",
      "  ğŸ” Batch 57/90  |  Loss: 17.9000\n",
      "  ğŸ” Batch 58/90  |  Loss: 27.3253\n",
      "  ğŸ” Batch 59/90  |  Loss: 31.7339\n",
      "  ğŸ” Batch 60/90  |  Loss: 25.9456\n",
      "  ğŸ” Batch 61/90  |  Loss: 31.0544\n",
      "  ğŸ” Batch 62/90  |  Loss: 18.9078\n",
      "  ğŸ” Batch 63/90  |  Loss: 29.9103\n",
      "  ğŸ” Batch 64/90  |  Loss: 33.8348\n",
      "  ğŸ” Batch 65/90  |  Loss: 30.7717\n",
      "  ğŸ” Batch 66/90  |  Loss: 40.0385\n",
      "  ğŸ” Batch 67/90  |  Loss: 21.8758\n",
      "  ğŸ” Batch 68/90  |  Loss: 31.6078\n",
      "  ğŸ” Batch 69/90  |  Loss: 41.4482\n",
      "  ğŸ” Batch 70/90  |  Loss: 22.0438\n",
      "  ğŸ” Batch 71/90  |  Loss: 30.1977\n",
      "  ğŸ” Batch 72/90  |  Loss: 39.9779\n",
      "  ğŸ” Batch 73/90  |  Loss: 39.6668\n",
      "  ğŸ” Batch 74/90  |  Loss: 47.0476\n",
      "  ğŸ” Batch 75/90  |  Loss: 27.1295\n",
      "  ğŸ” Batch 76/90  |  Loss: 22.6450\n",
      "  ğŸ” Batch 77/90  |  Loss: 46.0222\n",
      "  ğŸ” Batch 78/90  |  Loss: 14.1977\n",
      "  ğŸ” Batch 79/90  |  Loss: 56.6713\n",
      "  ğŸ” Batch 80/90  |  Loss: 37.3300\n",
      "  ğŸ” Batch 81/90  |  Loss: 44.8732\n",
      "  ğŸ” Batch 82/90  |  Loss: 30.4814\n",
      "  ğŸ” Batch 83/90  |  Loss: 14.3790\n",
      "  ğŸ” Batch 84/90  |  Loss: 31.3757\n",
      "  ğŸ” Batch 85/90  |  Loss: 38.7359\n",
      "  ğŸ” Batch 86/90  |  Loss: 16.8250\n",
      "  ğŸ” Batch 87/90  |  Loss: 35.0457\n",
      "  ğŸ” Batch 88/90  |  Loss: 44.4540\n",
      "  ğŸ” Batch 89/90  |  Loss: 40.1128\n",
      "  ğŸ” Batch 90/90  |  Loss: 51.6447\n",
      "\n",
      "ğŸŒ€ Epoch 6/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 36.8540\n",
      "  ğŸ” Batch 2/90  |  Loss: 32.0013\n",
      "  ğŸ” Batch 3/90  |  Loss: 35.0523\n",
      "  ğŸ” Batch 4/90  |  Loss: 22.9397\n",
      "  ğŸ” Batch 5/90  |  Loss: 15.7654\n",
      "  ğŸ” Batch 6/90  |  Loss: 29.8212\n",
      "  ğŸ” Batch 7/90  |  Loss: 39.6223\n",
      "  ğŸ” Batch 8/90  |  Loss: 42.1848\n",
      "  ğŸ” Batch 9/90  |  Loss: 18.2522\n",
      "  ğŸ” Batch 10/90  |  Loss: 31.0660\n",
      "  ğŸ” Batch 11/90  |  Loss: 49.2997\n",
      "  ğŸ” Batch 12/90  |  Loss: 36.3885\n",
      "  ğŸ” Batch 13/90  |  Loss: 27.4323\n",
      "  ğŸ” Batch 14/90  |  Loss: 51.8860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 15/90  |  Loss: 50.5224\n",
      "  ğŸ” Batch 16/90  |  Loss: 46.2011\n",
      "  ğŸ” Batch 17/90  |  Loss: 64.0084\n",
      "  ğŸ” Batch 18/90  |  Loss: 22.8450\n",
      "  ğŸ” Batch 19/90  |  Loss: 36.6750\n",
      "  ğŸ” Batch 20/90  |  Loss: 27.9693\n",
      "  ğŸ” Batch 21/90  |  Loss: 41.1095\n",
      "  ğŸ” Batch 22/90  |  Loss: 26.7638\n",
      "  ğŸ” Batch 23/90  |  Loss: 45.1848\n",
      "  ğŸ” Batch 24/90  |  Loss: 20.4221\n",
      "  ğŸ” Batch 25/90  |  Loss: 26.2882\n",
      "  ğŸ” Batch 26/90  |  Loss: 38.2784\n",
      "  ğŸ” Batch 27/90  |  Loss: 35.6829\n",
      "  ğŸ” Batch 28/90  |  Loss: 55.4551\n",
      "  ğŸ” Batch 29/90  |  Loss: 34.1029\n",
      "  ğŸ” Batch 30/90  |  Loss: 48.1946\n",
      "  ğŸ” Batch 31/90  |  Loss: 41.3725\n",
      "  ğŸ” Batch 32/90  |  Loss: 26.6597\n",
      "  ğŸ” Batch 33/90  |  Loss: 36.5121\n",
      "  ğŸ” Batch 34/90  |  Loss: 30.4972\n",
      "  ğŸ” Batch 35/90  |  Loss: 15.2056\n",
      "  ğŸ” Batch 36/90  |  Loss: 29.6527\n",
      "  ğŸ” Batch 37/90  |  Loss: 42.3915\n",
      "  ğŸ” Batch 38/90  |  Loss: 41.9601\n",
      "  ğŸ” Batch 39/90  |  Loss: 31.8090\n",
      "  ğŸ” Batch 40/90  |  Loss: 34.3510\n",
      "  ğŸ” Batch 41/90  |  Loss: 53.1672\n",
      "  ğŸ” Batch 42/90  |  Loss: 34.4458\n",
      "  ğŸ” Batch 43/90  |  Loss: 39.0093\n",
      "  ğŸ” Batch 44/90  |  Loss: 35.3469\n",
      "  ğŸ” Batch 45/90  |  Loss: 26.1195\n",
      "  ğŸ” Batch 46/90  |  Loss: 29.5124\n",
      "  ğŸ” Batch 47/90  |  Loss: 19.1179\n",
      "  ğŸ” Batch 48/90  |  Loss: 28.1291\n",
      "  ğŸ” Batch 49/90  |  Loss: 53.4169\n",
      "  ğŸ” Batch 50/90  |  Loss: 31.4379\n",
      "  ğŸ” Batch 51/90  |  Loss: 29.0009\n",
      "  ğŸ” Batch 52/90  |  Loss: 60.5509\n",
      "  ğŸ” Batch 53/90  |  Loss: 45.6256\n",
      "  ğŸ” Batch 54/90  |  Loss: 54.6527\n",
      "  ğŸ” Batch 55/90  |  Loss: 33.4029\n",
      "  ğŸ” Batch 56/90  |  Loss: 50.1201\n",
      "  ğŸ” Batch 57/90  |  Loss: 27.8942\n",
      "  ğŸ” Batch 58/90  |  Loss: 27.1510\n",
      "  ğŸ” Batch 59/90  |  Loss: 57.2804\n",
      "  ğŸ” Batch 60/90  |  Loss: 54.4249\n",
      "  ğŸ” Batch 61/90  |  Loss: 37.3301\n",
      "  ğŸ” Batch 62/90  |  Loss: 59.1966\n",
      "  ğŸ” Batch 63/90  |  Loss: 27.5543\n",
      "  ğŸ” Batch 64/90  |  Loss: 26.3671\n",
      "  ğŸ” Batch 65/90  |  Loss: 47.5047\n",
      "  ğŸ” Batch 66/90  |  Loss: 37.9536\n",
      "  ğŸ” Batch 67/90  |  Loss: 53.1438\n",
      "  ğŸ” Batch 68/90  |  Loss: 36.0715\n",
      "  ğŸ” Batch 69/90  |  Loss: 66.6610\n",
      "  ğŸ” Batch 70/90  |  Loss: 20.3776\n",
      "  ğŸ” Batch 71/90  |  Loss: 31.4268\n",
      "  ğŸ” Batch 72/90  |  Loss: 21.6819\n",
      "  ğŸ” Batch 73/90  |  Loss: 12.5010\n",
      "  ğŸ” Batch 74/90  |  Loss: 44.7627\n",
      "  ğŸ” Batch 75/90  |  Loss: 44.2061\n",
      "  ğŸ” Batch 76/90  |  Loss: 43.7257\n",
      "  ğŸ” Batch 77/90  |  Loss: 37.8233\n",
      "  ğŸ” Batch 78/90  |  Loss: 66.0108\n",
      "  ğŸ” Batch 79/90  |  Loss: 35.7314\n",
      "  ğŸ” Batch 80/90  |  Loss: 31.5525\n",
      "  ğŸ” Batch 81/90  |  Loss: 34.9849\n",
      "  ğŸ” Batch 82/90  |  Loss: 22.5195\n",
      "  ğŸ” Batch 83/90  |  Loss: 40.5465\n",
      "  ğŸ” Batch 84/90  |  Loss: 32.0708\n",
      "  ğŸ” Batch 85/90  |  Loss: 30.1953\n",
      "  ğŸ” Batch 86/90  |  Loss: 57.9519\n",
      "  ğŸ” Batch 87/90  |  Loss: 39.3829\n",
      "  ğŸ” Batch 88/90  |  Loss: 43.3042\n",
      "  ğŸ” Batch 89/90  |  Loss: 20.9123\n",
      "  ğŸ” Batch 90/90  |  Loss: 62.2406\n",
      "\n",
      "ğŸŒ€ Epoch 7/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 25.6708\n",
      "  ğŸ” Batch 2/90  |  Loss: 59.8599\n",
      "  ğŸ” Batch 3/90  |  Loss: 55.7119\n",
      "  ğŸ” Batch 4/90  |  Loss: 14.9483\n",
      "  ğŸ” Batch 5/90  |  Loss: 34.2394\n",
      "  ğŸ” Batch 6/90  |  Loss: 24.7194\n",
      "  ğŸ” Batch 7/90  |  Loss: 13.4123\n",
      "  ğŸ” Batch 8/90  |  Loss: 10.4680\n",
      "  ğŸ” Batch 9/90  |  Loss: 31.1160\n",
      "  ğŸ” Batch 10/90  |  Loss: 22.0510\n",
      "  ğŸ” Batch 11/90  |  Loss: 33.3600\n",
      "  ğŸ” Batch 12/90  |  Loss: 44.1484\n",
      "  ğŸ” Batch 13/90  |  Loss: 21.2272\n",
      "  ğŸ” Batch 14/90  |  Loss: 6.8174\n",
      "  ğŸ” Batch 15/90  |  Loss: 34.6455\n",
      "  ğŸ” Batch 16/90  |  Loss: 30.2135\n",
      "  ğŸ” Batch 17/90  |  Loss: 54.9668\n",
      "  ğŸ” Batch 18/90  |  Loss: 34.6867\n",
      "  ğŸ” Batch 19/90  |  Loss: 35.5099\n",
      "  ğŸ” Batch 20/90  |  Loss: 28.7107\n",
      "  ğŸ” Batch 21/90  |  Loss: 16.0998\n",
      "  ğŸ” Batch 22/90  |  Loss: 49.2677\n",
      "  ğŸ” Batch 23/90  |  Loss: 33.6421\n",
      "  ğŸ” Batch 24/90  |  Loss: 20.1077\n",
      "  ğŸ” Batch 25/90  |  Loss: 44.2022\n",
      "  ğŸ” Batch 26/90  |  Loss: 22.0831\n",
      "  ğŸ” Batch 27/90  |  Loss: 25.7247\n",
      "  ğŸ” Batch 28/90  |  Loss: 26.7947\n",
      "  ğŸ” Batch 29/90  |  Loss: 28.4401\n",
      "  ğŸ” Batch 30/90  |  Loss: 41.4402\n",
      "  ğŸ” Batch 31/90  |  Loss: 18.7087\n",
      "  ğŸ” Batch 32/90  |  Loss: 46.0620\n",
      "  ğŸ” Batch 33/90  |  Loss: 26.9226\n",
      "  ğŸ” Batch 34/90  |  Loss: 26.1978\n",
      "  ğŸ” Batch 35/90  |  Loss: 27.5490\n",
      "  ğŸ” Batch 36/90  |  Loss: 35.4554\n",
      "  ğŸ” Batch 37/90  |  Loss: 32.3494\n",
      "  ğŸ” Batch 38/90  |  Loss: 56.7654\n",
      "  ğŸ” Batch 39/90  |  Loss: 31.9236\n",
      "  ğŸ” Batch 40/90  |  Loss: 59.7260\n",
      "  ğŸ” Batch 41/90  |  Loss: 34.2294\n",
      "  ğŸ” Batch 42/90  |  Loss: 39.1242\n",
      "  ğŸ” Batch 43/90  |  Loss: 21.2822\n",
      "  ğŸ” Batch 44/90  |  Loss: 40.6096\n",
      "  ğŸ” Batch 45/90  |  Loss: 43.4761\n",
      "  ğŸ” Batch 46/90  |  Loss: 53.1299\n",
      "  ğŸ” Batch 47/90  |  Loss: 59.5137\n",
      "  ğŸ” Batch 48/90  |  Loss: 49.0200\n",
      "  ğŸ” Batch 49/90  |  Loss: 52.2703\n",
      "  ğŸ” Batch 50/90  |  Loss: 31.5967\n",
      "  ğŸ” Batch 51/90  |  Loss: 42.5606\n",
      "  ğŸ” Batch 52/90  |  Loss: 60.8975\n",
      "  ğŸ” Batch 53/90  |  Loss: 46.4965\n",
      "  ğŸ” Batch 54/90  |  Loss: 37.2087\n",
      "  ğŸ” Batch 55/90  |  Loss: 44.2180\n",
      "  ğŸ” Batch 56/90  |  Loss: 32.5179\n",
      "  ğŸ” Batch 57/90  |  Loss: 30.5148\n",
      "  ğŸ” Batch 58/90  |  Loss: 35.9804\n",
      "  ğŸ” Batch 59/90  |  Loss: 26.3007\n",
      "  ğŸ” Batch 60/90  |  Loss: 31.9900\n",
      "  ğŸ” Batch 61/90  |  Loss: 43.1802\n",
      "  ğŸ” Batch 62/90  |  Loss: 57.6429\n",
      "  ğŸ” Batch 63/90  |  Loss: 49.6201\n",
      "  ğŸ” Batch 64/90  |  Loss: 26.5379\n",
      "  ğŸ” Batch 65/90  |  Loss: 34.3938\n",
      "  ğŸ” Batch 66/90  |  Loss: 45.4037\n",
      "  ğŸ” Batch 67/90  |  Loss: 31.2555\n",
      "  ğŸ” Batch 68/90  |  Loss: 45.1417\n",
      "  ğŸ” Batch 69/90  |  Loss: 29.9960\n",
      "  ğŸ” Batch 70/90  |  Loss: 34.3953\n",
      "  ğŸ” Batch 71/90  |  Loss: 80.3857\n",
      "  ğŸ” Batch 72/90  |  Loss: 50.0737\n",
      "  ğŸ” Batch 73/90  |  Loss: 70.6448\n",
      "  ğŸ” Batch 74/90  |  Loss: 18.4691\n",
      "  ğŸ” Batch 75/90  |  Loss: 25.8169\n",
      "  ğŸ” Batch 76/90  |  Loss: 57.8875\n",
      "  ğŸ” Batch 77/90  |  Loss: 40.9181\n",
      "  ğŸ” Batch 78/90  |  Loss: 80.8219\n",
      "  ğŸ” Batch 79/90  |  Loss: 33.5223\n",
      "  ğŸ” Batch 80/90  |  Loss: 36.7875\n",
      "  ğŸ” Batch 81/90  |  Loss: 42.1303\n",
      "  ğŸ” Batch 82/90  |  Loss: 27.6598\n",
      "  ğŸ” Batch 83/90  |  Loss: 30.9739\n",
      "  ğŸ” Batch 84/90  |  Loss: 23.1032\n",
      "  ğŸ” Batch 85/90  |  Loss: 71.5643\n",
      "  ğŸ” Batch 86/90  |  Loss: 29.0667\n",
      "  ğŸ” Batch 87/90  |  Loss: 28.5843\n",
      "  ğŸ” Batch 88/90  |  Loss: 58.0922\n",
      "  ğŸ” Batch 89/90  |  Loss: 24.5933\n",
      "  ğŸ” Batch 90/90  |  Loss: 30.2678\n",
      "\n",
      "ğŸŒ€ Epoch 8/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 21.3807\n",
      "  ğŸ” Batch 2/90  |  Loss: 27.3860\n",
      "  ğŸ” Batch 3/90  |  Loss: 21.6757\n",
      "  ğŸ” Batch 4/90  |  Loss: 26.6611\n",
      "  ğŸ” Batch 5/90  |  Loss: 49.9031\n",
      "  ğŸ” Batch 6/90  |  Loss: 27.6129\n",
      "  ğŸ” Batch 7/90  |  Loss: 40.1130\n",
      "  ğŸ” Batch 8/90  |  Loss: 31.8942\n",
      "  ğŸ” Batch 9/90  |  Loss: 34.8600\n",
      "  ğŸ” Batch 10/90  |  Loss: 26.6708\n",
      "  ğŸ” Batch 11/90  |  Loss: 33.2231\n",
      "  ğŸ” Batch 12/90  |  Loss: 24.8139\n",
      "  ğŸ” Batch 13/90  |  Loss: 29.2715\n",
      "  ğŸ” Batch 14/90  |  Loss: 46.5342\n",
      "  ğŸ” Batch 15/90  |  Loss: 34.1810\n",
      "  ğŸ” Batch 16/90  |  Loss: 37.1741\n",
      "  ğŸ” Batch 17/90  |  Loss: 22.9450\n",
      "  ğŸ” Batch 18/90  |  Loss: 23.8226\n",
      "  ğŸ” Batch 19/90  |  Loss: 27.9791\n",
      "  ğŸ” Batch 20/90  |  Loss: 30.6593\n",
      "  ğŸ” Batch 21/90  |  Loss: 34.7790\n",
      "  ğŸ” Batch 22/90  |  Loss: 45.9871\n",
      "  ğŸ” Batch 23/90  |  Loss: 37.3332\n",
      "  ğŸ” Batch 24/90  |  Loss: 35.8887\n",
      "  ğŸ” Batch 25/90  |  Loss: 56.1500\n",
      "  ğŸ” Batch 26/90  |  Loss: 36.1054\n",
      "  ğŸ” Batch 27/90  |  Loss: 45.8890\n",
      "  ğŸ” Batch 28/90  |  Loss: 30.3863\n",
      "  ğŸ” Batch 29/90  |  Loss: 38.0133\n",
      "  ğŸ” Batch 30/90  |  Loss: 35.3906\n",
      "  ğŸ” Batch 31/90  |  Loss: 42.6255\n",
      "  ğŸ” Batch 32/90  |  Loss: 47.1750\n",
      "  ğŸ” Batch 33/90  |  Loss: 24.9087\n",
      "  ğŸ” Batch 34/90  |  Loss: 32.4745\n",
      "  ğŸ” Batch 35/90  |  Loss: 44.1469\n",
      "  ğŸ” Batch 36/90  |  Loss: 32.5925\n",
      "  ğŸ” Batch 37/90  |  Loss: 43.3520\n",
      "  ğŸ” Batch 38/90  |  Loss: 22.3298\n",
      "  ğŸ” Batch 39/90  |  Loss: 31.9903\n",
      "  ğŸ” Batch 40/90  |  Loss: 29.2860\n",
      "  ğŸ” Batch 41/90  |  Loss: 46.3343\n",
      "  ğŸ” Batch 42/90  |  Loss: 46.9750\n",
      "  ğŸ” Batch 43/90  |  Loss: 34.8649\n",
      "  ğŸ” Batch 44/90  |  Loss: 27.8697\n",
      "  ğŸ” Batch 45/90  |  Loss: 33.7999\n",
      "  ğŸ” Batch 46/90  |  Loss: 77.4784\n",
      "  ğŸ” Batch 47/90  |  Loss: 48.4805\n",
      "  ğŸ” Batch 48/90  |  Loss: 54.9764\n",
      "  ğŸ” Batch 49/90  |  Loss: 57.6415\n",
      "  ğŸ” Batch 50/90  |  Loss: 62.9228\n",
      "  ğŸ” Batch 51/90  |  Loss: 38.1289\n",
      "  ğŸ” Batch 52/90  |  Loss: 46.5043\n",
      "  ğŸ” Batch 53/90  |  Loss: 33.8433\n",
      "  ğŸ” Batch 54/90  |  Loss: 57.5028\n",
      "  ğŸ” Batch 55/90  |  Loss: 34.7018\n",
      "  ğŸ” Batch 56/90  |  Loss: 53.2607\n",
      "  ğŸ” Batch 57/90  |  Loss: 27.9226\n",
      "  ğŸ” Batch 58/90  |  Loss: 39.3286\n",
      "  ğŸ” Batch 59/90  |  Loss: 66.2659\n",
      "  ğŸ” Batch 60/90  |  Loss: 27.3165\n",
      "  ğŸ” Batch 61/90  |  Loss: 27.0052\n",
      "  ğŸ” Batch 62/90  |  Loss: 40.1248\n",
      "  ğŸ” Batch 63/90  |  Loss: 36.7989\n",
      "  ğŸ” Batch 64/90  |  Loss: 35.5251\n",
      "  ğŸ” Batch 65/90  |  Loss: 48.7216\n",
      "  ğŸ” Batch 66/90  |  Loss: 30.8723\n",
      "  ğŸ” Batch 67/90  |  Loss: 41.7078\n",
      "  ğŸ” Batch 68/90  |  Loss: 47.9232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 69/90  |  Loss: 44.2271\n",
      "  ğŸ” Batch 70/90  |  Loss: 22.6312\n",
      "  ğŸ” Batch 71/90  |  Loss: 37.7139\n",
      "  ğŸ” Batch 72/90  |  Loss: 32.5627\n",
      "  ğŸ” Batch 73/90  |  Loss: 32.3286\n",
      "  ğŸ” Batch 74/90  |  Loss: 12.5748\n",
      "  ğŸ” Batch 75/90  |  Loss: 63.3756\n",
      "  ğŸ” Batch 76/90  |  Loss: 30.7993\n",
      "  ğŸ” Batch 77/90  |  Loss: 43.3963\n",
      "  ğŸ” Batch 78/90  |  Loss: 26.4356\n",
      "  ğŸ” Batch 79/90  |  Loss: 40.2838\n",
      "  ğŸ” Batch 80/90  |  Loss: 51.6383\n",
      "  ğŸ” Batch 81/90  |  Loss: 20.3173\n",
      "  ğŸ” Batch 82/90  |  Loss: 40.4568\n",
      "  ğŸ” Batch 83/90  |  Loss: 36.9136\n",
      "  ğŸ” Batch 84/90  |  Loss: 32.0689\n",
      "  ğŸ” Batch 85/90  |  Loss: 23.5240\n",
      "  ğŸ” Batch 86/90  |  Loss: 55.2768\n",
      "  ğŸ” Batch 87/90  |  Loss: 47.2539\n",
      "  ğŸ” Batch 88/90  |  Loss: 31.0945\n",
      "  ğŸ” Batch 89/90  |  Loss: 23.9719\n",
      "  ğŸ” Batch 90/90  |  Loss: 36.9371\n",
      "\n",
      "ğŸŒ€ Epoch 9/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 22.2626\n",
      "  ğŸ” Batch 2/90  |  Loss: 34.7298\n",
      "  ğŸ” Batch 3/90  |  Loss: 26.4900\n",
      "  ğŸ” Batch 4/90  |  Loss: 24.8715\n",
      "  ğŸ” Batch 5/90  |  Loss: 36.6919\n",
      "  ğŸ” Batch 6/90  |  Loss: 24.8087\n",
      "  ğŸ” Batch 7/90  |  Loss: 30.9458\n",
      "  ğŸ” Batch 8/90  |  Loss: 52.3858\n",
      "  ğŸ” Batch 9/90  |  Loss: 19.8246\n",
      "  ğŸ” Batch 10/90  |  Loss: 62.4710\n",
      "  ğŸ” Batch 11/90  |  Loss: 30.2867\n",
      "  ğŸ” Batch 12/90  |  Loss: 40.4422\n",
      "  ğŸ” Batch 13/90  |  Loss: 33.8450\n",
      "  ğŸ” Batch 14/90  |  Loss: 28.6537\n",
      "  ğŸ” Batch 15/90  |  Loss: 31.1577\n",
      "  ğŸ” Batch 16/90  |  Loss: 50.9012\n",
      "  ğŸ” Batch 17/90  |  Loss: 20.2310\n",
      "  ğŸ” Batch 18/90  |  Loss: 36.0687\n",
      "  ğŸ” Batch 19/90  |  Loss: 36.0986\n",
      "  ğŸ” Batch 20/90  |  Loss: 40.1949\n",
      "  ğŸ” Batch 21/90  |  Loss: 34.1279\n",
      "  ğŸ” Batch 22/90  |  Loss: 38.4354\n",
      "  ğŸ” Batch 23/90  |  Loss: 40.3206\n",
      "  ğŸ” Batch 24/90  |  Loss: 51.0207\n",
      "  ğŸ” Batch 25/90  |  Loss: 51.8103\n",
      "  ğŸ” Batch 26/90  |  Loss: 52.0038\n",
      "  ğŸ” Batch 27/90  |  Loss: 54.7657\n",
      "  ğŸ” Batch 28/90  |  Loss: 67.7356\n",
      "  ğŸ” Batch 29/90  |  Loss: 23.2465\n",
      "  ğŸ” Batch 30/90  |  Loss: 34.6924\n",
      "  ğŸ” Batch 31/90  |  Loss: 55.1822\n",
      "  ğŸ” Batch 32/90  |  Loss: 53.8390\n",
      "  ğŸ” Batch 33/90  |  Loss: 28.6816\n",
      "  ğŸ” Batch 34/90  |  Loss: 26.8826\n",
      "  ğŸ” Batch 35/90  |  Loss: 28.3070\n",
      "  ğŸ” Batch 36/90  |  Loss: 24.3004\n",
      "  ğŸ” Batch 37/90  |  Loss: 28.9082\n",
      "  ğŸ” Batch 38/90  |  Loss: 53.9305\n",
      "  ğŸ” Batch 39/90  |  Loss: 25.0360\n",
      "  ğŸ” Batch 40/90  |  Loss: 50.7199\n",
      "  ğŸ” Batch 41/90  |  Loss: 41.1029\n",
      "  ğŸ” Batch 42/90  |  Loss: 44.4349\n",
      "  ğŸ” Batch 43/90  |  Loss: 48.9700\n",
      "  ğŸ” Batch 44/90  |  Loss: 49.6929\n",
      "  ğŸ” Batch 45/90  |  Loss: 35.5498\n",
      "  ğŸ” Batch 46/90  |  Loss: 23.7045\n",
      "  ğŸ” Batch 47/90  |  Loss: 46.9317\n",
      "  ğŸ” Batch 48/90  |  Loss: 32.8832\n",
      "  ğŸ” Batch 49/90  |  Loss: 48.3834\n",
      "  ğŸ” Batch 50/90  |  Loss: 22.4183\n",
      "  ğŸ” Batch 51/90  |  Loss: 22.2198\n",
      "  ğŸ” Batch 52/90  |  Loss: 53.0780\n",
      "  ğŸ” Batch 53/90  |  Loss: 27.7006\n",
      "  ğŸ” Batch 54/90  |  Loss: 19.4325\n",
      "  ğŸ” Batch 55/90  |  Loss: 49.9345\n",
      "  ğŸ” Batch 56/90  |  Loss: 41.3103\n",
      "  ğŸ” Batch 57/90  |  Loss: 26.0937\n",
      "  ğŸ” Batch 58/90  |  Loss: 29.3471\n",
      "  ğŸ” Batch 59/90  |  Loss: 70.3106\n",
      "  ğŸ” Batch 60/90  |  Loss: 14.5444\n",
      "  ğŸ” Batch 61/90  |  Loss: 25.5337\n",
      "  ğŸ” Batch 62/90  |  Loss: 28.1355\n",
      "  ğŸ” Batch 63/90  |  Loss: 35.8433\n",
      "  ğŸ” Batch 64/90  |  Loss: 28.8786\n",
      "  ğŸ” Batch 65/90  |  Loss: 52.0479\n",
      "  ğŸ” Batch 66/90  |  Loss: 36.5880\n",
      "  ğŸ” Batch 67/90  |  Loss: 46.2526\n",
      "  ğŸ” Batch 68/90  |  Loss: 35.9696\n",
      "  ğŸ” Batch 69/90  |  Loss: 44.4942\n",
      "  ğŸ” Batch 70/90  |  Loss: 41.8233\n",
      "  ğŸ” Batch 71/90  |  Loss: 36.1711\n",
      "  ğŸ” Batch 72/90  |  Loss: 52.4886\n",
      "  ğŸ” Batch 73/90  |  Loss: 35.5966\n",
      "  ğŸ” Batch 74/90  |  Loss: 25.9365\n",
      "  ğŸ” Batch 75/90  |  Loss: 31.2743\n",
      "  ğŸ” Batch 76/90  |  Loss: 19.2340\n",
      "  ğŸ” Batch 77/90  |  Loss: 50.5603\n",
      "  ğŸ” Batch 78/90  |  Loss: 42.4052\n",
      "  ğŸ” Batch 79/90  |  Loss: 31.0214\n",
      "  ğŸ” Batch 80/90  |  Loss: 26.6903\n",
      "  ğŸ” Batch 81/90  |  Loss: 50.0690\n",
      "  ğŸ” Batch 82/90  |  Loss: 29.2671\n",
      "  ğŸ” Batch 83/90  |  Loss: 36.7308\n",
      "  ğŸ” Batch 84/90  |  Loss: 53.3839\n",
      "  ğŸ” Batch 85/90  |  Loss: 27.2084\n",
      "  ğŸ” Batch 86/90  |  Loss: 36.5405\n",
      "  ğŸ” Batch 87/90  |  Loss: 47.4608\n",
      "  ğŸ” Batch 88/90  |  Loss: 71.4674\n",
      "  ğŸ” Batch 89/90  |  Loss: 19.6072\n",
      "  ğŸ” Batch 90/90  |  Loss: 47.4626\n",
      "\n",
      "ğŸŒ€ Epoch 10/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 47.7520\n",
      "  ğŸ” Batch 2/90  |  Loss: 48.4311\n",
      "  ğŸ” Batch 3/90  |  Loss: 20.2785\n",
      "  ğŸ” Batch 4/90  |  Loss: 14.7713\n",
      "  ğŸ” Batch 5/90  |  Loss: 50.1855\n",
      "  ğŸ” Batch 6/90  |  Loss: 15.8011\n",
      "  ğŸ” Batch 7/90  |  Loss: 43.5389\n",
      "  ğŸ” Batch 8/90  |  Loss: 13.8963\n",
      "  ğŸ” Batch 9/90  |  Loss: 49.1003\n",
      "  ğŸ” Batch 10/90  |  Loss: 40.0450\n",
      "  ğŸ” Batch 11/90  |  Loss: 27.1656\n",
      "  ğŸ” Batch 12/90  |  Loss: 18.3474\n",
      "  ğŸ” Batch 13/90  |  Loss: 22.0061\n",
      "  ğŸ” Batch 14/90  |  Loss: 42.9359\n",
      "  ğŸ” Batch 15/90  |  Loss: 24.0066\n",
      "  ğŸ” Batch 16/90  |  Loss: 33.2412\n",
      "  ğŸ” Batch 17/90  |  Loss: 34.4415\n",
      "  ğŸ” Batch 18/90  |  Loss: 35.9489\n",
      "  ğŸ” Batch 19/90  |  Loss: 37.3773\n",
      "  ğŸ” Batch 20/90  |  Loss: 47.3123\n",
      "  ğŸ” Batch 21/90  |  Loss: 21.5771\n",
      "  ğŸ” Batch 22/90  |  Loss: 69.2364\n",
      "  ğŸ” Batch 23/90  |  Loss: 72.2928\n",
      "  ğŸ” Batch 24/90  |  Loss: 21.0344\n",
      "  ğŸ” Batch 25/90  |  Loss: 28.6492\n",
      "  ğŸ” Batch 26/90  |  Loss: 18.4012\n",
      "  ğŸ” Batch 27/90  |  Loss: 31.6539\n",
      "  ğŸ” Batch 28/90  |  Loss: 41.5298\n",
      "  ğŸ” Batch 29/90  |  Loss: 46.7480\n",
      "  ğŸ” Batch 30/90  |  Loss: 52.4152\n",
      "  ğŸ” Batch 31/90  |  Loss: 29.5621\n",
      "  ğŸ” Batch 32/90  |  Loss: 50.5481\n",
      "  ğŸ” Batch 33/90  |  Loss: 27.2144\n",
      "  ğŸ” Batch 34/90  |  Loss: 37.1770\n",
      "  ğŸ” Batch 35/90  |  Loss: 40.0850\n",
      "  ğŸ” Batch 36/90  |  Loss: 42.6003\n",
      "  ğŸ” Batch 37/90  |  Loss: 50.7337\n",
      "  ğŸ” Batch 38/90  |  Loss: 37.5833\n",
      "  ğŸ” Batch 39/90  |  Loss: 52.4211\n",
      "  ğŸ” Batch 40/90  |  Loss: 41.8256\n",
      "  ğŸ” Batch 41/90  |  Loss: 48.9993\n",
      "  ğŸ” Batch 42/90  |  Loss: 39.4480\n",
      "  ğŸ” Batch 43/90  |  Loss: 59.3536\n",
      "  ğŸ” Batch 44/90  |  Loss: 30.6725\n",
      "  ğŸ” Batch 45/90  |  Loss: 46.5546\n",
      "  ğŸ” Batch 46/90  |  Loss: 26.2590\n",
      "  ğŸ” Batch 47/90  |  Loss: 49.6154\n",
      "  ğŸ” Batch 48/90  |  Loss: 39.7488\n",
      "  ğŸ” Batch 49/90  |  Loss: 39.4621\n",
      "  ğŸ” Batch 50/90  |  Loss: 54.7148\n",
      "  ğŸ” Batch 51/90  |  Loss: 40.6490\n",
      "  ğŸ” Batch 52/90  |  Loss: 84.5536\n",
      "  ğŸ” Batch 53/90  |  Loss: 25.8889\n",
      "  ğŸ” Batch 54/90  |  Loss: 44.4736\n",
      "  ğŸ” Batch 55/90  |  Loss: 37.0838\n",
      "  ğŸ” Batch 56/90  |  Loss: 41.0376\n",
      "  ğŸ” Batch 57/90  |  Loss: 32.5748\n",
      "  ğŸ” Batch 58/90  |  Loss: 16.4468\n",
      "  ğŸ” Batch 59/90  |  Loss: 51.2573\n",
      "  ğŸ” Batch 60/90  |  Loss: 19.2053\n",
      "  ğŸ” Batch 61/90  |  Loss: 23.8217\n",
      "  ğŸ” Batch 62/90  |  Loss: 47.6673\n",
      "  ğŸ” Batch 63/90  |  Loss: 30.0755\n",
      "  ğŸ” Batch 64/90  |  Loss: 19.4482\n",
      "  ğŸ” Batch 65/90  |  Loss: 31.9461\n",
      "  ğŸ” Batch 66/90  |  Loss: 26.2364\n",
      "  ğŸ” Batch 67/90  |  Loss: 25.2610\n",
      "  ğŸ” Batch 68/90  |  Loss: 42.8312\n",
      "  ğŸ” Batch 69/90  |  Loss: 48.3598\n",
      "  ğŸ” Batch 70/90  |  Loss: 26.1366\n",
      "  ğŸ” Batch 71/90  |  Loss: 67.7599\n",
      "  ğŸ” Batch 72/90  |  Loss: 30.3447\n",
      "  ğŸ” Batch 73/90  |  Loss: 30.4401\n",
      "  ğŸ” Batch 74/90  |  Loss: 48.9110\n",
      "  ğŸ” Batch 75/90  |  Loss: 29.9691\n",
      "  ğŸ” Batch 76/90  |  Loss: 35.8256\n",
      "  ğŸ” Batch 77/90  |  Loss: 11.8152\n",
      "  ğŸ” Batch 78/90  |  Loss: 32.8587\n",
      "  ğŸ” Batch 79/90  |  Loss: 61.7417\n",
      "  ğŸ” Batch 80/90  |  Loss: 21.3903\n",
      "  ğŸ” Batch 81/90  |  Loss: 24.0090\n",
      "  ğŸ” Batch 82/90  |  Loss: 36.9366\n",
      "  ğŸ” Batch 83/90  |  Loss: 17.4569\n",
      "  ğŸ” Batch 84/90  |  Loss: 42.9565\n",
      "  ğŸ” Batch 85/90  |  Loss: 22.0652\n",
      "  ğŸ” Batch 86/90  |  Loss: 71.4349\n",
      "  ğŸ” Batch 87/90  |  Loss: 29.6301\n",
      "  ğŸ” Batch 88/90  |  Loss: 33.7297\n",
      "  ğŸ” Batch 89/90  |  Loss: 35.8918\n",
      "  ğŸ” Batch 90/90  |  Loss: 30.6097\n",
      "\n",
      "ğŸŒ€ Epoch 11/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 19.5654\n",
      "  ğŸ” Batch 2/90  |  Loss: 24.4587\n",
      "  ğŸ” Batch 3/90  |  Loss: 16.8440\n",
      "  ğŸ” Batch 4/90  |  Loss: 46.3590\n",
      "  ğŸ” Batch 5/90  |  Loss: 41.2305\n",
      "  ğŸ” Batch 6/90  |  Loss: 46.1616\n",
      "  ğŸ” Batch 7/90  |  Loss: 41.7571\n",
      "  ğŸ” Batch 8/90  |  Loss: 25.8764\n",
      "  ğŸ” Batch 9/90  |  Loss: 41.1919\n",
      "  ğŸ” Batch 10/90  |  Loss: 30.2566\n",
      "  ğŸ” Batch 11/90  |  Loss: 35.8893\n",
      "  ğŸ” Batch 12/90  |  Loss: 34.3151\n",
      "  ğŸ” Batch 13/90  |  Loss: 46.1006\n",
      "  ğŸ” Batch 14/90  |  Loss: 33.8721\n",
      "  ğŸ” Batch 15/90  |  Loss: 35.9920\n",
      "  ğŸ” Batch 16/90  |  Loss: 35.1077\n",
      "  ğŸ” Batch 17/90  |  Loss: 31.7081\n",
      "  ğŸ” Batch 18/90  |  Loss: 41.4159\n",
      "  ğŸ” Batch 19/90  |  Loss: 27.2747\n",
      "  ğŸ” Batch 20/90  |  Loss: 64.2149\n",
      "  ğŸ” Batch 21/90  |  Loss: 43.3806\n",
      "  ğŸ” Batch 22/90  |  Loss: 19.0819\n",
      "  ğŸ” Batch 23/90  |  Loss: 55.7620\n",
      "  ğŸ” Batch 24/90  |  Loss: 65.2536\n",
      "  ğŸ” Batch 25/90  |  Loss: 32.2831\n",
      "  ğŸ” Batch 26/90  |  Loss: 34.5900\n",
      "  ğŸ” Batch 27/90  |  Loss: 26.6452\n",
      "  ğŸ” Batch 28/90  |  Loss: 30.5299\n",
      "  ğŸ” Batch 29/90  |  Loss: 28.5037\n",
      "  ğŸ” Batch 30/90  |  Loss: 34.3105\n",
      "  ğŸ” Batch 31/90  |  Loss: 31.1177\n",
      "  ğŸ” Batch 32/90  |  Loss: 44.7339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 33/90  |  Loss: 26.7518\n",
      "  ğŸ” Batch 34/90  |  Loss: 45.2562\n",
      "  ğŸ” Batch 35/90  |  Loss: 31.2005\n",
      "  ğŸ” Batch 36/90  |  Loss: 47.6240\n",
      "  ğŸ” Batch 37/90  |  Loss: 43.2655\n",
      "  ğŸ” Batch 38/90  |  Loss: 25.1442\n",
      "  ğŸ” Batch 39/90  |  Loss: 40.1578\n",
      "  ğŸ” Batch 40/90  |  Loss: 37.2058\n",
      "  ğŸ” Batch 41/90  |  Loss: 21.1184\n",
      "  ğŸ” Batch 42/90  |  Loss: 34.7729\n",
      "  ğŸ” Batch 43/90  |  Loss: 26.0717\n",
      "  ğŸ” Batch 44/90  |  Loss: 47.2353\n",
      "  ğŸ” Batch 45/90  |  Loss: 33.0627\n",
      "  ğŸ” Batch 46/90  |  Loss: 9.2316\n",
      "  ğŸ” Batch 47/90  |  Loss: 36.0490\n",
      "  ğŸ” Batch 48/90  |  Loss: 31.5535\n",
      "  ğŸ” Batch 49/90  |  Loss: 60.0398\n",
      "  ğŸ” Batch 50/90  |  Loss: 39.7037\n",
      "  ğŸ” Batch 51/90  |  Loss: 39.4868\n",
      "  ğŸ” Batch 52/90  |  Loss: 16.9941\n",
      "  ğŸ” Batch 53/90  |  Loss: 25.4269\n",
      "  ğŸ” Batch 54/90  |  Loss: 21.5534\n",
      "  ğŸ” Batch 55/90  |  Loss: 28.4258\n",
      "  ğŸ” Batch 56/90  |  Loss: 20.6591\n",
      "  ğŸ” Batch 57/90  |  Loss: 23.8285\n",
      "  ğŸ” Batch 58/90  |  Loss: 20.1258\n",
      "  ğŸ” Batch 59/90  |  Loss: 34.9431\n",
      "  ğŸ” Batch 60/90  |  Loss: 34.6198\n",
      "  ğŸ” Batch 61/90  |  Loss: 27.9988\n",
      "  ğŸ” Batch 62/90  |  Loss: 35.1405\n",
      "  ğŸ” Batch 63/90  |  Loss: 23.5786\n",
      "  ğŸ” Batch 64/90  |  Loss: 42.7921\n",
      "  ğŸ” Batch 65/90  |  Loss: 22.9070\n",
      "  ğŸ” Batch 66/90  |  Loss: 26.2871\n",
      "  ğŸ” Batch 67/90  |  Loss: 45.4645\n",
      "  ğŸ” Batch 68/90  |  Loss: 43.0931\n",
      "  ğŸ” Batch 69/90  |  Loss: 26.4453\n",
      "  ğŸ” Batch 70/90  |  Loss: 55.4949\n",
      "  ğŸ” Batch 71/90  |  Loss: 34.2231\n",
      "  ğŸ” Batch 72/90  |  Loss: 14.2153\n",
      "  ğŸ” Batch 73/90  |  Loss: 40.4131\n",
      "  ğŸ” Batch 74/90  |  Loss: 41.4460\n",
      "  ğŸ” Batch 75/90  |  Loss: 52.0976\n",
      "  ğŸ” Batch 76/90  |  Loss: 25.9157\n",
      "  ğŸ” Batch 77/90  |  Loss: 23.5914\n",
      "  ğŸ” Batch 78/90  |  Loss: 29.4845\n",
      "  ğŸ” Batch 79/90  |  Loss: 34.0743\n",
      "  ğŸ” Batch 80/90  |  Loss: 35.9963\n",
      "  ğŸ” Batch 81/90  |  Loss: 32.5712\n",
      "  ğŸ” Batch 82/90  |  Loss: 30.9402\n",
      "  ğŸ” Batch 83/90  |  Loss: 13.8833\n",
      "  ğŸ” Batch 84/90  |  Loss: 41.2181\n",
      "  ğŸ” Batch 85/90  |  Loss: 40.6406\n",
      "  ğŸ” Batch 86/90  |  Loss: 31.2362\n",
      "  ğŸ” Batch 87/90  |  Loss: 35.2758\n",
      "  ğŸ” Batch 88/90  |  Loss: 29.6244\n",
      "  ğŸ” Batch 89/90  |  Loss: 40.5643\n",
      "  ğŸ” Batch 90/90  |  Loss: 33.7368\n",
      "\n",
      "ğŸŒ€ Epoch 12/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 18.0858\n",
      "  ğŸ” Batch 2/90  |  Loss: 45.8175\n",
      "  ğŸ” Batch 3/90  |  Loss: 42.2485\n",
      "  ğŸ” Batch 4/90  |  Loss: 26.6453\n",
      "  ğŸ” Batch 5/90  |  Loss: 29.2343\n",
      "  ğŸ” Batch 6/90  |  Loss: 31.8520\n",
      "  ğŸ” Batch 7/90  |  Loss: 49.8359\n",
      "  ğŸ” Batch 8/90  |  Loss: 36.2192\n",
      "  ğŸ” Batch 9/90  |  Loss: 27.8768\n",
      "  ğŸ” Batch 10/90  |  Loss: 35.0576\n",
      "  ğŸ” Batch 11/90  |  Loss: 28.0424\n",
      "  ğŸ” Batch 12/90  |  Loss: 30.0071\n",
      "  ğŸ” Batch 13/90  |  Loss: 50.8217\n",
      "  ğŸ” Batch 14/90  |  Loss: 48.7103\n",
      "  ğŸ” Batch 15/90  |  Loss: 26.1770\n",
      "  ğŸ” Batch 16/90  |  Loss: 20.4651\n",
      "  ğŸ” Batch 17/90  |  Loss: 36.0271\n",
      "  ğŸ” Batch 18/90  |  Loss: 17.3402\n",
      "  ğŸ” Batch 19/90  |  Loss: 60.3232\n",
      "  ğŸ” Batch 20/90  |  Loss: 25.8522\n",
      "  ğŸ” Batch 21/90  |  Loss: 17.6625\n",
      "  ğŸ” Batch 22/90  |  Loss: 40.2367\n",
      "  ğŸ” Batch 23/90  |  Loss: 20.1811\n",
      "  ğŸ” Batch 24/90  |  Loss: 37.4929\n",
      "  ğŸ” Batch 25/90  |  Loss: 15.1131\n",
      "  ğŸ” Batch 26/90  |  Loss: 23.3325\n",
      "  ğŸ” Batch 27/90  |  Loss: 21.3639\n",
      "  ğŸ” Batch 28/90  |  Loss: 36.9331\n",
      "  ğŸ” Batch 29/90  |  Loss: 29.4806\n",
      "  ğŸ” Batch 30/90  |  Loss: 16.1250\n",
      "  ğŸ” Batch 31/90  |  Loss: 23.1586\n",
      "  ğŸ” Batch 32/90  |  Loss: 31.1599\n",
      "  ğŸ” Batch 33/90  |  Loss: 37.1024\n",
      "  ğŸ” Batch 34/90  |  Loss: 43.1090\n",
      "  ğŸ” Batch 35/90  |  Loss: 43.0134\n",
      "  ğŸ” Batch 36/90  |  Loss: 25.1237\n",
      "  ğŸ” Batch 37/90  |  Loss: 19.2394\n",
      "  ğŸ” Batch 38/90  |  Loss: 34.9901\n",
      "  ğŸ” Batch 39/90  |  Loss: 18.4857\n",
      "  ğŸ” Batch 40/90  |  Loss: 31.6463\n",
      "  ğŸ” Batch 41/90  |  Loss: 17.7383\n",
      "  ğŸ” Batch 42/90  |  Loss: 14.0390\n",
      "  ğŸ” Batch 43/90  |  Loss: 22.8487\n",
      "  ğŸ” Batch 44/90  |  Loss: 17.8427\n",
      "  ğŸ” Batch 45/90  |  Loss: 21.4733\n",
      "  ğŸ” Batch 46/90  |  Loss: 17.5770\n",
      "  ğŸ” Batch 47/90  |  Loss: 39.1180\n",
      "  ğŸ” Batch 48/90  |  Loss: 32.9862\n",
      "  ğŸ” Batch 49/90  |  Loss: 25.9821\n",
      "  ğŸ” Batch 50/90  |  Loss: 39.6665\n",
      "  ğŸ” Batch 51/90  |  Loss: 20.2790\n",
      "  ğŸ” Batch 52/90  |  Loss: 40.2337\n",
      "  ğŸ” Batch 53/90  |  Loss: 18.5176\n",
      "  ğŸ” Batch 54/90  |  Loss: 56.2974\n",
      "  ğŸ” Batch 55/90  |  Loss: 55.9212\n",
      "  ğŸ” Batch 56/90  |  Loss: 25.1836\n",
      "  ğŸ” Batch 57/90  |  Loss: 22.2224\n",
      "  ğŸ” Batch 58/90  |  Loss: 55.4490\n",
      "  ğŸ” Batch 59/90  |  Loss: 45.2387\n",
      "  ğŸ” Batch 60/90  |  Loss: 47.3037\n",
      "  ğŸ” Batch 61/90  |  Loss: 31.6350\n",
      "  ğŸ” Batch 62/90  |  Loss: 17.8108\n",
      "  ğŸ” Batch 63/90  |  Loss: 20.4031\n",
      "  ğŸ” Batch 64/90  |  Loss: 24.5355\n",
      "  ğŸ” Batch 65/90  |  Loss: 16.3469\n",
      "  ğŸ” Batch 66/90  |  Loss: 44.8973\n",
      "  ğŸ” Batch 67/90  |  Loss: 19.1600\n",
      "  ğŸ” Batch 68/90  |  Loss: 30.6609\n",
      "  ğŸ” Batch 69/90  |  Loss: 42.0604\n",
      "  ğŸ” Batch 70/90  |  Loss: 51.2807\n",
      "  ğŸ” Batch 71/90  |  Loss: 46.8025\n",
      "  ğŸ” Batch 72/90  |  Loss: 42.2666\n",
      "  ğŸ” Batch 73/90  |  Loss: 34.9151\n",
      "  ğŸ” Batch 74/90  |  Loss: 28.8944\n",
      "  ğŸ” Batch 75/90  |  Loss: 44.2121\n",
      "  ğŸ” Batch 76/90  |  Loss: 25.8738\n",
      "  ğŸ” Batch 77/90  |  Loss: 60.4290\n",
      "  ğŸ” Batch 78/90  |  Loss: 28.0687\n",
      "  ğŸ” Batch 79/90  |  Loss: 24.3971\n",
      "  ğŸ” Batch 80/90  |  Loss: 67.4727\n",
      "  ğŸ” Batch 81/90  |  Loss: 25.3225\n",
      "  ğŸ” Batch 82/90  |  Loss: 32.8292\n",
      "  ğŸ” Batch 83/90  |  Loss: 42.4297\n",
      "  ğŸ” Batch 84/90  |  Loss: 39.7004\n",
      "  ğŸ” Batch 85/90  |  Loss: 15.1597\n",
      "  ğŸ” Batch 86/90  |  Loss: 24.7466\n",
      "  ğŸ” Batch 87/90  |  Loss: 57.0778\n",
      "  ğŸ” Batch 88/90  |  Loss: 41.9648\n",
      "  ğŸ” Batch 89/90  |  Loss: 42.8974\n",
      "  ğŸ” Batch 90/90  |  Loss: 40.9138\n",
      "\n",
      "ğŸŒ€ Epoch 13/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 22.9263\n",
      "  ğŸ” Batch 2/90  |  Loss: 26.1251\n",
      "  ğŸ” Batch 3/90  |  Loss: 16.7150\n",
      "  ğŸ” Batch 4/90  |  Loss: 38.5993\n",
      "  ğŸ” Batch 5/90  |  Loss: 64.2978\n",
      "  ğŸ” Batch 6/90  |  Loss: 37.2736\n",
      "  ğŸ” Batch 7/90  |  Loss: 43.6679\n",
      "  ğŸ” Batch 8/90  |  Loss: 44.9039\n",
      "  ğŸ” Batch 9/90  |  Loss: 57.3859\n",
      "  ğŸ” Batch 10/90  |  Loss: 15.0389\n",
      "  ğŸ” Batch 11/90  |  Loss: 21.0003\n",
      "  ğŸ” Batch 12/90  |  Loss: 16.3250\n",
      "  ğŸ” Batch 13/90  |  Loss: 41.5481\n",
      "  ğŸ” Batch 14/90  |  Loss: 17.1233\n",
      "  ğŸ” Batch 15/90  |  Loss: 41.4560\n",
      "  ğŸ” Batch 16/90  |  Loss: 83.2225\n",
      "  ğŸ” Batch 17/90  |  Loss: 62.7599\n",
      "  ğŸ” Batch 18/90  |  Loss: 43.6256\n",
      "  ğŸ” Batch 19/90  |  Loss: 17.4927\n",
      "  ğŸ” Batch 20/90  |  Loss: 48.5498\n",
      "  ğŸ” Batch 21/90  |  Loss: 11.7379\n",
      "  ğŸ” Batch 22/90  |  Loss: 45.9228\n",
      "  ğŸ” Batch 23/90  |  Loss: 23.4018\n",
      "  ğŸ” Batch 24/90  |  Loss: 17.3220\n",
      "  ğŸ” Batch 25/90  |  Loss: 31.0093\n",
      "  ğŸ” Batch 26/90  |  Loss: 49.8810\n",
      "  ğŸ” Batch 27/90  |  Loss: 23.8986\n",
      "  ğŸ” Batch 28/90  |  Loss: 22.3605\n",
      "  ğŸ” Batch 29/90  |  Loss: 37.4599\n",
      "  ğŸ” Batch 30/90  |  Loss: 21.4198\n",
      "  ğŸ” Batch 31/90  |  Loss: 51.9132\n",
      "  ğŸ” Batch 32/90  |  Loss: 14.4130\n",
      "  ğŸ” Batch 33/90  |  Loss: 31.1282\n",
      "  ğŸ” Batch 34/90  |  Loss: 44.3947\n",
      "  ğŸ” Batch 35/90  |  Loss: 26.8141\n",
      "  ğŸ” Batch 36/90  |  Loss: 24.5704\n",
      "  ğŸ” Batch 37/90  |  Loss: 12.5707\n",
      "  ğŸ” Batch 38/90  |  Loss: 32.2261\n",
      "  ğŸ” Batch 39/90  |  Loss: 39.0072\n",
      "  ğŸ” Batch 40/90  |  Loss: 12.6354\n",
      "  ğŸ” Batch 41/90  |  Loss: 18.8538\n",
      "  ğŸ” Batch 42/90  |  Loss: 45.9919\n",
      "  ğŸ” Batch 43/90  |  Loss: 29.8855\n",
      "  ğŸ” Batch 44/90  |  Loss: 42.4159\n",
      "  ğŸ” Batch 45/90  |  Loss: 41.3904\n",
      "  ğŸ” Batch 46/90  |  Loss: 27.7300\n",
      "  ğŸ” Batch 47/90  |  Loss: 39.2534\n",
      "  ğŸ” Batch 48/90  |  Loss: 34.7465\n",
      "  ğŸ” Batch 49/90  |  Loss: 19.3929\n",
      "  ğŸ” Batch 50/90  |  Loss: 45.5958\n",
      "  ğŸ” Batch 51/90  |  Loss: 43.5398\n",
      "  ğŸ” Batch 52/90  |  Loss: 32.4013\n",
      "  ğŸ” Batch 53/90  |  Loss: 33.7023\n",
      "  ğŸ” Batch 54/90  |  Loss: 17.1996\n",
      "  ğŸ” Batch 55/90  |  Loss: 38.8563\n",
      "  ğŸ” Batch 56/90  |  Loss: 43.0975\n",
      "  ğŸ” Batch 57/90  |  Loss: 32.3919\n",
      "  ğŸ” Batch 58/90  |  Loss: 43.3220\n",
      "  ğŸ” Batch 59/90  |  Loss: 24.7843\n",
      "  ğŸ” Batch 60/90  |  Loss: 25.6188\n",
      "  ğŸ” Batch 61/90  |  Loss: 24.0816\n",
      "  ğŸ” Batch 62/90  |  Loss: 32.9225\n",
      "  ğŸ” Batch 63/90  |  Loss: 23.2165\n",
      "  ğŸ” Batch 64/90  |  Loss: 43.6948\n",
      "  ğŸ” Batch 65/90  |  Loss: 41.8573\n",
      "  ğŸ” Batch 66/90  |  Loss: 28.0898\n",
      "  ğŸ” Batch 67/90  |  Loss: 18.5703\n",
      "  ğŸ” Batch 68/90  |  Loss: 39.3212\n",
      "  ğŸ” Batch 69/90  |  Loss: 34.4723\n",
      "  ğŸ” Batch 70/90  |  Loss: 27.9946\n",
      "  ğŸ” Batch 71/90  |  Loss: 24.2452\n",
      "  ğŸ” Batch 72/90  |  Loss: 18.5806\n",
      "  ğŸ” Batch 73/90  |  Loss: 50.1891\n",
      "  ğŸ” Batch 74/90  |  Loss: 49.6258\n",
      "  ğŸ” Batch 75/90  |  Loss: 43.8580\n",
      "  ğŸ” Batch 76/90  |  Loss: 49.5781\n",
      "  ğŸ” Batch 77/90  |  Loss: 71.5611\n",
      "  ğŸ” Batch 78/90  |  Loss: 38.8638\n",
      "  ğŸ” Batch 79/90  |  Loss: 32.3992\n",
      "  ğŸ” Batch 80/90  |  Loss: 15.7660\n",
      "  ğŸ” Batch 81/90  |  Loss: 32.6152\n",
      "  ğŸ” Batch 82/90  |  Loss: 14.1192\n",
      "  ğŸ” Batch 83/90  |  Loss: 36.1875\n",
      "  ğŸ” Batch 84/90  |  Loss: 20.3937\n",
      "  ğŸ” Batch 85/90  |  Loss: 46.9318\n",
      "  ğŸ” Batch 86/90  |  Loss: 42.1138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 87/90  |  Loss: 34.1722\n",
      "  ğŸ” Batch 88/90  |  Loss: 26.5620\n",
      "  ğŸ” Batch 89/90  |  Loss: 27.8569\n",
      "  ğŸ” Batch 90/90  |  Loss: 13.3883\n",
      "\n",
      "ğŸŒ€ Epoch 14/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 17.2401\n",
      "  ğŸ” Batch 2/90  |  Loss: 28.0318\n",
      "  ğŸ” Batch 3/90  |  Loss: 38.0662\n",
      "  ğŸ” Batch 4/90  |  Loss: 40.8195\n",
      "  ğŸ” Batch 5/90  |  Loss: 31.8189\n",
      "  ğŸ” Batch 6/90  |  Loss: 33.2877\n",
      "  ğŸ” Batch 7/90  |  Loss: 57.8924\n",
      "  ğŸ” Batch 8/90  |  Loss: 26.1997\n",
      "  ğŸ” Batch 9/90  |  Loss: 39.3231\n",
      "  ğŸ” Batch 10/90  |  Loss: 18.6482\n",
      "  ğŸ” Batch 11/90  |  Loss: 30.5321\n",
      "  ğŸ” Batch 12/90  |  Loss: 28.8739\n",
      "  ğŸ” Batch 13/90  |  Loss: 41.4587\n",
      "  ğŸ” Batch 14/90  |  Loss: 54.4390\n",
      "  ğŸ” Batch 15/90  |  Loss: 21.5133\n",
      "  ğŸ” Batch 16/90  |  Loss: 16.0625\n",
      "  ğŸ” Batch 17/90  |  Loss: 50.0350\n",
      "  ğŸ” Batch 18/90  |  Loss: 26.6634\n",
      "  ğŸ” Batch 19/90  |  Loss: 16.7548\n",
      "  ğŸ” Batch 20/90  |  Loss: 22.8135\n",
      "  ğŸ” Batch 21/90  |  Loss: 34.3850\n",
      "  ğŸ” Batch 22/90  |  Loss: 31.7829\n",
      "  ğŸ” Batch 23/90  |  Loss: 36.3451\n",
      "  ğŸ” Batch 24/90  |  Loss: 37.5911\n",
      "  ğŸ” Batch 25/90  |  Loss: 32.8937\n",
      "  ğŸ” Batch 26/90  |  Loss: 20.5821\n",
      "  ğŸ” Batch 27/90  |  Loss: 16.7452\n",
      "  ğŸ” Batch 28/90  |  Loss: 39.3492\n",
      "  ğŸ” Batch 29/90  |  Loss: 37.2622\n",
      "  ğŸ” Batch 30/90  |  Loss: 37.8397\n",
      "  ğŸ” Batch 31/90  |  Loss: 13.8016\n",
      "  ğŸ” Batch 32/90  |  Loss: 50.4228\n",
      "  ğŸ” Batch 33/90  |  Loss: 20.3493\n",
      "  ğŸ” Batch 34/90  |  Loss: 37.0965\n",
      "  ğŸ” Batch 35/90  |  Loss: 40.7586\n",
      "  ğŸ” Batch 36/90  |  Loss: 23.6253\n",
      "  ğŸ” Batch 37/90  |  Loss: 36.0648\n",
      "  ğŸ” Batch 38/90  |  Loss: 30.1095\n",
      "  ğŸ” Batch 39/90  |  Loss: 45.3024\n",
      "  ğŸ” Batch 40/90  |  Loss: 32.7170\n",
      "  ğŸ” Batch 41/90  |  Loss: 24.7144\n",
      "  ğŸ” Batch 42/90  |  Loss: 11.5256\n",
      "  ğŸ” Batch 43/90  |  Loss: 48.6908\n",
      "  ğŸ” Batch 44/90  |  Loss: 27.9532\n",
      "  ğŸ” Batch 45/90  |  Loss: 50.7660\n",
      "  ğŸ” Batch 46/90  |  Loss: 41.5382\n",
      "  ğŸ” Batch 47/90  |  Loss: 37.2861\n",
      "  ğŸ” Batch 48/90  |  Loss: 31.9532\n",
      "  ğŸ” Batch 49/90  |  Loss: 62.8552\n",
      "  ğŸ” Batch 50/90  |  Loss: 25.3385\n",
      "  ğŸ” Batch 51/90  |  Loss: 16.1149\n",
      "  ğŸ” Batch 52/90  |  Loss: 65.7633\n",
      "  ğŸ” Batch 53/90  |  Loss: 19.2072\n",
      "  ğŸ” Batch 54/90  |  Loss: 21.1540\n",
      "  ğŸ” Batch 55/90  |  Loss: 28.9442\n",
      "  ğŸ” Batch 56/90  |  Loss: 23.8799\n",
      "  ğŸ” Batch 57/90  |  Loss: 30.8418\n",
      "  ğŸ” Batch 58/90  |  Loss: 14.6236\n",
      "  ğŸ” Batch 59/90  |  Loss: 22.9199\n",
      "  ğŸ” Batch 60/90  |  Loss: 44.8462\n",
      "  ğŸ” Batch 61/90  |  Loss: 33.3378\n",
      "  ğŸ” Batch 62/90  |  Loss: 42.9769\n",
      "  ğŸ” Batch 63/90  |  Loss: 13.9663\n",
      "  ğŸ” Batch 64/90  |  Loss: 36.9168\n",
      "  ğŸ” Batch 65/90  |  Loss: 46.0196\n",
      "  ğŸ” Batch 66/90  |  Loss: 7.7133\n",
      "  ğŸ” Batch 67/90  |  Loss: 24.0370\n",
      "  ğŸ” Batch 68/90  |  Loss: 29.6593\n",
      "  ğŸ” Batch 69/90  |  Loss: 35.7382\n",
      "  ğŸ” Batch 70/90  |  Loss: 19.7045\n",
      "  ğŸ” Batch 71/90  |  Loss: 37.8333\n",
      "  ğŸ” Batch 72/90  |  Loss: 31.0794\n",
      "  ğŸ” Batch 73/90  |  Loss: 53.3026\n",
      "  ğŸ” Batch 74/90  |  Loss: 29.8721\n",
      "  ğŸ” Batch 75/90  |  Loss: 30.3136\n",
      "  ğŸ” Batch 76/90  |  Loss: 41.5339\n",
      "  ğŸ” Batch 77/90  |  Loss: 34.1281\n",
      "  ğŸ” Batch 78/90  |  Loss: 14.8912\n",
      "  ğŸ” Batch 79/90  |  Loss: 30.0959\n",
      "  ğŸ” Batch 80/90  |  Loss: 32.7295\n",
      "  ğŸ” Batch 81/90  |  Loss: 17.6995\n",
      "  ğŸ” Batch 82/90  |  Loss: 33.6787\n",
      "  ğŸ” Batch 83/90  |  Loss: 19.4063\n",
      "  ğŸ” Batch 84/90  |  Loss: 29.8205\n",
      "  ğŸ” Batch 85/90  |  Loss: 30.9570\n",
      "  ğŸ” Batch 86/90  |  Loss: 25.5002\n",
      "  ğŸ” Batch 87/90  |  Loss: 23.1579\n",
      "  ğŸ” Batch 88/90  |  Loss: 15.2809\n",
      "  ğŸ” Batch 89/90  |  Loss: 16.6329\n",
      "  ğŸ” Batch 90/90  |  Loss: 43.5244\n",
      "\n",
      "ğŸŒ€ Epoch 15/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 40.5400\n",
      "  ğŸ” Batch 2/90  |  Loss: 30.9228\n",
      "  ğŸ” Batch 3/90  |  Loss: 19.2866\n",
      "  ğŸ” Batch 4/90  |  Loss: 30.0573\n",
      "  ğŸ” Batch 5/90  |  Loss: 20.1700\n",
      "  ğŸ” Batch 6/90  |  Loss: 12.8214\n",
      "  ğŸ” Batch 7/90  |  Loss: 32.7494\n",
      "  ğŸ” Batch 8/90  |  Loss: 61.1685\n",
      "  ğŸ” Batch 9/90  |  Loss: 13.0912\n",
      "  ğŸ” Batch 10/90  |  Loss: 47.3937\n",
      "  ğŸ” Batch 11/90  |  Loss: 54.5120\n",
      "  ğŸ” Batch 12/90  |  Loss: 49.8950\n",
      "  ğŸ” Batch 13/90  |  Loss: 35.3702\n",
      "  ğŸ” Batch 14/90  |  Loss: 38.2421\n",
      "  ğŸ” Batch 15/90  |  Loss: 17.0562\n",
      "  ğŸ” Batch 16/90  |  Loss: 47.7542\n",
      "  ğŸ” Batch 17/90  |  Loss: 27.7713\n",
      "  ğŸ” Batch 18/90  |  Loss: 14.6876\n",
      "  ğŸ” Batch 19/90  |  Loss: 16.4334\n",
      "  ğŸ” Batch 20/90  |  Loss: 13.9367\n",
      "  ğŸ” Batch 21/90  |  Loss: 27.3129\n",
      "  ğŸ” Batch 22/90  |  Loss: 54.8456\n",
      "  ğŸ” Batch 23/90  |  Loss: 11.7416\n",
      "  ğŸ” Batch 24/90  |  Loss: 19.9057\n",
      "  ğŸ” Batch 25/90  |  Loss: 24.3197\n",
      "  ğŸ” Batch 26/90  |  Loss: 39.7761\n",
      "  ğŸ” Batch 27/90  |  Loss: 14.4046\n",
      "  ğŸ” Batch 28/90  |  Loss: 37.2563\n",
      "  ğŸ” Batch 29/90  |  Loss: 7.5044\n",
      "  ğŸ” Batch 30/90  |  Loss: 35.9576\n",
      "  ğŸ” Batch 31/90  |  Loss: 14.5072\n",
      "  ğŸ” Batch 32/90  |  Loss: 43.2817\n",
      "  ğŸ” Batch 33/90  |  Loss: 40.6689\n",
      "  ğŸ” Batch 34/90  |  Loss: 39.2864\n",
      "  ğŸ” Batch 35/90  |  Loss: 27.7651\n",
      "  ğŸ” Batch 36/90  |  Loss: 23.4455\n",
      "  ğŸ” Batch 37/90  |  Loss: 38.0310\n",
      "  ğŸ” Batch 38/90  |  Loss: 40.1793\n",
      "  ğŸ” Batch 39/90  |  Loss: 43.3107\n",
      "  ğŸ” Batch 40/90  |  Loss: 51.8811\n",
      "  ğŸ” Batch 41/90  |  Loss: 25.8158\n",
      "  ğŸ” Batch 42/90  |  Loss: 15.8271\n",
      "  ğŸ” Batch 43/90  |  Loss: 28.7553\n",
      "  ğŸ” Batch 44/90  |  Loss: 28.0802\n",
      "  ğŸ” Batch 45/90  |  Loss: 19.6742\n",
      "  ğŸ” Batch 46/90  |  Loss: 20.7629\n",
      "  ğŸ” Batch 47/90  |  Loss: 26.5044\n",
      "  ğŸ” Batch 48/90  |  Loss: 21.0122\n",
      "  ğŸ” Batch 49/90  |  Loss: 36.6093\n",
      "  ğŸ” Batch 50/90  |  Loss: 21.9598\n",
      "  ğŸ” Batch 51/90  |  Loss: 34.0394\n",
      "  ğŸ” Batch 52/90  |  Loss: 14.2800\n",
      "  ğŸ” Batch 53/90  |  Loss: 27.1480\n",
      "  ğŸ” Batch 54/90  |  Loss: 40.6675\n",
      "  ğŸ” Batch 55/90  |  Loss: 36.4729\n",
      "  ğŸ” Batch 56/90  |  Loss: 29.9297\n",
      "  ğŸ” Batch 57/90  |  Loss: 53.5345\n",
      "  ğŸ” Batch 58/90  |  Loss: 27.0474\n",
      "  ğŸ” Batch 59/90  |  Loss: 51.0386\n",
      "  ğŸ” Batch 60/90  |  Loss: 29.0096\n",
      "  ğŸ” Batch 61/90  |  Loss: 34.0406\n",
      "  ğŸ” Batch 62/90  |  Loss: 25.2015\n",
      "  ğŸ” Batch 63/90  |  Loss: 22.0725\n",
      "  ğŸ” Batch 64/90  |  Loss: 41.0938\n",
      "  ğŸ” Batch 65/90  |  Loss: 30.8759\n",
      "  ğŸ” Batch 66/90  |  Loss: 18.9909\n",
      "  ğŸ” Batch 67/90  |  Loss: 30.7287\n",
      "  ğŸ” Batch 68/90  |  Loss: 18.0958\n",
      "  ğŸ” Batch 69/90  |  Loss: 27.7112\n",
      "  ğŸ” Batch 70/90  |  Loss: 19.2906\n",
      "  ğŸ” Batch 71/90  |  Loss: 25.6533\n",
      "  ğŸ” Batch 72/90  |  Loss: 53.6705\n",
      "  ğŸ” Batch 73/90  |  Loss: 35.8755\n",
      "  ğŸ” Batch 74/90  |  Loss: 36.8597\n",
      "  ğŸ” Batch 75/90  |  Loss: 16.0378\n",
      "  ğŸ” Batch 76/90  |  Loss: 54.6259\n",
      "  ğŸ” Batch 77/90  |  Loss: 40.1635\n",
      "  ğŸ” Batch 78/90  |  Loss: 27.8838\n",
      "  ğŸ” Batch 79/90  |  Loss: 17.7987\n",
      "  ğŸ” Batch 80/90  |  Loss: 10.6027\n",
      "  ğŸ” Batch 81/90  |  Loss: 24.6729\n",
      "  ğŸ” Batch 82/90  |  Loss: 25.1533\n",
      "  ğŸ” Batch 83/90  |  Loss: 45.3325\n",
      "  ğŸ” Batch 84/90  |  Loss: 37.3315\n",
      "  ğŸ” Batch 85/90  |  Loss: 23.3157\n",
      "  ğŸ” Batch 86/90  |  Loss: 65.0592\n",
      "  ğŸ” Batch 87/90  |  Loss: 38.1104\n",
      "  ğŸ” Batch 88/90  |  Loss: 41.6002\n",
      "  ğŸ” Batch 89/90  |  Loss: 30.4177\n",
      "  ğŸ” Batch 90/90  |  Loss: 37.2431\n",
      "\n",
      "ğŸŒ€ Epoch 16/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 5.6581\n",
      "  ğŸ” Batch 2/90  |  Loss: 13.1666\n",
      "  ğŸ” Batch 3/90  |  Loss: 20.6152\n",
      "  ğŸ” Batch 4/90  |  Loss: 20.8204\n",
      "  ğŸ” Batch 5/90  |  Loss: 25.6743\n",
      "  ğŸ” Batch 6/90  |  Loss: 21.1648\n",
      "  ğŸ” Batch 7/90  |  Loss: 45.0621\n",
      "  ğŸ” Batch 8/90  |  Loss: 32.8170\n",
      "  ğŸ” Batch 9/90  |  Loss: 19.6163\n",
      "  ğŸ” Batch 10/90  |  Loss: 12.9685\n",
      "  ğŸ” Batch 11/90  |  Loss: 27.1915\n",
      "  ğŸ” Batch 12/90  |  Loss: 41.4956\n",
      "  ğŸ” Batch 13/90  |  Loss: 17.5256\n",
      "  ğŸ” Batch 14/90  |  Loss: 17.4023\n",
      "  ğŸ” Batch 15/90  |  Loss: 27.6129\n",
      "  ğŸ” Batch 16/90  |  Loss: 52.3957\n",
      "  ğŸ” Batch 17/90  |  Loss: 24.2874\n",
      "  ğŸ” Batch 18/90  |  Loss: 25.8710\n",
      "  ğŸ” Batch 19/90  |  Loss: 16.3579\n",
      "  ğŸ” Batch 20/90  |  Loss: 25.5276\n",
      "  ğŸ” Batch 21/90  |  Loss: 44.1257\n",
      "  ğŸ” Batch 22/90  |  Loss: 25.0642\n",
      "  ğŸ” Batch 23/90  |  Loss: 39.2926\n",
      "  ğŸ” Batch 24/90  |  Loss: 32.6378\n",
      "  ğŸ” Batch 25/90  |  Loss: 41.2715\n",
      "  ğŸ” Batch 26/90  |  Loss: 25.3766\n",
      "  ğŸ” Batch 27/90  |  Loss: 19.3358\n",
      "  ğŸ” Batch 28/90  |  Loss: 62.9971\n",
      "  ğŸ” Batch 29/90  |  Loss: 64.6050\n",
      "  ğŸ” Batch 30/90  |  Loss: 12.6727\n",
      "  ğŸ” Batch 31/90  |  Loss: 42.9407\n",
      "  ğŸ” Batch 32/90  |  Loss: 30.1454\n",
      "  ğŸ” Batch 33/90  |  Loss: 37.4081\n",
      "  ğŸ” Batch 34/90  |  Loss: 45.9049\n",
      "  ğŸ” Batch 35/90  |  Loss: 40.8595\n",
      "  ğŸ” Batch 36/90  |  Loss: 44.8756\n",
      "  ğŸ” Batch 37/90  |  Loss: 61.3923\n",
      "  ğŸ” Batch 38/90  |  Loss: 17.4640\n",
      "  ğŸ” Batch 39/90  |  Loss: 13.1639\n",
      "  ğŸ” Batch 40/90  |  Loss: 20.1251\n",
      "  ğŸ” Batch 41/90  |  Loss: 25.4693\n",
      "  ğŸ” Batch 42/90  |  Loss: 33.4596\n",
      "  ğŸ” Batch 43/90  |  Loss: 14.1089\n",
      "  ğŸ” Batch 44/90  |  Loss: 40.3837\n",
      "  ğŸ” Batch 45/90  |  Loss: 45.2343\n",
      "  ğŸ” Batch 46/90  |  Loss: 43.4462\n",
      "  ğŸ” Batch 47/90  |  Loss: 42.5460\n",
      "  ğŸ” Batch 48/90  |  Loss: 10.5962\n",
      "  ğŸ” Batch 49/90  |  Loss: 15.6312\n",
      "  ğŸ” Batch 50/90  |  Loss: 28.1926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 51/90  |  Loss: 48.1432\n",
      "  ğŸ” Batch 52/90  |  Loss: 23.8178\n",
      "  ğŸ” Batch 53/90  |  Loss: 17.2280\n",
      "  ğŸ” Batch 54/90  |  Loss: 17.3741\n",
      "  ğŸ” Batch 55/90  |  Loss: 19.8234\n",
      "  ğŸ” Batch 56/90  |  Loss: 53.0913\n",
      "  ğŸ” Batch 57/90  |  Loss: 37.4415\n",
      "  ğŸ” Batch 58/90  |  Loss: 37.3817\n",
      "  ğŸ” Batch 59/90  |  Loss: 17.0999\n",
      "  ğŸ” Batch 60/90  |  Loss: 37.9023\n",
      "  ğŸ” Batch 61/90  |  Loss: 33.2010\n",
      "  ğŸ” Batch 62/90  |  Loss: 67.8523\n",
      "  ğŸ” Batch 63/90  |  Loss: 20.0402\n",
      "  ğŸ” Batch 64/90  |  Loss: 47.5721\n",
      "  ğŸ” Batch 65/90  |  Loss: 45.1194\n",
      "  ğŸ” Batch 66/90  |  Loss: 22.8312\n",
      "  ğŸ” Batch 67/90  |  Loss: 13.5329\n",
      "  ğŸ” Batch 68/90  |  Loss: 19.1485\n",
      "  ğŸ” Batch 69/90  |  Loss: 39.8029\n",
      "  ğŸ” Batch 70/90  |  Loss: 21.6467\n",
      "  ğŸ” Batch 71/90  |  Loss: 55.9161\n",
      "  ğŸ” Batch 72/90  |  Loss: 30.4877\n",
      "  ğŸ” Batch 73/90  |  Loss: 20.7721\n",
      "  ğŸ” Batch 74/90  |  Loss: 25.9492\n",
      "  ğŸ” Batch 75/90  |  Loss: 21.5214\n",
      "  ğŸ” Batch 76/90  |  Loss: 34.3954\n",
      "  ğŸ” Batch 77/90  |  Loss: 46.9880\n",
      "  ğŸ” Batch 78/90  |  Loss: 31.8774\n",
      "  ğŸ” Batch 79/90  |  Loss: 28.5427\n",
      "  ğŸ” Batch 80/90  |  Loss: 29.8321\n",
      "  ğŸ” Batch 81/90  |  Loss: 17.4978\n",
      "  ğŸ” Batch 82/90  |  Loss: 19.4603\n",
      "  ğŸ” Batch 83/90  |  Loss: 36.9140\n",
      "  ğŸ” Batch 84/90  |  Loss: 6.5160\n",
      "  ğŸ” Batch 85/90  |  Loss: 23.9679\n",
      "  ğŸ” Batch 86/90  |  Loss: 17.8178\n",
      "  ğŸ” Batch 87/90  |  Loss: 31.3835\n",
      "  ğŸ” Batch 88/90  |  Loss: 23.0129\n",
      "  ğŸ” Batch 89/90  |  Loss: 13.3984\n",
      "  ğŸ” Batch 90/90  |  Loss: 30.5848\n",
      "\n",
      "ğŸŒ€ Epoch 17/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 18.6705\n",
      "  ğŸ” Batch 2/90  |  Loss: 18.1056\n",
      "  ğŸ” Batch 3/90  |  Loss: 52.2703\n",
      "  ğŸ” Batch 4/90  |  Loss: 42.2410\n",
      "  ğŸ” Batch 5/90  |  Loss: 60.7320\n",
      "  ğŸ” Batch 6/90  |  Loss: 43.4454\n",
      "  ğŸ” Batch 7/90  |  Loss: 25.1577\n",
      "  ğŸ” Batch 8/90  |  Loss: 31.1229\n",
      "  ğŸ” Batch 9/90  |  Loss: 20.2141\n",
      "  ğŸ” Batch 10/90  |  Loss: 42.4529\n",
      "  ğŸ” Batch 11/90  |  Loss: 19.0305\n",
      "  ğŸ” Batch 12/90  |  Loss: 31.9038\n",
      "  ğŸ” Batch 13/90  |  Loss: 22.8116\n",
      "  ğŸ” Batch 14/90  |  Loss: 24.2458\n",
      "  ğŸ” Batch 15/90  |  Loss: 33.7445\n",
      "  ğŸ” Batch 16/90  |  Loss: 14.4006\n",
      "  ğŸ” Batch 17/90  |  Loss: 20.0590\n",
      "  ğŸ” Batch 18/90  |  Loss: 20.4055\n",
      "  ğŸ” Batch 19/90  |  Loss: 46.9148\n",
      "  ğŸ” Batch 20/90  |  Loss: 26.4417\n",
      "  ğŸ” Batch 21/90  |  Loss: 23.8360\n",
      "  ğŸ” Batch 22/90  |  Loss: 34.2779\n",
      "  ğŸ” Batch 23/90  |  Loss: 26.4520\n",
      "  ğŸ” Batch 24/90  |  Loss: 47.7380\n",
      "  ğŸ” Batch 25/90  |  Loss: 13.4505\n",
      "  ğŸ” Batch 26/90  |  Loss: 25.0778\n",
      "  ğŸ” Batch 27/90  |  Loss: 20.5271\n",
      "  ğŸ” Batch 28/90  |  Loss: 10.5180\n",
      "  ğŸ” Batch 29/90  |  Loss: 17.4455\n",
      "  ğŸ” Batch 30/90  |  Loss: 39.6526\n",
      "  ğŸ” Batch 31/90  |  Loss: 15.7457\n",
      "  ğŸ” Batch 32/90  |  Loss: 23.5721\n",
      "  ğŸ” Batch 33/90  |  Loss: 29.2063\n",
      "  ğŸ” Batch 34/90  |  Loss: 53.2215\n",
      "  ğŸ” Batch 35/90  |  Loss: 31.3992\n",
      "  ğŸ” Batch 36/90  |  Loss: 24.8121\n",
      "  ğŸ” Batch 37/90  |  Loss: 9.7662\n",
      "  ğŸ” Batch 38/90  |  Loss: 24.2647\n",
      "  ğŸ” Batch 39/90  |  Loss: 9.8896\n",
      "  ğŸ” Batch 40/90  |  Loss: 11.4671\n",
      "  ğŸ” Batch 41/90  |  Loss: 23.2839\n",
      "  ğŸ” Batch 42/90  |  Loss: 14.6469\n",
      "  ğŸ” Batch 43/90  |  Loss: 32.2954\n",
      "  ğŸ” Batch 44/90  |  Loss: 27.0764\n",
      "  ğŸ” Batch 45/90  |  Loss: 65.1904\n",
      "  ğŸ” Batch 46/90  |  Loss: 35.3462\n",
      "  ğŸ” Batch 47/90  |  Loss: 25.7590\n",
      "  ğŸ” Batch 48/90  |  Loss: 42.0064\n",
      "  ğŸ” Batch 49/90  |  Loss: 21.3009\n",
      "  ğŸ” Batch 50/90  |  Loss: 18.5959\n",
      "  ğŸ” Batch 51/90  |  Loss: 56.6827\n",
      "  ğŸ” Batch 52/90  |  Loss: 28.9882\n",
      "  ğŸ” Batch 53/90  |  Loss: 18.6609\n",
      "  ğŸ” Batch 54/90  |  Loss: 27.2837\n",
      "  ğŸ” Batch 55/90  |  Loss: 15.3976\n",
      "  ğŸ” Batch 56/90  |  Loss: 21.4245\n",
      "  ğŸ” Batch 57/90  |  Loss: 22.2214\n",
      "  ğŸ” Batch 58/90  |  Loss: 27.5746\n",
      "  ğŸ” Batch 59/90  |  Loss: 22.3860\n",
      "  ğŸ” Batch 60/90  |  Loss: 26.3834\n",
      "  ğŸ” Batch 61/90  |  Loss: 21.1402\n",
      "  ğŸ” Batch 62/90  |  Loss: 35.9267\n",
      "  ğŸ” Batch 63/90  |  Loss: 50.5512\n",
      "  ğŸ” Batch 64/90  |  Loss: 33.7782\n",
      "  ğŸ” Batch 65/90  |  Loss: 39.6239\n",
      "  ğŸ” Batch 66/90  |  Loss: 42.4260\n",
      "  ğŸ” Batch 67/90  |  Loss: 21.2116\n",
      "  ğŸ” Batch 68/90  |  Loss: 25.5073\n",
      "  ğŸ” Batch 69/90  |  Loss: 24.1263\n",
      "  ğŸ” Batch 70/90  |  Loss: 44.7495\n",
      "  ğŸ” Batch 71/90  |  Loss: 41.5745\n",
      "  ğŸ” Batch 72/90  |  Loss: 10.9488\n",
      "  ğŸ” Batch 73/90  |  Loss: 26.0130\n",
      "  ğŸ” Batch 74/90  |  Loss: 36.0794\n",
      "  ğŸ” Batch 75/90  |  Loss: 22.4844\n",
      "  ğŸ” Batch 76/90  |  Loss: 15.0819\n",
      "  ğŸ” Batch 77/90  |  Loss: 11.9030\n",
      "  ğŸ” Batch 78/90  |  Loss: 33.2618\n",
      "  ğŸ” Batch 79/90  |  Loss: 22.3068\n",
      "  ğŸ” Batch 80/90  |  Loss: 46.4695\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_recognition\n\u001b[0;32m      3\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/st_gcn/mediapipe-asl.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43minit_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\main.py:50\u001b[0m, in \u001b[0;36minit_recognition\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m p \u001b[38;5;241m=\u001b[39m tool_module\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mparse_args([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m'\u001b[39m, config_path])\n\u001b[0;32m     49\u001b[0m processor \u001b[38;5;241m=\u001b[39m recog\u001b[38;5;241m.\u001b[39mProcessor(p)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\tools\\recognition.py:81\u001b[0m, in \u001b[0;36mProcessor.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ğŸ” Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  |  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\optim\\adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m         state_steps,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\optim\\adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\optim\\adam.py:433\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 433\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from main import init_recognition\n",
    "\n",
    "config_path = 'config/st_gcn/mediapipe-asl.yaml'\n",
    "init_recognition(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdaf178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\reem1\\anaconda3\\envs\\stgcn-env\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\reem1\\anaconda3\\envs\\stgcn-env\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/9.3 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.1/9.3 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47974cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor start\n",
      "Loading data...\n",
      "Loading model...\n",
      "Starting training for 80 epochs\n",
      "\n",
      "ğŸŒ€ Epoch 1/80\n",
      "[MODEL INIT] Linear input features: 102627\n",
      "  ğŸ” Batch 1/90  |  Loss: 6.7001\n",
      "  ğŸ” Batch 2/90  |  Loss: 46.8071\n",
      "  ğŸ” Batch 3/90  |  Loss: 68.1366\n",
      "  ğŸ” Batch 4/90  |  Loss: 54.8527\n",
      "  ğŸ” Batch 5/90  |  Loss: 94.7677\n",
      "  ğŸ” Batch 6/90  |  Loss: 109.3040\n",
      "  ğŸ” Batch 7/90  |  Loss: 86.0430\n",
      "  ğŸ” Batch 8/90  |  Loss: 72.9892\n",
      "  ğŸ” Batch 9/90  |  Loss: 116.9219\n",
      "  ğŸ” Batch 10/90  |  Loss: 80.2228\n",
      "  ğŸ” Batch 11/90  |  Loss: 113.7112\n",
      "  ğŸ” Batch 12/90  |  Loss: 70.9310\n",
      "  ğŸ” Batch 13/90  |  Loss: 81.0211\n",
      "  ğŸ” Batch 14/90  |  Loss: 115.5419\n",
      "  ğŸ” Batch 15/90  |  Loss: 102.8656\n",
      "  ğŸ” Batch 16/90  |  Loss: 77.2405\n",
      "  ğŸ” Batch 17/90  |  Loss: 89.3968\n",
      "  ğŸ” Batch 18/90  |  Loss: 87.6094\n",
      "  ğŸ” Batch 19/90  |  Loss: 94.0931\n",
      "  ğŸ” Batch 20/90  |  Loss: 84.9703\n",
      "  ğŸ” Batch 21/90  |  Loss: 83.6202\n",
      "  ğŸ” Batch 22/90  |  Loss: 51.1522\n",
      "  ğŸ” Batch 23/90  |  Loss: 92.0081\n",
      "  ğŸ” Batch 24/90  |  Loss: 131.5857\n",
      "  ğŸ” Batch 25/90  |  Loss: 83.0988\n",
      "  ğŸ” Batch 26/90  |  Loss: 55.6021\n",
      "  ğŸ” Batch 27/90  |  Loss: 60.2304\n",
      "  ğŸ” Batch 28/90  |  Loss: 102.1564\n",
      "  ğŸ” Batch 29/90  |  Loss: 75.3548\n",
      "  ğŸ” Batch 30/90  |  Loss: 93.3130\n",
      "  ğŸ” Batch 31/90  |  Loss: 107.3688\n",
      "  ğŸ” Batch 32/90  |  Loss: 98.2318\n",
      "  ğŸ” Batch 33/90  |  Loss: 30.8069\n",
      "  ğŸ” Batch 34/90  |  Loss: 108.1255\n",
      "  ğŸ” Batch 35/90  |  Loss: 86.7490\n",
      "  ğŸ” Batch 36/90  |  Loss: 78.0360\n",
      "  ğŸ” Batch 37/90  |  Loss: 89.4244\n",
      "  ğŸ” Batch 38/90  |  Loss: 77.4238\n",
      "  ğŸ” Batch 39/90  |  Loss: 81.8413\n",
      "  ğŸ” Batch 40/90  |  Loss: 76.9811\n",
      "  ğŸ” Batch 41/90  |  Loss: 88.0660\n",
      "  ğŸ” Batch 42/90  |  Loss: 49.2516\n",
      "  ğŸ” Batch 43/90  |  Loss: 58.5073\n",
      "  ğŸ” Batch 44/90  |  Loss: 60.9759\n",
      "  ğŸ” Batch 45/90  |  Loss: 69.3562\n",
      "  ğŸ” Batch 46/90  |  Loss: 74.1826\n",
      "  ğŸ” Batch 47/90  |  Loss: 107.0049\n",
      "  ğŸ” Batch 48/90  |  Loss: 55.1381\n",
      "  ğŸ” Batch 49/90  |  Loss: 82.3880\n",
      "  ğŸ” Batch 50/90  |  Loss: 80.2374\n",
      "  ğŸ” Batch 51/90  |  Loss: 117.4622\n",
      "  ğŸ” Batch 52/90  |  Loss: 119.3077\n",
      "  ğŸ” Batch 53/90  |  Loss: 115.6323\n",
      "  ğŸ” Batch 54/90  |  Loss: 93.8799\n",
      "  ğŸ” Batch 55/90  |  Loss: 108.1274\n",
      "  ğŸ” Batch 56/90  |  Loss: 147.9650\n",
      "  ğŸ” Batch 57/90  |  Loss: 59.8347\n",
      "  ğŸ” Batch 58/90  |  Loss: 89.8018\n",
      "  ğŸ” Batch 59/90  |  Loss: 98.6485\n",
      "  ğŸ” Batch 60/90  |  Loss: 116.6579\n",
      "  ğŸ” Batch 61/90  |  Loss: 115.1897\n",
      "  ğŸ” Batch 62/90  |  Loss: 56.2403\n",
      "  ğŸ” Batch 63/90  |  Loss: 77.6593\n",
      "  ğŸ” Batch 64/90  |  Loss: 60.1416\n",
      "  ğŸ” Batch 65/90  |  Loss: 64.4847\n",
      "  ğŸ” Batch 66/90  |  Loss: 108.2256\n",
      "  ğŸ” Batch 67/90  |  Loss: 48.9977\n",
      "  ğŸ” Batch 68/90  |  Loss: 87.8614\n",
      "  ğŸ” Batch 69/90  |  Loss: 93.5948\n",
      "  ğŸ” Batch 70/90  |  Loss: 86.8256\n",
      "  ğŸ” Batch 71/90  |  Loss: 100.9431\n",
      "  ğŸ” Batch 72/90  |  Loss: 78.3230\n",
      "  ğŸ” Batch 73/90  |  Loss: 78.9021\n",
      "  ğŸ” Batch 74/90  |  Loss: 56.1819\n",
      "  ğŸ” Batch 75/90  |  Loss: 78.9516\n",
      "  ğŸ” Batch 76/90  |  Loss: 67.9518\n",
      "  ğŸ” Batch 77/90  |  Loss: 69.7217\n",
      "  ğŸ” Batch 78/90  |  Loss: 76.9167\n",
      "  ğŸ” Batch 79/90  |  Loss: 88.0067\n",
      "  ğŸ” Batch 80/90  |  Loss: 64.3725\n",
      "  ğŸ” Batch 81/90  |  Loss: 70.8635\n",
      "  ğŸ” Batch 82/90  |  Loss: 49.8634\n",
      "  ğŸ” Batch 83/90  |  Loss: 31.5280\n",
      "  ğŸ” Batch 84/90  |  Loss: 70.4891\n",
      "  ğŸ” Batch 85/90  |  Loss: 52.4982\n",
      "  ğŸ” Batch 86/90  |  Loss: 69.7691\n",
      "  ğŸ” Batch 87/90  |  Loss: 56.8456\n",
      "  ğŸ” Batch 88/90  |  Loss: 84.3580\n",
      "  ğŸ” Batch 89/90  |  Loss: 62.3828\n",
      "  ğŸ” Batch 90/90  |  Loss: 63.4419\n",
      "\n",
      "ğŸ“Š Epoch 1 Summary:\n",
      "   âœ… Accuracy: 0.0250\n",
      "   ğŸ” Recall:   0.0245\n",
      "   â­ F1 Score: 0.0239\n",
      "\n",
      "ğŸŒ€ Epoch 2/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 60.1886\n",
      "  ğŸ” Batch 2/90  |  Loss: 39.4013\n",
      "  ğŸ” Batch 3/90  |  Loss: 43.2383\n",
      "  ğŸ” Batch 4/90  |  Loss: 29.5383\n",
      "  ğŸ” Batch 5/90  |  Loss: 34.1019\n",
      "  ğŸ” Batch 6/90  |  Loss: 24.8746\n",
      "  ğŸ” Batch 7/90  |  Loss: 38.4315\n",
      "  ğŸ” Batch 8/90  |  Loss: 52.1402\n",
      "  ğŸ” Batch 9/90  |  Loss: 25.3245\n",
      "  ğŸ” Batch 10/90  |  Loss: 58.7444\n",
      "  ğŸ” Batch 11/90  |  Loss: 46.3468\n",
      "  ğŸ” Batch 12/90  |  Loss: 50.4296\n",
      "  ğŸ” Batch 13/90  |  Loss: 35.5052\n",
      "  ğŸ” Batch 14/90  |  Loss: 56.4515\n",
      "  ğŸ” Batch 15/90  |  Loss: 40.7713\n",
      "  ğŸ” Batch 16/90  |  Loss: 39.0757\n",
      "  ğŸ” Batch 17/90  |  Loss: 19.6429\n",
      "  ğŸ” Batch 18/90  |  Loss: 60.6417\n",
      "  ğŸ” Batch 19/90  |  Loss: 63.4431\n",
      "  ğŸ” Batch 20/90  |  Loss: 68.4705\n",
      "  ğŸ” Batch 21/90  |  Loss: 40.4803\n",
      "  ğŸ” Batch 22/90  |  Loss: 35.6910\n",
      "  ğŸ” Batch 23/90  |  Loss: 42.4848\n",
      "  ğŸ” Batch 24/90  |  Loss: 79.0227\n",
      "  ğŸ” Batch 25/90  |  Loss: 44.9847\n",
      "  ğŸ” Batch 26/90  |  Loss: 47.2501\n",
      "  ğŸ” Batch 27/90  |  Loss: 61.7231\n",
      "  ğŸ” Batch 28/90  |  Loss: 31.3425\n",
      "  ğŸ” Batch 29/90  |  Loss: 52.0440\n",
      "  ğŸ” Batch 30/90  |  Loss: 39.3909\n",
      "  ğŸ” Batch 31/90  |  Loss: 34.6086\n",
      "  ğŸ” Batch 32/90  |  Loss: 25.2439\n",
      "  ğŸ” Batch 33/90  |  Loss: 72.8443\n",
      "  ğŸ” Batch 34/90  |  Loss: 56.2098\n",
      "  ğŸ” Batch 35/90  |  Loss: 44.0140\n",
      "  ğŸ” Batch 36/90  |  Loss: 43.5101\n",
      "  ğŸ” Batch 37/90  |  Loss: 67.1856\n",
      "  ğŸ” Batch 38/90  |  Loss: 47.8411\n",
      "  ğŸ” Batch 39/90  |  Loss: 54.4291\n",
      "  ğŸ” Batch 40/90  |  Loss: 40.9906\n",
      "  ğŸ” Batch 41/90  |  Loss: 51.5712\n",
      "  ğŸ” Batch 42/90  |  Loss: 49.0925\n",
      "  ğŸ” Batch 43/90  |  Loss: 44.7910\n",
      "  ğŸ” Batch 44/90  |  Loss: 55.0735\n",
      "  ğŸ” Batch 45/90  |  Loss: 41.9534\n",
      "  ğŸ” Batch 46/90  |  Loss: 46.9864\n",
      "  ğŸ” Batch 47/90  |  Loss: 16.3763\n",
      "  ğŸ” Batch 48/90  |  Loss: 48.4629\n",
      "  ğŸ” Batch 49/90  |  Loss: 29.9872\n",
      "  ğŸ” Batch 50/90  |  Loss: 69.5350\n",
      "  ğŸ” Batch 51/90  |  Loss: 48.4678\n",
      "  ğŸ” Batch 52/90  |  Loss: 37.9378\n",
      "  ğŸ” Batch 53/90  |  Loss: 47.0374\n",
      "  ğŸ” Batch 54/90  |  Loss: 87.8208\n",
      "  ğŸ” Batch 55/90  |  Loss: 28.3776\n",
      "  ğŸ” Batch 56/90  |  Loss: 52.6669\n",
      "  ğŸ” Batch 57/90  |  Loss: 62.9120\n",
      "  ğŸ” Batch 58/90  |  Loss: 17.7613\n",
      "  ğŸ” Batch 59/90  |  Loss: 50.1768\n",
      "  ğŸ” Batch 60/90  |  Loss: 54.5165\n",
      "  ğŸ” Batch 61/90  |  Loss: 30.4561\n",
      "  ğŸ” Batch 62/90  |  Loss: 40.7663\n",
      "  ğŸ” Batch 63/90  |  Loss: 36.5324\n",
      "  ğŸ” Batch 64/90  |  Loss: 36.2157\n",
      "  ğŸ” Batch 65/90  |  Loss: 45.2481\n",
      "  ğŸ” Batch 66/90  |  Loss: 47.2625\n",
      "  ğŸ” Batch 67/90  |  Loss: 37.4212\n",
      "  ğŸ” Batch 68/90  |  Loss: 33.6581\n",
      "  ğŸ” Batch 69/90  |  Loss: 24.4592\n",
      "  ğŸ” Batch 70/90  |  Loss: 41.2166\n",
      "  ğŸ” Batch 71/90  |  Loss: 31.9582\n",
      "  ğŸ” Batch 72/90  |  Loss: 30.6226\n",
      "  ğŸ” Batch 73/90  |  Loss: 39.6737\n",
      "  ğŸ” Batch 74/90  |  Loss: 41.3517\n",
      "  ğŸ” Batch 75/90  |  Loss: 48.2763\n",
      "  ğŸ” Batch 76/90  |  Loss: 40.7658\n",
      "  ğŸ” Batch 77/90  |  Loss: 35.8194\n",
      "  ğŸ” Batch 78/90  |  Loss: 42.2572\n",
      "  ğŸ” Batch 79/90  |  Loss: 41.2336\n",
      "  ğŸ” Batch 80/90  |  Loss: 23.2770\n",
      "  ğŸ” Batch 81/90  |  Loss: 42.5381\n",
      "  ğŸ” Batch 82/90  |  Loss: 33.3023\n",
      "  ğŸ” Batch 83/90  |  Loss: 43.1324\n",
      "  ğŸ” Batch 84/90  |  Loss: 29.5431\n",
      "  ğŸ” Batch 85/90  |  Loss: 32.6276\n",
      "  ğŸ” Batch 86/90  |  Loss: 28.1988\n",
      "  ğŸ” Batch 87/90  |  Loss: 51.0754\n",
      "  ğŸ” Batch 88/90  |  Loss: 38.3904\n",
      "  ğŸ” Batch 89/90  |  Loss: 28.8600\n",
      "  ğŸ” Batch 90/90  |  Loss: 36.1885\n",
      "\n",
      "ğŸ“Š Epoch 2 Summary:\n",
      "   âœ… Accuracy: 0.0417\n",
      "   ğŸ” Recall:   0.0417\n",
      "   â­ F1 Score: 0.0407\n",
      "\n",
      "ğŸŒ€ Epoch 3/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 33.1976\n",
      "  ğŸ” Batch 2/90  |  Loss: 33.8448\n",
      "  ğŸ” Batch 3/90  |  Loss: 42.8384\n",
      "  ğŸ” Batch 4/90  |  Loss: 19.8464\n",
      "  ğŸ” Batch 5/90  |  Loss: 41.4780\n",
      "  ğŸ” Batch 6/90  |  Loss: 12.1359\n",
      "  ğŸ” Batch 7/90  |  Loss: 50.4405\n",
      "  ğŸ” Batch 8/90  |  Loss: 54.6313\n",
      "  ğŸ” Batch 9/90  |  Loss: 41.8073\n",
      "  ğŸ” Batch 10/90  |  Loss: 38.1185\n",
      "  ğŸ” Batch 11/90  |  Loss: 35.6943\n",
      "  ğŸ” Batch 12/90  |  Loss: 28.3819\n",
      "  ğŸ” Batch 13/90  |  Loss: 26.3465\n",
      "  ğŸ” Batch 14/90  |  Loss: 50.4607\n",
      "  ğŸ” Batch 15/90  |  Loss: 35.8962\n",
      "  ğŸ” Batch 16/90  |  Loss: 20.2988\n",
      "  ğŸ” Batch 17/90  |  Loss: 52.2021\n",
      "  ğŸ” Batch 18/90  |  Loss: 21.9640\n",
      "  ğŸ” Batch 19/90  |  Loss: 34.3272\n",
      "  ğŸ” Batch 20/90  |  Loss: 36.6661\n",
      "  ğŸ” Batch 21/90  |  Loss: 40.1141\n",
      "  ğŸ” Batch 22/90  |  Loss: 42.8211\n",
      "  ğŸ” Batch 23/90  |  Loss: 40.6214\n",
      "  ğŸ” Batch 24/90  |  Loss: 35.5524\n",
      "  ğŸ” Batch 25/90  |  Loss: 29.9935\n",
      "  ğŸ” Batch 26/90  |  Loss: 30.0553\n",
      "  ğŸ” Batch 27/90  |  Loss: 26.1251\n",
      "  ğŸ” Batch 28/90  |  Loss: 38.1023\n",
      "  ğŸ” Batch 29/90  |  Loss: 20.4978\n",
      "  ğŸ” Batch 30/90  |  Loss: 38.8202\n",
      "  ğŸ” Batch 31/90  |  Loss: 43.4071\n",
      "  ğŸ” Batch 32/90  |  Loss: 48.2446\n",
      "  ğŸ” Batch 33/90  |  Loss: 35.0520\n",
      "  ğŸ” Batch 34/90  |  Loss: 27.2181\n",
      "  ğŸ” Batch 35/90  |  Loss: 56.2074\n",
      "  ğŸ” Batch 36/90  |  Loss: 36.0009\n",
      "  ğŸ” Batch 37/90  |  Loss: 53.9517\n",
      "  ğŸ” Batch 38/90  |  Loss: 60.9315\n",
      "  ğŸ” Batch 39/90  |  Loss: 49.6279\n",
      "  ğŸ” Batch 40/90  |  Loss: 46.5724\n",
      "  ğŸ” Batch 41/90  |  Loss: 35.9205\n",
      "  ğŸ” Batch 42/90  |  Loss: 33.2489\n",
      "  ğŸ” Batch 43/90  |  Loss: 38.1481\n",
      "  ğŸ” Batch 44/90  |  Loss: 31.8548\n",
      "  ğŸ” Batch 45/90  |  Loss: 39.5094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 46/90  |  Loss: 49.4643\n",
      "  ğŸ” Batch 47/90  |  Loss: 28.4118\n",
      "  ğŸ” Batch 48/90  |  Loss: 29.2672\n",
      "  ğŸ” Batch 49/90  |  Loss: 59.7581\n",
      "  ğŸ” Batch 50/90  |  Loss: 62.5281\n",
      "  ğŸ” Batch 51/90  |  Loss: 32.3601\n",
      "  ğŸ” Batch 52/90  |  Loss: 27.8667\n",
      "  ğŸ” Batch 53/90  |  Loss: 61.7669\n",
      "  ğŸ” Batch 54/90  |  Loss: 57.3587\n",
      "  ğŸ” Batch 55/90  |  Loss: 35.9903\n",
      "  ğŸ” Batch 56/90  |  Loss: 47.2859\n",
      "  ğŸ” Batch 57/90  |  Loss: 26.2325\n",
      "  ğŸ” Batch 58/90  |  Loss: 75.4217\n",
      "  ğŸ” Batch 59/90  |  Loss: 44.9879\n",
      "  ğŸ” Batch 60/90  |  Loss: 34.4263\n",
      "  ğŸ” Batch 61/90  |  Loss: 26.6675\n",
      "  ğŸ” Batch 62/90  |  Loss: 37.5714\n",
      "  ğŸ” Batch 63/90  |  Loss: 23.9343\n",
      "  ğŸ” Batch 64/90  |  Loss: 55.2577\n",
      "  ğŸ” Batch 65/90  |  Loss: 56.8142\n",
      "  ğŸ” Batch 66/90  |  Loss: 41.5297\n",
      "  ğŸ” Batch 67/90  |  Loss: 50.8064\n",
      "  ğŸ” Batch 68/90  |  Loss: 70.0491\n",
      "  ğŸ” Batch 69/90  |  Loss: 61.0649\n",
      "  ğŸ” Batch 70/90  |  Loss: 55.8083\n",
      "  ğŸ” Batch 71/90  |  Loss: 20.3956\n",
      "  ğŸ” Batch 72/90  |  Loss: 33.9900\n",
      "  ğŸ” Batch 73/90  |  Loss: 42.5566\n",
      "  ğŸ” Batch 74/90  |  Loss: 49.4086\n",
      "  ğŸ” Batch 75/90  |  Loss: 32.6148\n",
      "  ğŸ” Batch 76/90  |  Loss: 34.3917\n",
      "  ğŸ” Batch 77/90  |  Loss: 50.1334\n",
      "  ğŸ” Batch 78/90  |  Loss: 25.5029\n",
      "  ğŸ” Batch 79/90  |  Loss: 41.3594\n",
      "  ğŸ” Batch 80/90  |  Loss: 49.3820\n",
      "  ğŸ” Batch 81/90  |  Loss: 41.6027\n",
      "  ğŸ” Batch 82/90  |  Loss: 44.9384\n",
      "  ğŸ” Batch 83/90  |  Loss: 44.2945\n",
      "  ğŸ” Batch 84/90  |  Loss: 10.7885\n",
      "  ğŸ” Batch 85/90  |  Loss: 41.1589\n",
      "  ğŸ” Batch 86/90  |  Loss: 51.5219\n",
      "  ğŸ” Batch 87/90  |  Loss: 59.8445\n",
      "  ğŸ” Batch 88/90  |  Loss: 80.8886\n",
      "  ğŸ” Batch 89/90  |  Loss: 50.5091\n",
      "  ğŸ” Batch 90/90  |  Loss: 31.0061\n",
      "\n",
      "ğŸ“Š Epoch 3 Summary:\n",
      "   âœ… Accuracy: 0.0667\n",
      "   ğŸ” Recall:   0.0667\n",
      "   â­ F1 Score: 0.0660\n",
      "\n",
      "ğŸŒ€ Epoch 4/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 55.8953\n",
      "  ğŸ” Batch 2/90  |  Loss: 34.9589\n",
      "  ğŸ” Batch 3/90  |  Loss: 21.0796\n",
      "  ğŸ” Batch 4/90  |  Loss: 31.1922\n",
      "  ğŸ” Batch 5/90  |  Loss: 13.6149\n",
      "  ğŸ” Batch 6/90  |  Loss: 36.8926\n",
      "  ğŸ” Batch 7/90  |  Loss: 24.8936\n",
      "  ğŸ” Batch 8/90  |  Loss: 31.3947\n",
      "  ğŸ” Batch 9/90  |  Loss: 29.5613\n",
      "  ğŸ” Batch 10/90  |  Loss: 53.0827\n",
      "  ğŸ” Batch 11/90  |  Loss: 54.1593\n",
      "  ğŸ” Batch 12/90  |  Loss: 33.4329\n",
      "  ğŸ” Batch 13/90  |  Loss: 46.1937\n",
      "  ğŸ” Batch 14/90  |  Loss: 29.4338\n",
      "  ğŸ” Batch 15/90  |  Loss: 28.7444\n",
      "  ğŸ” Batch 16/90  |  Loss: 34.6195\n",
      "  ğŸ” Batch 17/90  |  Loss: 52.9872\n",
      "  ğŸ” Batch 18/90  |  Loss: 37.7972\n",
      "  ğŸ” Batch 19/90  |  Loss: 22.9105\n",
      "  ğŸ” Batch 20/90  |  Loss: 26.4576\n",
      "  ğŸ” Batch 21/90  |  Loss: 42.3681\n",
      "  ğŸ” Batch 22/90  |  Loss: 38.9379\n",
      "  ğŸ” Batch 23/90  |  Loss: 41.7432\n",
      "  ğŸ” Batch 24/90  |  Loss: 18.9671\n",
      "  ğŸ” Batch 25/90  |  Loss: 48.9787\n",
      "  ğŸ” Batch 26/90  |  Loss: 24.1081\n",
      "  ğŸ” Batch 27/90  |  Loss: 23.7840\n",
      "  ğŸ” Batch 28/90  |  Loss: 34.4266\n",
      "  ğŸ” Batch 29/90  |  Loss: 28.7901\n",
      "  ğŸ” Batch 30/90  |  Loss: 26.2715\n",
      "  ğŸ” Batch 31/90  |  Loss: 17.3704\n",
      "  ğŸ” Batch 32/90  |  Loss: 22.8309\n",
      "  ğŸ” Batch 33/90  |  Loss: 35.0989\n",
      "  ğŸ” Batch 34/90  |  Loss: 31.3232\n",
      "  ğŸ” Batch 35/90  |  Loss: 36.8900\n",
      "  ğŸ” Batch 36/90  |  Loss: 9.1290\n",
      "  ğŸ” Batch 37/90  |  Loss: 28.8982\n",
      "  ğŸ” Batch 38/90  |  Loss: 20.0218\n",
      "  ğŸ” Batch 39/90  |  Loss: 60.6983\n",
      "  ğŸ” Batch 40/90  |  Loss: 33.7677\n",
      "  ğŸ” Batch 41/90  |  Loss: 26.4153\n",
      "  ğŸ” Batch 42/90  |  Loss: 42.5181\n",
      "  ğŸ” Batch 43/90  |  Loss: 32.2549\n",
      "  ğŸ” Batch 44/90  |  Loss: 30.1623\n",
      "  ğŸ” Batch 45/90  |  Loss: 11.4557\n",
      "  ğŸ” Batch 46/90  |  Loss: 19.3744\n",
      "  ğŸ” Batch 47/90  |  Loss: 27.6726\n",
      "  ğŸ” Batch 48/90  |  Loss: 44.8216\n",
      "  ğŸ” Batch 49/90  |  Loss: 63.8345\n",
      "  ğŸ” Batch 50/90  |  Loss: 14.3350\n",
      "  ğŸ” Batch 51/90  |  Loss: 26.4315\n",
      "  ğŸ” Batch 52/90  |  Loss: 65.0520\n",
      "  ğŸ” Batch 53/90  |  Loss: 45.3250\n",
      "  ğŸ” Batch 54/90  |  Loss: 48.9014\n",
      "  ğŸ” Batch 55/90  |  Loss: 30.2979\n",
      "  ğŸ” Batch 56/90  |  Loss: 46.0769\n",
      "  ğŸ” Batch 57/90  |  Loss: 54.3324\n",
      "  ğŸ” Batch 58/90  |  Loss: 52.1388\n",
      "  ğŸ” Batch 59/90  |  Loss: 23.8768\n",
      "  ğŸ” Batch 60/90  |  Loss: 37.2709\n",
      "  ğŸ” Batch 61/90  |  Loss: 29.1726\n",
      "  ğŸ” Batch 62/90  |  Loss: 39.7017\n",
      "  ğŸ” Batch 63/90  |  Loss: 66.4113\n",
      "  ğŸ” Batch 64/90  |  Loss: 32.3587\n",
      "  ğŸ” Batch 65/90  |  Loss: 31.2347\n",
      "  ğŸ” Batch 66/90  |  Loss: 41.6602\n",
      "  ğŸ” Batch 67/90  |  Loss: 66.6276\n",
      "  ğŸ” Batch 68/90  |  Loss: 47.3716\n",
      "  ğŸ” Batch 69/90  |  Loss: 50.4641\n",
      "  ğŸ” Batch 70/90  |  Loss: 35.0913\n",
      "  ğŸ” Batch 71/90  |  Loss: 42.5627\n",
      "  ğŸ” Batch 72/90  |  Loss: 62.2213\n",
      "  ğŸ” Batch 73/90  |  Loss: 50.5916\n",
      "  ğŸ” Batch 74/90  |  Loss: 36.3823\n",
      "  ğŸ” Batch 75/90  |  Loss: 36.6579\n",
      "  ğŸ” Batch 76/90  |  Loss: 37.2393\n",
      "  ğŸ” Batch 77/90  |  Loss: 39.2414\n",
      "  ğŸ” Batch 78/90  |  Loss: 32.5392\n",
      "  ğŸ” Batch 79/90  |  Loss: 16.7241\n",
      "  ğŸ” Batch 80/90  |  Loss: 43.6511\n",
      "  ğŸ” Batch 81/90  |  Loss: 49.0617\n",
      "  ğŸ” Batch 82/90  |  Loss: 38.5156\n",
      "  ğŸ” Batch 83/90  |  Loss: 24.3821\n",
      "  ğŸ” Batch 84/90  |  Loss: 29.9934\n",
      "  ğŸ” Batch 85/90  |  Loss: 43.8460\n",
      "  ğŸ” Batch 86/90  |  Loss: 43.5785\n",
      "  ğŸ” Batch 87/90  |  Loss: 25.7488\n",
      "  ğŸ” Batch 88/90  |  Loss: 26.1798\n",
      "  ğŸ” Batch 89/90  |  Loss: 40.3359\n",
      "  ğŸ” Batch 90/90  |  Loss: 25.9089\n",
      "\n",
      "ğŸ“Š Epoch 4 Summary:\n",
      "   âœ… Accuracy: 0.0986\n",
      "   ğŸ” Recall:   0.0986\n",
      "   â­ F1 Score: 0.0990\n",
      "\n",
      "ğŸŒ€ Epoch 5/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 34.1248\n",
      "  ğŸ” Batch 2/90  |  Loss: 51.9889\n",
      "  ğŸ” Batch 3/90  |  Loss: 38.4792\n",
      "  ğŸ” Batch 4/90  |  Loss: 41.3430\n",
      "  ğŸ” Batch 5/90  |  Loss: 44.7657\n",
      "  ğŸ” Batch 6/90  |  Loss: 29.9122\n",
      "  ğŸ” Batch 7/90  |  Loss: 44.9890\n",
      "  ğŸ” Batch 8/90  |  Loss: 32.6448\n",
      "  ğŸ” Batch 9/90  |  Loss: 16.3304\n",
      "  ğŸ” Batch 10/90  |  Loss: 26.8999\n",
      "  ğŸ” Batch 11/90  |  Loss: 43.8587\n",
      "  ğŸ” Batch 12/90  |  Loss: 58.8665\n",
      "  ğŸ” Batch 13/90  |  Loss: 39.5897\n",
      "  ğŸ” Batch 14/90  |  Loss: 44.6820\n",
      "  ğŸ” Batch 15/90  |  Loss: 28.4256\n",
      "  ğŸ” Batch 16/90  |  Loss: 51.2082\n",
      "  ğŸ” Batch 17/90  |  Loss: 38.1265\n",
      "  ğŸ” Batch 18/90  |  Loss: 20.9659\n",
      "  ğŸ” Batch 19/90  |  Loss: 34.9182\n",
      "  ğŸ” Batch 20/90  |  Loss: 18.4524\n",
      "  ğŸ” Batch 21/90  |  Loss: 21.0683\n",
      "  ğŸ” Batch 22/90  |  Loss: 47.4640\n",
      "  ğŸ” Batch 23/90  |  Loss: 36.0811\n",
      "  ğŸ” Batch 24/90  |  Loss: 30.6643\n",
      "  ğŸ” Batch 25/90  |  Loss: 43.0000\n",
      "  ğŸ” Batch 26/90  |  Loss: 52.6472\n",
      "  ğŸ” Batch 27/90  |  Loss: 18.3738\n",
      "  ğŸ” Batch 28/90  |  Loss: 36.9642\n",
      "  ğŸ” Batch 29/90  |  Loss: 32.0149\n",
      "  ğŸ” Batch 30/90  |  Loss: 30.3686\n",
      "  ğŸ” Batch 31/90  |  Loss: 55.1343\n",
      "  ğŸ” Batch 32/90  |  Loss: 47.7995\n",
      "  ğŸ” Batch 33/90  |  Loss: 29.5628\n",
      "  ğŸ” Batch 34/90  |  Loss: 31.8453\n",
      "  ğŸ” Batch 35/90  |  Loss: 40.9845\n",
      "  ğŸ” Batch 36/90  |  Loss: 43.1096\n",
      "  ğŸ” Batch 37/90  |  Loss: 27.6479\n",
      "  ğŸ” Batch 38/90  |  Loss: 46.1194\n",
      "  ğŸ” Batch 39/90  |  Loss: 34.3903\n",
      "  ğŸ” Batch 40/90  |  Loss: 48.0930\n",
      "  ğŸ” Batch 41/90  |  Loss: 48.1696\n",
      "  ğŸ” Batch 42/90  |  Loss: 28.5880\n",
      "  ğŸ” Batch 43/90  |  Loss: 34.1097\n",
      "  ğŸ” Batch 44/90  |  Loss: 51.0787\n",
      "  ğŸ” Batch 45/90  |  Loss: 67.7813\n",
      "  ğŸ” Batch 46/90  |  Loss: 24.7323\n",
      "  ğŸ” Batch 47/90  |  Loss: 43.1839\n",
      "  ğŸ” Batch 48/90  |  Loss: 50.8686\n",
      "  ğŸ” Batch 49/90  |  Loss: 36.6862\n",
      "  ğŸ” Batch 50/90  |  Loss: 38.1847\n",
      "  ğŸ” Batch 51/90  |  Loss: 77.0286\n",
      "  ğŸ” Batch 52/90  |  Loss: 36.8150\n",
      "  ğŸ” Batch 53/90  |  Loss: 36.8145\n",
      "  ğŸ” Batch 54/90  |  Loss: 38.8119\n",
      "  ğŸ” Batch 55/90  |  Loss: 41.1972\n",
      "  ğŸ” Batch 56/90  |  Loss: 22.8873\n",
      "  ğŸ” Batch 57/90  |  Loss: 67.2887\n",
      "  ğŸ” Batch 58/90  |  Loss: 25.3450\n",
      "  ğŸ” Batch 59/90  |  Loss: 44.8455\n",
      "  ğŸ” Batch 60/90  |  Loss: 51.6203\n",
      "  ğŸ” Batch 61/90  |  Loss: 29.2619\n",
      "  ğŸ” Batch 62/90  |  Loss: 24.3096\n",
      "  ğŸ” Batch 63/90  |  Loss: 31.8176\n",
      "  ğŸ” Batch 64/90  |  Loss: 68.1533\n",
      "  ğŸ” Batch 65/90  |  Loss: 50.6173\n",
      "  ğŸ” Batch 66/90  |  Loss: 32.6910\n",
      "  ğŸ” Batch 67/90  |  Loss: 52.6537\n",
      "  ğŸ” Batch 68/90  |  Loss: 29.1655\n",
      "  ğŸ” Batch 69/90  |  Loss: 21.7637\n",
      "  ğŸ” Batch 70/90  |  Loss: 22.5706\n",
      "  ğŸ” Batch 71/90  |  Loss: 35.3754\n",
      "  ğŸ” Batch 72/90  |  Loss: 58.0529\n",
      "  ğŸ” Batch 73/90  |  Loss: 24.3980\n",
      "  ğŸ” Batch 74/90  |  Loss: 29.9147\n",
      "  ğŸ” Batch 75/90  |  Loss: 70.8707\n",
      "  ğŸ” Batch 76/90  |  Loss: 55.7914\n",
      "  ğŸ” Batch 77/90  |  Loss: 30.6386\n",
      "  ğŸ” Batch 78/90  |  Loss: 34.2989\n",
      "  ğŸ” Batch 79/90  |  Loss: 58.3076\n",
      "  ğŸ” Batch 80/90  |  Loss: 32.0113\n",
      "  ğŸ” Batch 81/90  |  Loss: 37.7370\n",
      "  ğŸ” Batch 82/90  |  Loss: 35.4454\n",
      "  ğŸ” Batch 83/90  |  Loss: 36.2503\n",
      "  ğŸ” Batch 84/90  |  Loss: 28.6278\n",
      "  ğŸ” Batch 85/90  |  Loss: 38.2433\n",
      "  ğŸ” Batch 86/90  |  Loss: 33.6790\n",
      "  ğŸ” Batch 87/90  |  Loss: 51.3841\n",
      "  ğŸ” Batch 88/90  |  Loss: 27.7363\n",
      "  ğŸ” Batch 89/90  |  Loss: 19.8972\n",
      "  ğŸ” Batch 90/90  |  Loss: 18.6870\n",
      "\n",
      "ğŸ“Š Epoch 5 Summary:\n",
      "   âœ… Accuracy: 0.0847\n",
      "   ğŸ” Recall:   0.0847\n",
      "   â­ F1 Score: 0.0853\n",
      "\n",
      "ğŸŒ€ Epoch 6/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 16.5615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 2/90  |  Loss: 20.3836\n",
      "  ğŸ” Batch 3/90  |  Loss: 26.2943\n",
      "  ğŸ” Batch 4/90  |  Loss: 31.0392\n",
      "  ğŸ” Batch 5/90  |  Loss: 17.0107\n",
      "  ğŸ” Batch 6/90  |  Loss: 24.4907\n",
      "  ğŸ” Batch 7/90  |  Loss: 27.9901\n",
      "  ğŸ” Batch 8/90  |  Loss: 35.0915\n",
      "  ğŸ” Batch 9/90  |  Loss: 55.0964\n",
      "  ğŸ” Batch 10/90  |  Loss: 49.9237\n",
      "  ğŸ” Batch 11/90  |  Loss: 36.5223\n",
      "  ğŸ” Batch 12/90  |  Loss: 21.8519\n",
      "  ğŸ” Batch 13/90  |  Loss: 39.2543\n",
      "  ğŸ” Batch 14/90  |  Loss: 31.2035\n",
      "  ğŸ” Batch 15/90  |  Loss: 36.1579\n",
      "  ğŸ” Batch 16/90  |  Loss: 31.8771\n",
      "  ğŸ” Batch 17/90  |  Loss: 35.9530\n",
      "  ğŸ” Batch 18/90  |  Loss: 19.0505\n",
      "  ğŸ” Batch 19/90  |  Loss: 28.9901\n",
      "  ğŸ” Batch 20/90  |  Loss: 27.0600\n",
      "  ğŸ” Batch 21/90  |  Loss: 29.6704\n",
      "  ğŸ” Batch 22/90  |  Loss: 33.6376\n",
      "  ğŸ” Batch 23/90  |  Loss: 37.0036\n",
      "  ğŸ” Batch 24/90  |  Loss: 42.8539\n",
      "  ğŸ” Batch 25/90  |  Loss: 12.2684\n",
      "  ğŸ” Batch 26/90  |  Loss: 18.8642\n",
      "  ğŸ” Batch 27/90  |  Loss: 33.2716\n",
      "  ğŸ” Batch 28/90  |  Loss: 38.4153\n",
      "  ğŸ” Batch 29/90  |  Loss: 42.9470\n",
      "  ğŸ” Batch 30/90  |  Loss: 25.3200\n",
      "  ğŸ” Batch 31/90  |  Loss: 48.1235\n",
      "  ğŸ” Batch 32/90  |  Loss: 22.5361\n",
      "  ğŸ” Batch 33/90  |  Loss: 38.4484\n",
      "  ğŸ” Batch 34/90  |  Loss: 26.9266\n",
      "  ğŸ” Batch 35/90  |  Loss: 25.2966\n",
      "  ğŸ” Batch 36/90  |  Loss: 68.5516\n",
      "  ğŸ” Batch 37/90  |  Loss: 28.4401\n",
      "  ğŸ” Batch 38/90  |  Loss: 80.5246\n",
      "  ğŸ” Batch 39/90  |  Loss: 54.5061\n",
      "  ğŸ” Batch 40/90  |  Loss: 52.0858\n",
      "  ğŸ” Batch 41/90  |  Loss: 54.2342\n",
      "  ğŸ” Batch 42/90  |  Loss: 48.3521\n",
      "  ğŸ” Batch 43/90  |  Loss: 28.3480\n",
      "  ğŸ” Batch 44/90  |  Loss: 48.8899\n",
      "  ğŸ” Batch 45/90  |  Loss: 28.2180\n",
      "  ğŸ” Batch 46/90  |  Loss: 15.0664\n",
      "  ğŸ” Batch 47/90  |  Loss: 27.5362\n",
      "  ğŸ” Batch 48/90  |  Loss: 21.8721\n",
      "  ğŸ” Batch 49/90  |  Loss: 25.5735\n",
      "  ğŸ” Batch 50/90  |  Loss: 18.9016\n",
      "  ğŸ” Batch 51/90  |  Loss: 44.4667\n",
      "  ğŸ” Batch 52/90  |  Loss: 40.1274\n",
      "  ğŸ” Batch 53/90  |  Loss: 49.9494\n",
      "  ğŸ” Batch 54/90  |  Loss: 26.5425\n",
      "  ğŸ” Batch 55/90  |  Loss: 39.5239\n",
      "  ğŸ” Batch 56/90  |  Loss: 44.6423\n",
      "  ğŸ” Batch 57/90  |  Loss: 40.2146\n",
      "  ğŸ” Batch 58/90  |  Loss: 41.3038\n",
      "  ğŸ” Batch 59/90  |  Loss: 40.3181\n",
      "  ğŸ” Batch 60/90  |  Loss: 74.1251\n",
      "  ğŸ” Batch 61/90  |  Loss: 49.4767\n",
      "  ğŸ” Batch 62/90  |  Loss: 40.1842\n",
      "  ğŸ” Batch 63/90  |  Loss: 31.7299\n",
      "  ğŸ” Batch 64/90  |  Loss: 41.9993\n",
      "  ğŸ” Batch 65/90  |  Loss: 45.2521\n",
      "  ğŸ” Batch 66/90  |  Loss: 24.3522\n",
      "  ğŸ” Batch 67/90  |  Loss: 23.0025\n",
      "  ğŸ” Batch 68/90  |  Loss: 40.4994\n",
      "  ğŸ” Batch 69/90  |  Loss: 12.1611\n",
      "  ğŸ” Batch 70/90  |  Loss: 34.9456\n",
      "  ğŸ” Batch 71/90  |  Loss: 21.7960\n",
      "  ğŸ” Batch 72/90  |  Loss: 45.6176\n",
      "  ğŸ” Batch 73/90  |  Loss: 27.7947\n",
      "  ğŸ” Batch 74/90  |  Loss: 44.0107\n",
      "  ğŸ” Batch 75/90  |  Loss: 29.5965\n",
      "  ğŸ” Batch 76/90  |  Loss: 59.0405\n",
      "  ğŸ” Batch 77/90  |  Loss: 37.0467\n",
      "  ğŸ” Batch 78/90  |  Loss: 44.4276\n",
      "  ğŸ” Batch 79/90  |  Loss: 34.0030\n",
      "  ğŸ” Batch 80/90  |  Loss: 32.6902\n",
      "  ğŸ” Batch 81/90  |  Loss: 32.9733\n",
      "  ğŸ” Batch 82/90  |  Loss: 53.1094\n",
      "  ğŸ” Batch 83/90  |  Loss: 34.1454\n",
      "  ğŸ” Batch 84/90  |  Loss: 28.3373\n",
      "  ğŸ” Batch 85/90  |  Loss: 40.9089\n",
      "  ğŸ” Batch 86/90  |  Loss: 38.7308\n",
      "  ğŸ” Batch 87/90  |  Loss: 28.7158\n",
      "  ğŸ” Batch 88/90  |  Loss: 52.5398\n",
      "  ğŸ” Batch 89/90  |  Loss: 35.2094\n",
      "  ğŸ” Batch 90/90  |  Loss: 38.5729\n",
      "\n",
      "ğŸ“Š Epoch 6 Summary:\n",
      "   âœ… Accuracy: 0.1125\n",
      "   ğŸ” Recall:   0.1125\n",
      "   â­ F1 Score: 0.1112\n",
      "\n",
      "ğŸŒ€ Epoch 7/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 30.4358\n",
      "  ğŸ” Batch 2/90  |  Loss: 32.1418\n",
      "  ğŸ” Batch 3/90  |  Loss: 28.5071\n",
      "  ğŸ” Batch 4/90  |  Loss: 50.0394\n",
      "  ğŸ” Batch 5/90  |  Loss: 37.0179\n",
      "  ğŸ” Batch 6/90  |  Loss: 48.0522\n",
      "  ğŸ” Batch 7/90  |  Loss: 29.9963\n",
      "  ğŸ” Batch 8/90  |  Loss: 28.1306\n",
      "  ğŸ” Batch 9/90  |  Loss: 33.2473\n",
      "  ğŸ” Batch 10/90  |  Loss: 50.2246\n",
      "  ğŸ” Batch 11/90  |  Loss: 48.7176\n",
      "  ğŸ” Batch 12/90  |  Loss: 39.8892\n",
      "  ğŸ” Batch 13/90  |  Loss: 24.4201\n",
      "  ğŸ” Batch 14/90  |  Loss: 15.8962\n",
      "  ğŸ” Batch 15/90  |  Loss: 12.2668\n",
      "  ğŸ” Batch 16/90  |  Loss: 34.4306\n",
      "  ğŸ” Batch 17/90  |  Loss: 38.8383\n",
      "  ğŸ” Batch 18/90  |  Loss: 18.9826\n",
      "  ğŸ” Batch 19/90  |  Loss: 54.8988\n",
      "  ğŸ” Batch 20/90  |  Loss: 31.9861\n",
      "  ğŸ” Batch 21/90  |  Loss: 35.8635\n",
      "  ğŸ” Batch 22/90  |  Loss: 21.5068\n",
      "  ğŸ” Batch 23/90  |  Loss: 41.4649\n",
      "  ğŸ” Batch 24/90  |  Loss: 52.8944\n",
      "  ğŸ” Batch 25/90  |  Loss: 31.9637\n",
      "  ğŸ” Batch 26/90  |  Loss: 50.4792\n",
      "  ğŸ” Batch 27/90  |  Loss: 37.4717\n",
      "  ğŸ” Batch 28/90  |  Loss: 28.8675\n",
      "  ğŸ” Batch 29/90  |  Loss: 58.2627\n",
      "  ğŸ” Batch 30/90  |  Loss: 42.7309\n",
      "  ğŸ” Batch 31/90  |  Loss: 27.5253\n",
      "  ğŸ” Batch 32/90  |  Loss: 22.4744\n",
      "  ğŸ” Batch 33/90  |  Loss: 13.9539\n",
      "  ğŸ” Batch 34/90  |  Loss: 34.7944\n",
      "  ğŸ” Batch 35/90  |  Loss: 43.3964\n",
      "  ğŸ” Batch 36/90  |  Loss: 45.9812\n",
      "  ğŸ” Batch 37/90  |  Loss: 36.3356\n",
      "  ğŸ” Batch 38/90  |  Loss: 56.9102\n",
      "  ğŸ” Batch 39/90  |  Loss: 38.8725\n",
      "  ğŸ” Batch 40/90  |  Loss: 53.3895\n",
      "  ğŸ” Batch 41/90  |  Loss: 20.4859\n",
      "  ğŸ” Batch 42/90  |  Loss: 33.5443\n",
      "  ğŸ” Batch 43/90  |  Loss: 46.6598\n",
      "  ğŸ” Batch 44/90  |  Loss: 34.2313\n",
      "  ğŸ” Batch 45/90  |  Loss: 38.7962\n",
      "  ğŸ” Batch 46/90  |  Loss: 23.5178\n",
      "  ğŸ” Batch 47/90  |  Loss: 35.8399\n",
      "  ğŸ” Batch 48/90  |  Loss: 41.2526\n",
      "  ğŸ” Batch 49/90  |  Loss: 33.4108\n",
      "  ğŸ” Batch 50/90  |  Loss: 63.5769\n",
      "  ğŸ” Batch 51/90  |  Loss: 37.8127\n",
      "  ğŸ” Batch 52/90  |  Loss: 45.8142\n",
      "  ğŸ” Batch 53/90  |  Loss: 17.3939\n",
      "  ğŸ” Batch 54/90  |  Loss: 30.2886\n",
      "  ğŸ” Batch 55/90  |  Loss: 42.9963\n",
      "  ğŸ” Batch 56/90  |  Loss: 45.2826\n",
      "  ğŸ” Batch 57/90  |  Loss: 25.7497\n",
      "  ğŸ” Batch 58/90  |  Loss: 25.5110\n",
      "  ğŸ” Batch 59/90  |  Loss: 58.0612\n",
      "  ğŸ” Batch 60/90  |  Loss: 54.7836\n",
      "  ğŸ” Batch 61/90  |  Loss: 46.4407\n",
      "  ğŸ” Batch 62/90  |  Loss: 48.3759\n",
      "  ğŸ” Batch 63/90  |  Loss: 55.4620\n",
      "  ğŸ” Batch 64/90  |  Loss: 38.0102\n",
      "  ğŸ” Batch 65/90  |  Loss: 21.5202\n",
      "  ğŸ” Batch 66/90  |  Loss: 35.8057\n",
      "  ğŸ” Batch 67/90  |  Loss: 14.2817\n",
      "  ğŸ” Batch 68/90  |  Loss: 17.7020\n",
      "  ğŸ” Batch 69/90  |  Loss: 16.9157\n",
      "  ğŸ” Batch 70/90  |  Loss: 38.6088\n",
      "  ğŸ” Batch 71/90  |  Loss: 28.4132\n",
      "  ğŸ” Batch 72/90  |  Loss: 36.5236\n",
      "  ğŸ” Batch 73/90  |  Loss: 33.8986\n",
      "  ğŸ” Batch 74/90  |  Loss: 23.8603\n",
      "  ğŸ” Batch 75/90  |  Loss: 25.1658\n",
      "  ğŸ” Batch 76/90  |  Loss: 29.3172\n",
      "  ğŸ” Batch 77/90  |  Loss: 43.7136\n",
      "  ğŸ” Batch 78/90  |  Loss: 32.2242\n",
      "  ğŸ” Batch 79/90  |  Loss: 29.1770\n",
      "  ğŸ” Batch 80/90  |  Loss: 47.1888\n",
      "  ğŸ” Batch 81/90  |  Loss: 27.5936\n",
      "  ğŸ” Batch 82/90  |  Loss: 53.7028\n",
      "  ğŸ” Batch 83/90  |  Loss: 32.8385\n",
      "  ğŸ” Batch 84/90  |  Loss: 36.4688\n",
      "  ğŸ” Batch 85/90  |  Loss: 27.9905\n",
      "  ğŸ” Batch 86/90  |  Loss: 48.0398\n",
      "  ğŸ” Batch 87/90  |  Loss: 47.2209\n",
      "  ğŸ” Batch 88/90  |  Loss: 54.4556\n",
      "  ğŸ” Batch 89/90  |  Loss: 43.1397\n",
      "  ğŸ” Batch 90/90  |  Loss: 20.4657\n",
      "\n",
      "ğŸ“Š Epoch 7 Summary:\n",
      "   âœ… Accuracy: 0.1472\n",
      "   ğŸ” Recall:   0.1472\n",
      "   â­ F1 Score: 0.1437\n",
      "\n",
      "ğŸŒ€ Epoch 8/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 26.7088\n",
      "  ğŸ” Batch 2/90  |  Loss: 44.8372\n",
      "  ğŸ” Batch 3/90  |  Loss: 26.2703\n",
      "  ğŸ” Batch 4/90  |  Loss: 33.0706\n",
      "  ğŸ” Batch 5/90  |  Loss: 47.5518\n",
      "  ğŸ” Batch 6/90  |  Loss: 18.7725\n",
      "  ğŸ” Batch 7/90  |  Loss: 26.4367\n",
      "  ğŸ” Batch 8/90  |  Loss: 36.5902\n",
      "  ğŸ” Batch 9/90  |  Loss: 27.1798\n",
      "  ğŸ” Batch 10/90  |  Loss: 33.9606\n",
      "  ğŸ” Batch 11/90  |  Loss: 41.1336\n",
      "  ğŸ” Batch 12/90  |  Loss: 33.6007\n",
      "  ğŸ” Batch 13/90  |  Loss: 32.2968\n",
      "  ğŸ” Batch 14/90  |  Loss: 36.5056\n",
      "  ğŸ” Batch 15/90  |  Loss: 31.0246\n",
      "  ğŸ” Batch 16/90  |  Loss: 35.0622\n",
      "  ğŸ” Batch 17/90  |  Loss: 44.8691\n",
      "  ğŸ” Batch 18/90  |  Loss: 37.8415\n",
      "  ğŸ” Batch 19/90  |  Loss: 21.0166\n",
      "  ğŸ” Batch 20/90  |  Loss: 32.4510\n",
      "  ğŸ” Batch 21/90  |  Loss: 26.8486\n",
      "  ğŸ” Batch 22/90  |  Loss: 39.0158\n",
      "  ğŸ” Batch 23/90  |  Loss: 16.1954\n",
      "  ğŸ” Batch 24/90  |  Loss: 34.9173\n",
      "  ğŸ” Batch 25/90  |  Loss: 10.9005\n",
      "  ğŸ” Batch 26/90  |  Loss: 34.4297\n",
      "  ğŸ” Batch 27/90  |  Loss: 29.8868\n",
      "  ğŸ” Batch 28/90  |  Loss: 31.1776\n",
      "  ğŸ” Batch 29/90  |  Loss: 43.6464\n",
      "  ğŸ” Batch 30/90  |  Loss: 11.2866\n",
      "  ğŸ” Batch 31/90  |  Loss: 50.4141\n",
      "  ğŸ” Batch 32/90  |  Loss: 19.4371\n",
      "  ğŸ” Batch 33/90  |  Loss: 25.5449\n",
      "  ğŸ” Batch 34/90  |  Loss: 59.5706\n",
      "  ğŸ” Batch 35/90  |  Loss: 32.8469\n",
      "  ğŸ” Batch 36/90  |  Loss: 47.5098\n",
      "  ğŸ” Batch 37/90  |  Loss: 42.5548\n",
      "  ğŸ” Batch 38/90  |  Loss: 47.1048\n",
      "  ğŸ” Batch 39/90  |  Loss: 36.3076\n",
      "  ğŸ” Batch 40/90  |  Loss: 46.7611\n",
      "  ğŸ” Batch 41/90  |  Loss: 26.6719\n",
      "  ğŸ” Batch 42/90  |  Loss: 31.9352\n",
      "  ğŸ” Batch 43/90  |  Loss: 21.5717\n",
      "  ğŸ” Batch 44/90  |  Loss: 26.0537\n",
      "  ğŸ” Batch 45/90  |  Loss: 30.6003\n",
      "  ğŸ” Batch 46/90  |  Loss: 26.4146\n",
      "  ğŸ” Batch 47/90  |  Loss: 13.5853\n",
      "  ğŸ” Batch 48/90  |  Loss: 28.6237\n",
      "  ğŸ” Batch 49/90  |  Loss: 17.8243\n",
      "  ğŸ” Batch 50/90  |  Loss: 37.9997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 51/90  |  Loss: 25.8688\n",
      "  ğŸ” Batch 52/90  |  Loss: 37.2398\n",
      "  ğŸ” Batch 53/90  |  Loss: 29.4791\n",
      "  ğŸ” Batch 54/90  |  Loss: 28.3254\n",
      "  ğŸ” Batch 55/90  |  Loss: 65.4359\n",
      "  ğŸ” Batch 56/90  |  Loss: 30.1027\n",
      "  ğŸ” Batch 57/90  |  Loss: 32.4709\n",
      "  ğŸ” Batch 58/90  |  Loss: 54.9029\n",
      "  ğŸ” Batch 59/90  |  Loss: 19.5084\n",
      "  ğŸ” Batch 60/90  |  Loss: 51.1944\n",
      "  ğŸ” Batch 61/90  |  Loss: 22.8023\n",
      "  ğŸ” Batch 62/90  |  Loss: 21.8436\n",
      "  ğŸ” Batch 63/90  |  Loss: 18.4970\n",
      "  ğŸ” Batch 64/90  |  Loss: 39.9810\n",
      "  ğŸ” Batch 65/90  |  Loss: 40.1784\n",
      "  ğŸ” Batch 66/90  |  Loss: 28.5825\n",
      "  ğŸ” Batch 67/90  |  Loss: 35.9120\n",
      "  ğŸ” Batch 68/90  |  Loss: 49.8991\n",
      "  ğŸ” Batch 69/90  |  Loss: 58.9919\n",
      "  ğŸ” Batch 70/90  |  Loss: 41.0803\n",
      "  ğŸ” Batch 71/90  |  Loss: 38.7223\n",
      "  ğŸ” Batch 72/90  |  Loss: 30.7252\n",
      "  ğŸ” Batch 73/90  |  Loss: 41.7815\n",
      "  ğŸ” Batch 74/90  |  Loss: 43.1066\n",
      "  ğŸ” Batch 75/90  |  Loss: 39.4997\n",
      "  ğŸ” Batch 76/90  |  Loss: 39.1537\n",
      "  ğŸ” Batch 77/90  |  Loss: 47.3061\n",
      "  ğŸ” Batch 78/90  |  Loss: 84.4586\n",
      "  ğŸ” Batch 79/90  |  Loss: 55.1429\n",
      "  ğŸ” Batch 80/90  |  Loss: 42.2084\n",
      "  ğŸ” Batch 81/90  |  Loss: 92.3772\n",
      "  ğŸ” Batch 82/90  |  Loss: 84.0279\n",
      "  ğŸ” Batch 83/90  |  Loss: 44.6101\n",
      "  ğŸ” Batch 84/90  |  Loss: 21.1280\n",
      "  ğŸ” Batch 85/90  |  Loss: 43.0876\n",
      "  ğŸ” Batch 86/90  |  Loss: 30.0985\n",
      "  ğŸ” Batch 87/90  |  Loss: 32.0522\n",
      "  ğŸ” Batch 88/90  |  Loss: 39.6902\n",
      "  ğŸ” Batch 89/90  |  Loss: 47.1745\n",
      "  ğŸ” Batch 90/90  |  Loss: 50.8207\n",
      "\n",
      "ğŸ“Š Epoch 8 Summary:\n",
      "   âœ… Accuracy: 0.1306\n",
      "   ğŸ” Recall:   0.1306\n",
      "   â­ F1 Score: 0.1286\n",
      "\n",
      "ğŸŒ€ Epoch 9/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 50.7682\n",
      "  ğŸ” Batch 2/90  |  Loss: 13.9521\n",
      "  ğŸ” Batch 3/90  |  Loss: 32.4820\n",
      "  ğŸ” Batch 4/90  |  Loss: 50.1893\n",
      "  ğŸ” Batch 5/90  |  Loss: 36.7847\n",
      "  ğŸ” Batch 6/90  |  Loss: 53.0492\n",
      "  ğŸ” Batch 7/90  |  Loss: 40.3701\n",
      "  ğŸ” Batch 8/90  |  Loss: 19.6448\n",
      "  ğŸ” Batch 9/90  |  Loss: 34.4487\n",
      "  ğŸ” Batch 10/90  |  Loss: 10.2852\n",
      "  ğŸ” Batch 11/90  |  Loss: 19.6010\n",
      "  ğŸ” Batch 12/90  |  Loss: 54.3355\n",
      "  ğŸ” Batch 13/90  |  Loss: 19.6298\n",
      "  ğŸ” Batch 14/90  |  Loss: 46.1922\n",
      "  ğŸ” Batch 15/90  |  Loss: 60.2676\n",
      "  ğŸ” Batch 16/90  |  Loss: 39.8165\n",
      "  ğŸ” Batch 17/90  |  Loss: 44.6023\n",
      "  ğŸ” Batch 18/90  |  Loss: 30.2623\n",
      "  ğŸ” Batch 19/90  |  Loss: 61.7368\n",
      "  ğŸ” Batch 20/90  |  Loss: 53.7840\n",
      "  ğŸ” Batch 21/90  |  Loss: 33.0047\n",
      "  ğŸ” Batch 22/90  |  Loss: 24.8078\n",
      "  ğŸ” Batch 23/90  |  Loss: 42.4280\n",
      "  ğŸ” Batch 24/90  |  Loss: 28.9349\n",
      "  ğŸ” Batch 25/90  |  Loss: 53.7183\n",
      "  ğŸ” Batch 26/90  |  Loss: 45.0098\n",
      "  ğŸ” Batch 27/90  |  Loss: 22.7934\n",
      "  ğŸ” Batch 28/90  |  Loss: 10.2120\n",
      "  ğŸ” Batch 29/90  |  Loss: 30.1805\n",
      "  ğŸ” Batch 30/90  |  Loss: 63.2477\n",
      "  ğŸ” Batch 31/90  |  Loss: 48.6573\n",
      "  ğŸ” Batch 32/90  |  Loss: 25.7722\n",
      "  ğŸ” Batch 33/90  |  Loss: 30.3150\n",
      "  ğŸ” Batch 34/90  |  Loss: 50.1200\n",
      "  ğŸ” Batch 35/90  |  Loss: 51.5741\n",
      "  ğŸ” Batch 36/90  |  Loss: 23.7632\n",
      "  ğŸ” Batch 37/90  |  Loss: 51.3173\n",
      "  ğŸ” Batch 38/90  |  Loss: 16.1719\n",
      "  ğŸ” Batch 39/90  |  Loss: 24.6640\n",
      "  ğŸ” Batch 40/90  |  Loss: 32.9258\n",
      "  ğŸ” Batch 41/90  |  Loss: 21.2479\n",
      "  ğŸ” Batch 42/90  |  Loss: 39.3323\n",
      "  ğŸ” Batch 43/90  |  Loss: 35.5821\n",
      "  ğŸ” Batch 44/90  |  Loss: 30.2739\n",
      "  ğŸ” Batch 45/90  |  Loss: 46.5706\n",
      "  ğŸ” Batch 46/90  |  Loss: 54.4547\n",
      "  ğŸ” Batch 47/90  |  Loss: 44.9957\n",
      "  ğŸ” Batch 48/90  |  Loss: 62.9643\n",
      "  ğŸ” Batch 49/90  |  Loss: 49.8392\n",
      "  ğŸ” Batch 50/90  |  Loss: 17.9267\n",
      "  ğŸ” Batch 51/90  |  Loss: 46.0937\n",
      "  ğŸ” Batch 52/90  |  Loss: 19.2275\n",
      "  ğŸ” Batch 53/90  |  Loss: 31.3349\n",
      "  ğŸ” Batch 54/90  |  Loss: 17.8756\n",
      "  ğŸ” Batch 55/90  |  Loss: 21.2015\n",
      "  ğŸ” Batch 56/90  |  Loss: 56.6650\n",
      "  ğŸ” Batch 57/90  |  Loss: 52.1294\n",
      "  ğŸ” Batch 58/90  |  Loss: 23.1631\n",
      "  ğŸ” Batch 59/90  |  Loss: 45.3972\n",
      "  ğŸ” Batch 60/90  |  Loss: 16.0479\n",
      "  ğŸ” Batch 61/90  |  Loss: 41.4568\n",
      "  ğŸ” Batch 62/90  |  Loss: 25.9648\n",
      "  ğŸ” Batch 63/90  |  Loss: 33.7451\n",
      "  ğŸ” Batch 64/90  |  Loss: 51.7091\n",
      "  ğŸ” Batch 65/90  |  Loss: 40.4776\n",
      "  ğŸ” Batch 66/90  |  Loss: 44.7696\n",
      "  ğŸ” Batch 67/90  |  Loss: 29.1572\n",
      "  ğŸ” Batch 68/90  |  Loss: 24.9737\n",
      "  ğŸ” Batch 69/90  |  Loss: 17.1751\n",
      "  ğŸ” Batch 70/90  |  Loss: 16.4738\n",
      "  ğŸ” Batch 71/90  |  Loss: 42.6608\n",
      "  ğŸ” Batch 72/90  |  Loss: 52.5291\n",
      "  ğŸ” Batch 73/90  |  Loss: 35.3928\n",
      "  ğŸ” Batch 74/90  |  Loss: 40.9526\n",
      "  ğŸ” Batch 75/90  |  Loss: 35.5735\n",
      "  ğŸ” Batch 76/90  |  Loss: 24.1515\n",
      "  ğŸ” Batch 77/90  |  Loss: 59.2640\n",
      "  ğŸ” Batch 78/90  |  Loss: 31.7981\n",
      "  ğŸ” Batch 79/90  |  Loss: 24.7367\n",
      "  ğŸ” Batch 80/90  |  Loss: 59.3364\n",
      "  ğŸ” Batch 81/90  |  Loss: 25.5160\n",
      "  ğŸ” Batch 82/90  |  Loss: 41.3399\n",
      "  ğŸ” Batch 83/90  |  Loss: 53.8360\n",
      "  ğŸ” Batch 84/90  |  Loss: 35.1737\n",
      "  ğŸ” Batch 85/90  |  Loss: 45.3416\n",
      "  ğŸ” Batch 86/90  |  Loss: 62.2412\n",
      "  ğŸ” Batch 87/90  |  Loss: 31.1615\n",
      "  ğŸ” Batch 88/90  |  Loss: 66.5004\n",
      "  ğŸ” Batch 89/90  |  Loss: 52.9609\n",
      "  ğŸ” Batch 90/90  |  Loss: 17.5704\n",
      "\n",
      "ğŸ“Š Epoch 9 Summary:\n",
      "   âœ… Accuracy: 0.1514\n",
      "   ğŸ” Recall:   0.1514\n",
      "   â­ F1 Score: 0.1510\n",
      "\n",
      "ğŸŒ€ Epoch 10/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 46.0831\n",
      "  ğŸ” Batch 2/90  |  Loss: 36.8233\n",
      "  ğŸ” Batch 3/90  |  Loss: 37.4620\n",
      "  ğŸ” Batch 4/90  |  Loss: 10.8939\n",
      "  ğŸ” Batch 5/90  |  Loss: 22.2410\n",
      "  ğŸ” Batch 6/90  |  Loss: 53.4793\n",
      "  ğŸ” Batch 7/90  |  Loss: 42.8868\n",
      "  ğŸ” Batch 8/90  |  Loss: 34.3574\n",
      "  ğŸ” Batch 9/90  |  Loss: 28.4129\n",
      "  ğŸ” Batch 10/90  |  Loss: 62.3515\n",
      "  ğŸ” Batch 11/90  |  Loss: 45.9452\n",
      "  ğŸ” Batch 12/90  |  Loss: 36.9271\n",
      "  ğŸ” Batch 13/90  |  Loss: 47.6104\n",
      "  ğŸ” Batch 14/90  |  Loss: 31.8238\n",
      "  ğŸ” Batch 15/90  |  Loss: 35.3544\n",
      "  ğŸ” Batch 16/90  |  Loss: 56.5102\n",
      "  ğŸ” Batch 17/90  |  Loss: 45.4891\n",
      "  ğŸ” Batch 18/90  |  Loss: 15.3650\n",
      "  ğŸ” Batch 19/90  |  Loss: 14.0690\n",
      "  ğŸ” Batch 20/90  |  Loss: 33.8518\n",
      "  ğŸ” Batch 21/90  |  Loss: 24.2136\n",
      "  ğŸ” Batch 22/90  |  Loss: 59.7472\n",
      "  ğŸ” Batch 23/90  |  Loss: 55.9654\n",
      "  ğŸ” Batch 24/90  |  Loss: 31.6202\n",
      "  ğŸ” Batch 25/90  |  Loss: 35.0202\n",
      "  ğŸ” Batch 26/90  |  Loss: 31.9675\n",
      "  ğŸ” Batch 27/90  |  Loss: 29.2671\n",
      "  ğŸ” Batch 28/90  |  Loss: 33.9811\n",
      "  ğŸ” Batch 29/90  |  Loss: 31.3250\n",
      "  ğŸ” Batch 30/90  |  Loss: 20.1127\n",
      "  ğŸ” Batch 31/90  |  Loss: 16.3259\n",
      "  ğŸ” Batch 32/90  |  Loss: 31.3488\n",
      "  ğŸ” Batch 33/90  |  Loss: 19.6260\n",
      "  ğŸ” Batch 34/90  |  Loss: 32.4487\n",
      "  ğŸ” Batch 35/90  |  Loss: 35.1774\n",
      "  ğŸ” Batch 36/90  |  Loss: 30.7758\n",
      "  ğŸ” Batch 37/90  |  Loss: 26.7789\n",
      "  ğŸ” Batch 38/90  |  Loss: 48.7480\n",
      "  ğŸ” Batch 39/90  |  Loss: 30.2676\n",
      "  ğŸ” Batch 40/90  |  Loss: 12.1965\n",
      "  ğŸ” Batch 41/90  |  Loss: 13.3396\n",
      "  ğŸ” Batch 42/90  |  Loss: 46.8855\n",
      "  ğŸ” Batch 43/90  |  Loss: 28.0785\n",
      "  ğŸ” Batch 44/90  |  Loss: 22.2925\n",
      "  ğŸ” Batch 45/90  |  Loss: 12.7766\n",
      "  ğŸ” Batch 46/90  |  Loss: 86.3644\n",
      "  ğŸ” Batch 47/90  |  Loss: 14.2883\n",
      "  ğŸ” Batch 48/90  |  Loss: 26.7501\n",
      "  ğŸ” Batch 49/90  |  Loss: 50.2904\n",
      "  ğŸ” Batch 50/90  |  Loss: 31.6772\n",
      "  ğŸ” Batch 51/90  |  Loss: 45.5574\n",
      "  ğŸ” Batch 52/90  |  Loss: 26.0952\n",
      "  ğŸ” Batch 53/90  |  Loss: 59.2534\n",
      "  ğŸ” Batch 54/90  |  Loss: 37.6806\n",
      "  ğŸ” Batch 55/90  |  Loss: 42.6393\n",
      "  ğŸ” Batch 56/90  |  Loss: 36.2861\n",
      "  ğŸ” Batch 57/90  |  Loss: 43.7244\n",
      "  ğŸ” Batch 58/90  |  Loss: 49.2354\n",
      "  ğŸ” Batch 59/90  |  Loss: 27.9043\n",
      "  ğŸ” Batch 60/90  |  Loss: 46.3121\n",
      "  ğŸ” Batch 61/90  |  Loss: 33.7826\n",
      "  ğŸ” Batch 62/90  |  Loss: 29.5785\n",
      "  ğŸ” Batch 63/90  |  Loss: 27.8241\n",
      "  ğŸ” Batch 64/90  |  Loss: 29.9260\n",
      "  ğŸ” Batch 65/90  |  Loss: 30.9550\n",
      "  ğŸ” Batch 66/90  |  Loss: 47.6657\n",
      "  ğŸ” Batch 67/90  |  Loss: 22.2204\n",
      "  ğŸ” Batch 68/90  |  Loss: 53.1193\n",
      "  ğŸ” Batch 69/90  |  Loss: 41.9731\n",
      "  ğŸ” Batch 70/90  |  Loss: 44.9080\n",
      "  ğŸ” Batch 71/90  |  Loss: 47.3423\n",
      "  ğŸ” Batch 72/90  |  Loss: 31.3431\n",
      "  ğŸ” Batch 73/90  |  Loss: 24.1687\n",
      "  ğŸ” Batch 74/90  |  Loss: 34.0549\n",
      "  ğŸ” Batch 75/90  |  Loss: 25.3179\n",
      "  ğŸ” Batch 76/90  |  Loss: 39.1708\n",
      "  ğŸ” Batch 77/90  |  Loss: 33.1722\n",
      "  ğŸ” Batch 78/90  |  Loss: 32.3790\n",
      "  ğŸ” Batch 79/90  |  Loss: 23.8840\n",
      "  ğŸ” Batch 80/90  |  Loss: 24.1480\n",
      "  ğŸ” Batch 81/90  |  Loss: 28.4563\n",
      "  ğŸ” Batch 82/90  |  Loss: 21.0680\n",
      "  ğŸ” Batch 83/90  |  Loss: 12.2623\n",
      "  ğŸ” Batch 84/90  |  Loss: 37.2071\n",
      "  ğŸ” Batch 85/90  |  Loss: 35.0510\n",
      "  ğŸ” Batch 86/90  |  Loss: 43.5521\n",
      "  ğŸ” Batch 87/90  |  Loss: 26.9322\n",
      "  ğŸ” Batch 88/90  |  Loss: 53.1418\n",
      "  ğŸ” Batch 89/90  |  Loss: 63.2900\n",
      "  ğŸ” Batch 90/90  |  Loss: 54.6318\n",
      "\n",
      "ğŸ“Š Epoch 10 Summary:\n",
      "   âœ… Accuracy: 0.1861\n",
      "   ğŸ” Recall:   0.1861\n",
      "   â­ F1 Score: 0.1823\n",
      "\n",
      "ğŸŒ€ Epoch 11/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 54.5118\n",
      "  ğŸ” Batch 2/90  |  Loss: 24.2027\n",
      "  ğŸ” Batch 3/90  |  Loss: 36.2833\n",
      "  ğŸ” Batch 4/90  |  Loss: 32.9966\n",
      "  ğŸ” Batch 5/90  |  Loss: 11.8936\n",
      "  ğŸ” Batch 6/90  |  Loss: 30.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 7/90  |  Loss: 17.2320\n",
      "  ğŸ” Batch 8/90  |  Loss: 35.6360\n",
      "  ğŸ” Batch 9/90  |  Loss: 40.9141\n",
      "  ğŸ” Batch 10/90  |  Loss: 40.7523\n",
      "  ğŸ” Batch 11/90  |  Loss: 31.6090\n",
      "  ğŸ” Batch 12/90  |  Loss: 35.4021\n",
      "  ğŸ” Batch 13/90  |  Loss: 58.4334\n",
      "  ğŸ” Batch 14/90  |  Loss: 35.3891\n",
      "  ğŸ” Batch 15/90  |  Loss: 24.6144\n",
      "  ğŸ” Batch 16/90  |  Loss: 49.4355\n",
      "  ğŸ” Batch 17/90  |  Loss: 23.2497\n",
      "  ğŸ” Batch 18/90  |  Loss: 19.9942\n",
      "  ğŸ” Batch 19/90  |  Loss: 23.3066\n",
      "  ğŸ” Batch 20/90  |  Loss: 29.7293\n",
      "  ğŸ” Batch 21/90  |  Loss: 11.6121\n",
      "  ğŸ” Batch 22/90  |  Loss: 49.2858\n",
      "  ğŸ” Batch 23/90  |  Loss: 36.6953\n",
      "  ğŸ” Batch 24/90  |  Loss: 44.4720\n",
      "  ğŸ” Batch 25/90  |  Loss: 19.1903\n",
      "  ğŸ” Batch 26/90  |  Loss: 47.4508\n",
      "  ğŸ” Batch 27/90  |  Loss: 33.3648\n",
      "  ğŸ” Batch 28/90  |  Loss: 26.3356\n",
      "  ğŸ” Batch 29/90  |  Loss: 82.0637\n",
      "  ğŸ” Batch 30/90  |  Loss: 59.6369\n",
      "  ğŸ” Batch 31/90  |  Loss: 64.1171\n",
      "  ğŸ” Batch 32/90  |  Loss: 31.9518\n",
      "  ğŸ” Batch 33/90  |  Loss: 34.4216\n",
      "  ğŸ” Batch 34/90  |  Loss: 15.1193\n",
      "  ğŸ” Batch 35/90  |  Loss: 35.6269\n",
      "  ğŸ” Batch 36/90  |  Loss: 43.2561\n",
      "  ğŸ” Batch 37/90  |  Loss: 35.1226\n",
      "  ğŸ” Batch 38/90  |  Loss: 23.5763\n",
      "  ğŸ” Batch 39/90  |  Loss: 51.7674\n",
      "  ğŸ” Batch 40/90  |  Loss: 27.3840\n",
      "  ğŸ” Batch 41/90  |  Loss: 30.7339\n",
      "  ğŸ” Batch 42/90  |  Loss: 42.3764\n",
      "  ğŸ” Batch 43/90  |  Loss: 24.1062\n",
      "  ğŸ” Batch 44/90  |  Loss: 26.8477\n",
      "  ğŸ” Batch 45/90  |  Loss: 29.9061\n",
      "  ğŸ” Batch 46/90  |  Loss: 31.5453\n",
      "  ğŸ” Batch 47/90  |  Loss: 30.8274\n",
      "  ğŸ” Batch 48/90  |  Loss: 30.0364\n",
      "  ğŸ” Batch 49/90  |  Loss: 31.3020\n",
      "  ğŸ” Batch 50/90  |  Loss: 41.9415\n",
      "  ğŸ” Batch 51/90  |  Loss: 71.6622\n",
      "  ğŸ” Batch 52/90  |  Loss: 35.0661\n",
      "  ğŸ” Batch 53/90  |  Loss: 46.8504\n",
      "  ğŸ” Batch 54/90  |  Loss: 26.9451\n",
      "  ğŸ” Batch 55/90  |  Loss: 45.9096\n",
      "  ğŸ” Batch 56/90  |  Loss: 59.1351\n",
      "  ğŸ” Batch 57/90  |  Loss: 26.0155\n",
      "  ğŸ” Batch 58/90  |  Loss: 26.1815\n",
      "  ğŸ” Batch 59/90  |  Loss: 18.6772\n",
      "  ğŸ” Batch 60/90  |  Loss: 44.5052\n",
      "  ğŸ” Batch 61/90  |  Loss: 20.5478\n",
      "  ğŸ” Batch 62/90  |  Loss: 45.5165\n",
      "  ğŸ” Batch 63/90  |  Loss: 16.3803\n",
      "  ğŸ” Batch 64/90  |  Loss: 30.9336\n",
      "  ğŸ” Batch 65/90  |  Loss: 26.5196\n",
      "  ğŸ” Batch 66/90  |  Loss: 36.4175\n",
      "  ğŸ” Batch 67/90  |  Loss: 30.8914\n",
      "  ğŸ” Batch 68/90  |  Loss: 35.8792\n",
      "  ğŸ” Batch 69/90  |  Loss: 31.0543\n",
      "  ğŸ” Batch 70/90  |  Loss: 23.5488\n",
      "  ğŸ” Batch 71/90  |  Loss: 35.9198\n",
      "  ğŸ” Batch 72/90  |  Loss: 36.4510\n",
      "  ğŸ” Batch 73/90  |  Loss: 64.8260\n",
      "  ğŸ” Batch 74/90  |  Loss: 24.0084\n",
      "  ğŸ” Batch 75/90  |  Loss: 29.8640\n",
      "  ğŸ” Batch 76/90  |  Loss: 47.2982\n",
      "  ğŸ” Batch 77/90  |  Loss: 38.1982\n",
      "  ğŸ” Batch 78/90  |  Loss: 58.8621\n",
      "  ğŸ” Batch 79/90  |  Loss: 35.0908\n",
      "  ğŸ” Batch 80/90  |  Loss: 32.4054\n",
      "  ğŸ” Batch 81/90  |  Loss: 29.2368\n",
      "  ğŸ” Batch 82/90  |  Loss: 26.1455\n",
      "  ğŸ” Batch 83/90  |  Loss: 49.5533\n",
      "  ğŸ” Batch 84/90  |  Loss: 34.9844\n",
      "  ğŸ” Batch 85/90  |  Loss: 14.2572\n",
      "  ğŸ” Batch 86/90  |  Loss: 32.7891\n",
      "  ğŸ” Batch 87/90  |  Loss: 19.9763\n",
      "  ğŸ” Batch 88/90  |  Loss: 55.4769\n",
      "  ğŸ” Batch 89/90  |  Loss: 28.6420\n",
      "  ğŸ” Batch 90/90  |  Loss: 33.7223\n",
      "\n",
      "ğŸ“Š Epoch 11 Summary:\n",
      "   âœ… Accuracy: 0.1667\n",
      "   ğŸ” Recall:   0.1667\n",
      "   â­ F1 Score: 0.1654\n",
      "\n",
      "ğŸŒ€ Epoch 12/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 16.7596\n",
      "  ğŸ” Batch 2/90  |  Loss: 16.0212\n",
      "  ğŸ” Batch 3/90  |  Loss: 37.6885\n",
      "  ğŸ” Batch 4/90  |  Loss: 6.4704\n",
      "  ğŸ” Batch 5/90  |  Loss: 18.6216\n",
      "  ğŸ” Batch 6/90  |  Loss: 30.5227\n",
      "  ğŸ” Batch 7/90  |  Loss: 23.3653\n",
      "  ğŸ” Batch 8/90  |  Loss: 34.9851\n",
      "  ğŸ” Batch 9/90  |  Loss: 31.1763\n",
      "  ğŸ” Batch 10/90  |  Loss: 36.3350\n",
      "  ğŸ” Batch 11/90  |  Loss: 63.2854\n",
      "  ğŸ” Batch 12/90  |  Loss: 31.4338\n",
      "  ğŸ” Batch 13/90  |  Loss: 30.5830\n",
      "  ğŸ” Batch 14/90  |  Loss: 35.6681\n",
      "  ğŸ” Batch 15/90  |  Loss: 40.4945\n",
      "  ğŸ” Batch 16/90  |  Loss: 25.5554\n",
      "  ğŸ” Batch 17/90  |  Loss: 39.3974\n",
      "  ğŸ” Batch 18/90  |  Loss: 27.7340\n",
      "  ğŸ” Batch 19/90  |  Loss: 24.9316\n",
      "  ğŸ” Batch 20/90  |  Loss: 43.2934\n",
      "  ğŸ” Batch 21/90  |  Loss: 12.6491\n",
      "  ğŸ” Batch 22/90  |  Loss: 27.9717\n",
      "  ğŸ” Batch 23/90  |  Loss: 34.9066\n",
      "  ğŸ” Batch 24/90  |  Loss: 24.0016\n",
      "  ğŸ” Batch 25/90  |  Loss: 25.2816\n",
      "  ğŸ” Batch 26/90  |  Loss: 47.3125\n",
      "  ğŸ” Batch 27/90  |  Loss: 22.0416\n",
      "  ğŸ” Batch 28/90  |  Loss: 24.9830\n",
      "  ğŸ” Batch 29/90  |  Loss: 32.5404\n",
      "  ğŸ” Batch 30/90  |  Loss: 25.1769\n",
      "  ğŸ” Batch 31/90  |  Loss: 24.1815\n",
      "  ğŸ” Batch 32/90  |  Loss: 26.5758\n",
      "  ğŸ” Batch 33/90  |  Loss: 41.2813\n",
      "  ğŸ” Batch 34/90  |  Loss: 8.9563\n",
      "  ğŸ” Batch 35/90  |  Loss: 34.6764\n",
      "  ğŸ” Batch 36/90  |  Loss: 43.8209\n",
      "  ğŸ” Batch 37/90  |  Loss: 13.1109\n",
      "  ğŸ” Batch 38/90  |  Loss: 42.8304\n",
      "  ğŸ” Batch 39/90  |  Loss: 30.8015\n",
      "  ğŸ” Batch 40/90  |  Loss: 42.6155\n",
      "  ğŸ” Batch 41/90  |  Loss: 27.9686\n",
      "  ğŸ” Batch 42/90  |  Loss: 17.7956\n",
      "  ğŸ” Batch 43/90  |  Loss: 34.6630\n",
      "  ğŸ” Batch 44/90  |  Loss: 51.2178\n",
      "  ğŸ” Batch 45/90  |  Loss: 23.8545\n",
      "  ğŸ” Batch 46/90  |  Loss: 15.3252\n",
      "  ğŸ” Batch 47/90  |  Loss: 39.8215\n",
      "  ğŸ” Batch 48/90  |  Loss: 38.4406\n",
      "  ğŸ” Batch 49/90  |  Loss: 13.5358\n",
      "  ğŸ” Batch 50/90  |  Loss: 16.7010\n",
      "  ğŸ” Batch 51/90  |  Loss: 31.4734\n",
      "  ğŸ” Batch 52/90  |  Loss: 46.1344\n",
      "  ğŸ” Batch 53/90  |  Loss: 16.9134\n",
      "  ğŸ” Batch 54/90  |  Loss: 27.1275\n",
      "  ğŸ” Batch 55/90  |  Loss: 29.3697\n",
      "  ğŸ” Batch 56/90  |  Loss: 19.2702\n",
      "  ğŸ” Batch 57/90  |  Loss: 31.5355\n",
      "  ğŸ” Batch 58/90  |  Loss: 28.1037\n",
      "  ğŸ” Batch 59/90  |  Loss: 20.7557\n",
      "  ğŸ” Batch 60/90  |  Loss: 26.3598\n",
      "  ğŸ” Batch 61/90  |  Loss: 23.1464\n",
      "  ğŸ” Batch 62/90  |  Loss: 12.7390\n",
      "  ğŸ” Batch 63/90  |  Loss: 20.6067\n",
      "  ğŸ” Batch 64/90  |  Loss: 34.0122\n",
      "  ğŸ” Batch 65/90  |  Loss: 14.0851\n",
      "  ğŸ” Batch 66/90  |  Loss: 15.6674\n",
      "  ğŸ” Batch 67/90  |  Loss: 30.6110\n",
      "  ğŸ” Batch 68/90  |  Loss: 22.7122\n",
      "  ğŸ” Batch 69/90  |  Loss: 38.4476\n",
      "  ğŸ” Batch 70/90  |  Loss: 61.6396\n",
      "  ğŸ” Batch 71/90  |  Loss: 14.5714\n",
      "  ğŸ” Batch 72/90  |  Loss: 54.4852\n",
      "  ğŸ” Batch 73/90  |  Loss: 31.8864\n",
      "  ğŸ” Batch 74/90  |  Loss: 47.0220\n",
      "  ğŸ” Batch 75/90  |  Loss: 34.7738\n",
      "  ğŸ” Batch 76/90  |  Loss: 18.2770\n",
      "  ğŸ” Batch 77/90  |  Loss: 57.5356\n",
      "  ğŸ” Batch 78/90  |  Loss: 21.4555\n",
      "  ğŸ” Batch 79/90  |  Loss: 56.8245\n",
      "  ğŸ” Batch 80/90  |  Loss: 77.0072\n",
      "  ğŸ” Batch 81/90  |  Loss: 26.4745\n",
      "  ğŸ” Batch 82/90  |  Loss: 23.4844\n",
      "  ğŸ” Batch 83/90  |  Loss: 42.8355\n",
      "  ğŸ” Batch 84/90  |  Loss: 29.2320\n",
      "  ğŸ” Batch 85/90  |  Loss: 35.4106\n",
      "  ğŸ” Batch 86/90  |  Loss: 35.2080\n",
      "  ğŸ” Batch 87/90  |  Loss: 30.0626\n",
      "  ğŸ” Batch 88/90  |  Loss: 24.0119\n",
      "  ğŸ” Batch 89/90  |  Loss: 35.2891\n",
      "  ğŸ” Batch 90/90  |  Loss: 33.5107\n",
      "\n",
      "ğŸ“Š Epoch 12 Summary:\n",
      "   âœ… Accuracy: 0.2000\n",
      "   ğŸ” Recall:   0.2000\n",
      "   â­ F1 Score: 0.2000\n",
      "\n",
      "ğŸŒ€ Epoch 13/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 23.2911\n",
      "  ğŸ” Batch 2/90  |  Loss: 29.8942\n",
      "  ğŸ” Batch 3/90  |  Loss: 48.9333\n",
      "  ğŸ” Batch 4/90  |  Loss: 28.2798\n",
      "  ğŸ” Batch 5/90  |  Loss: 52.5514\n",
      "  ğŸ” Batch 6/90  |  Loss: 62.8912\n",
      "  ğŸ” Batch 7/90  |  Loss: 54.8946\n",
      "  ğŸ” Batch 8/90  |  Loss: 25.6203\n",
      "  ğŸ” Batch 9/90  |  Loss: 30.3696\n",
      "  ğŸ” Batch 10/90  |  Loss: 54.2046\n",
      "  ğŸ” Batch 11/90  |  Loss: 50.9425\n",
      "  ğŸ” Batch 12/90  |  Loss: 22.7715\n",
      "  ğŸ” Batch 13/90  |  Loss: 51.3151\n",
      "  ğŸ” Batch 14/90  |  Loss: 22.3461\n",
      "  ğŸ” Batch 15/90  |  Loss: 52.4370\n",
      "  ğŸ” Batch 16/90  |  Loss: 27.3652\n",
      "  ğŸ” Batch 17/90  |  Loss: 16.5196\n",
      "  ğŸ” Batch 18/90  |  Loss: 25.0564\n",
      "  ğŸ” Batch 19/90  |  Loss: 24.5939\n",
      "  ğŸ” Batch 20/90  |  Loss: 43.6144\n",
      "  ğŸ” Batch 21/90  |  Loss: 16.6319\n",
      "  ğŸ” Batch 22/90  |  Loss: 12.2084\n",
      "  ğŸ” Batch 23/90  |  Loss: 52.9410\n",
      "  ğŸ” Batch 24/90  |  Loss: 22.8170\n",
      "  ğŸ” Batch 25/90  |  Loss: 33.9427\n",
      "  ğŸ” Batch 26/90  |  Loss: 15.1301\n",
      "  ğŸ” Batch 27/90  |  Loss: 46.0705\n",
      "  ğŸ” Batch 28/90  |  Loss: 24.5856\n",
      "  ğŸ” Batch 29/90  |  Loss: 25.4287\n",
      "  ğŸ” Batch 30/90  |  Loss: 20.1871\n",
      "  ğŸ” Batch 31/90  |  Loss: 38.2703\n",
      "  ğŸ” Batch 32/90  |  Loss: 41.5361\n",
      "  ğŸ” Batch 33/90  |  Loss: 20.8138\n",
      "  ğŸ” Batch 34/90  |  Loss: 24.9383\n",
      "  ğŸ” Batch 35/90  |  Loss: 24.1488\n",
      "  ğŸ” Batch 36/90  |  Loss: 40.7334\n",
      "  ğŸ” Batch 37/90  |  Loss: 26.7579\n",
      "  ğŸ” Batch 38/90  |  Loss: 17.0161\n",
      "  ğŸ” Batch 39/90  |  Loss: 32.2396\n",
      "  ğŸ” Batch 40/90  |  Loss: 21.3951\n",
      "  ğŸ” Batch 41/90  |  Loss: 54.2550\n",
      "  ğŸ” Batch 42/90  |  Loss: 33.5324\n",
      "  ğŸ” Batch 43/90  |  Loss: 9.4654\n",
      "  ğŸ” Batch 44/90  |  Loss: 23.1499\n",
      "  ğŸ” Batch 45/90  |  Loss: 48.3226\n",
      "  ğŸ” Batch 46/90  |  Loss: 22.2661\n",
      "  ğŸ” Batch 47/90  |  Loss: 33.2307\n",
      "  ğŸ” Batch 48/90  |  Loss: 37.0873\n",
      "  ğŸ” Batch 49/90  |  Loss: 26.3344\n",
      "  ğŸ” Batch 50/90  |  Loss: 57.2728\n",
      "  ğŸ” Batch 51/90  |  Loss: 28.1583\n",
      "  ğŸ” Batch 52/90  |  Loss: 23.7490\n",
      "  ğŸ” Batch 53/90  |  Loss: 25.3160\n",
      "  ğŸ” Batch 54/90  |  Loss: 33.7596\n",
      "  ğŸ” Batch 55/90  |  Loss: 35.0240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 56/90  |  Loss: 86.2786\n",
      "  ğŸ” Batch 57/90  |  Loss: 45.2006\n",
      "  ğŸ” Batch 58/90  |  Loss: 39.4227\n",
      "  ğŸ” Batch 59/90  |  Loss: 70.0200\n",
      "  ğŸ” Batch 60/90  |  Loss: 31.4906\n",
      "  ğŸ” Batch 61/90  |  Loss: 51.5275\n",
      "  ğŸ” Batch 62/90  |  Loss: 39.3782\n",
      "  ğŸ” Batch 63/90  |  Loss: 32.5579\n",
      "  ğŸ” Batch 64/90  |  Loss: 55.5834\n",
      "  ğŸ” Batch 65/90  |  Loss: 16.6565\n",
      "  ğŸ” Batch 66/90  |  Loss: 29.9939\n",
      "  ğŸ” Batch 67/90  |  Loss: 36.1752\n",
      "  ğŸ” Batch 68/90  |  Loss: 72.8983\n",
      "  ğŸ” Batch 69/90  |  Loss: 40.9518\n",
      "  ğŸ” Batch 70/90  |  Loss: 34.4895\n",
      "  ğŸ” Batch 71/90  |  Loss: 45.9167\n",
      "  ğŸ” Batch 72/90  |  Loss: 61.5069\n",
      "  ğŸ” Batch 73/90  |  Loss: 11.4890\n",
      "  ğŸ” Batch 74/90  |  Loss: 19.6226\n",
      "  ğŸ” Batch 75/90  |  Loss: 25.3721\n",
      "  ğŸ” Batch 76/90  |  Loss: 16.9222\n",
      "  ğŸ” Batch 77/90  |  Loss: 46.4656\n",
      "  ğŸ” Batch 78/90  |  Loss: 39.5839\n",
      "  ğŸ” Batch 79/90  |  Loss: 18.7021\n",
      "  ğŸ” Batch 80/90  |  Loss: 34.1833\n",
      "  ğŸ” Batch 81/90  |  Loss: 42.3804\n",
      "  ğŸ” Batch 82/90  |  Loss: 44.9638\n",
      "  ğŸ” Batch 83/90  |  Loss: 22.8329\n",
      "  ğŸ” Batch 84/90  |  Loss: 40.3999\n",
      "  ğŸ” Batch 85/90  |  Loss: 28.0464\n",
      "  ğŸ” Batch 86/90  |  Loss: 15.9813\n",
      "  ğŸ” Batch 87/90  |  Loss: 24.9267\n",
      "  ğŸ” Batch 88/90  |  Loss: 48.0218\n",
      "  ğŸ” Batch 89/90  |  Loss: 33.0158\n",
      "  ğŸ” Batch 90/90  |  Loss: 42.5135\n",
      "\n",
      "ğŸ“Š Epoch 13 Summary:\n",
      "   âœ… Accuracy: 0.1944\n",
      "   ğŸ” Recall:   0.1944\n",
      "   â­ F1 Score: 0.1935\n",
      "\n",
      "ğŸŒ€ Epoch 14/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 36.4155\n",
      "  ğŸ” Batch 2/90  |  Loss: 11.5951\n",
      "  ğŸ” Batch 3/90  |  Loss: 30.7942\n",
      "  ğŸ” Batch 4/90  |  Loss: 20.3704\n",
      "  ğŸ” Batch 5/90  |  Loss: 52.8928\n",
      "  ğŸ” Batch 6/90  |  Loss: 53.6462\n",
      "  ğŸ” Batch 7/90  |  Loss: 57.2406\n",
      "  ğŸ” Batch 8/90  |  Loss: 24.8689\n",
      "  ğŸ” Batch 9/90  |  Loss: 40.2135\n",
      "  ğŸ” Batch 10/90  |  Loss: 44.7017\n",
      "  ğŸ” Batch 11/90  |  Loss: 24.0977\n",
      "  ğŸ” Batch 12/90  |  Loss: 35.9886\n",
      "  ğŸ” Batch 13/90  |  Loss: 22.4298\n",
      "  ğŸ” Batch 14/90  |  Loss: 22.4656\n",
      "  ğŸ” Batch 15/90  |  Loss: 23.0262\n",
      "  ğŸ” Batch 16/90  |  Loss: 16.3923\n",
      "  ğŸ” Batch 17/90  |  Loss: 30.4789\n",
      "  ğŸ” Batch 18/90  |  Loss: 33.8486\n",
      "  ğŸ” Batch 19/90  |  Loss: 30.9689\n",
      "  ğŸ” Batch 20/90  |  Loss: 38.9093\n",
      "  ğŸ” Batch 21/90  |  Loss: 19.2365\n",
      "  ğŸ” Batch 22/90  |  Loss: 21.4289\n",
      "  ğŸ” Batch 23/90  |  Loss: 32.5448\n",
      "  ğŸ” Batch 24/90  |  Loss: 26.8871\n",
      "  ğŸ” Batch 25/90  |  Loss: 35.3096\n",
      "  ğŸ” Batch 26/90  |  Loss: 31.3241\n",
      "  ğŸ” Batch 27/90  |  Loss: 20.8331\n",
      "  ğŸ” Batch 28/90  |  Loss: 54.3913\n",
      "  ğŸ” Batch 29/90  |  Loss: 54.1595\n",
      "  ğŸ” Batch 30/90  |  Loss: 35.8050\n",
      "  ğŸ” Batch 31/90  |  Loss: 82.1126\n",
      "  ğŸ” Batch 32/90  |  Loss: 22.8864\n",
      "  ğŸ” Batch 33/90  |  Loss: 44.0409\n",
      "  ğŸ” Batch 34/90  |  Loss: 45.1669\n",
      "  ğŸ” Batch 35/90  |  Loss: 48.9291\n",
      "  ğŸ” Batch 36/90  |  Loss: 68.4897\n",
      "  ğŸ” Batch 37/90  |  Loss: 21.9883\n",
      "  ğŸ” Batch 38/90  |  Loss: 19.9021\n",
      "  ğŸ” Batch 39/90  |  Loss: 18.5711\n",
      "  ğŸ” Batch 40/90  |  Loss: 51.2214\n",
      "  ğŸ” Batch 41/90  |  Loss: 23.5730\n",
      "  ğŸ” Batch 42/90  |  Loss: 6.4618\n",
      "  ğŸ” Batch 43/90  |  Loss: 27.9122\n",
      "  ğŸ” Batch 44/90  |  Loss: 43.7173\n",
      "  ğŸ” Batch 45/90  |  Loss: 18.9507\n",
      "  ğŸ” Batch 46/90  |  Loss: 10.4031\n",
      "  ğŸ” Batch 47/90  |  Loss: 42.7917\n",
      "  ğŸ” Batch 48/90  |  Loss: 26.7034\n",
      "  ğŸ” Batch 49/90  |  Loss: 36.0823\n",
      "  ğŸ” Batch 50/90  |  Loss: 22.4300\n",
      "  ğŸ” Batch 51/90  |  Loss: 6.8004\n",
      "  ğŸ” Batch 52/90  |  Loss: 53.5841\n",
      "  ğŸ” Batch 53/90  |  Loss: 36.8088\n",
      "  ğŸ” Batch 54/90  |  Loss: 26.7297\n",
      "  ğŸ” Batch 55/90  |  Loss: 31.4625\n",
      "  ğŸ” Batch 56/90  |  Loss: 27.9048\n",
      "  ğŸ” Batch 57/90  |  Loss: 34.0436\n",
      "  ğŸ” Batch 58/90  |  Loss: 25.6566\n",
      "  ğŸ” Batch 59/90  |  Loss: 32.2817\n",
      "  ğŸ” Batch 60/90  |  Loss: 39.6925\n",
      "  ğŸ” Batch 61/90  |  Loss: 48.4919\n",
      "  ğŸ” Batch 62/90  |  Loss: 17.5249\n",
      "  ğŸ” Batch 63/90  |  Loss: 21.3545\n",
      "  ğŸ” Batch 64/90  |  Loss: 34.9099\n",
      "  ğŸ” Batch 65/90  |  Loss: 48.9048\n",
      "  ğŸ” Batch 66/90  |  Loss: 23.7802\n",
      "  ğŸ” Batch 67/90  |  Loss: 24.9564\n",
      "  ğŸ” Batch 68/90  |  Loss: 23.8751\n",
      "  ğŸ” Batch 69/90  |  Loss: 32.5259\n",
      "  ğŸ” Batch 70/90  |  Loss: 40.0480\n",
      "  ğŸ” Batch 71/90  |  Loss: 64.2263\n",
      "  ğŸ” Batch 72/90  |  Loss: 20.6791\n",
      "  ğŸ” Batch 73/90  |  Loss: 24.7583\n",
      "  ğŸ” Batch 74/90  |  Loss: 41.5296\n",
      "  ğŸ” Batch 75/90  |  Loss: 44.4050\n",
      "  ğŸ” Batch 76/90  |  Loss: 15.7678\n",
      "  ğŸ” Batch 77/90  |  Loss: 70.5191\n",
      "  ğŸ” Batch 78/90  |  Loss: 6.4081\n",
      "  ğŸ” Batch 79/90  |  Loss: 23.4656\n",
      "  ğŸ” Batch 80/90  |  Loss: 39.4982\n",
      "  ğŸ” Batch 81/90  |  Loss: 41.0350\n",
      "  ğŸ” Batch 82/90  |  Loss: 24.3264\n",
      "  ğŸ” Batch 83/90  |  Loss: 51.6010\n",
      "  ğŸ” Batch 84/90  |  Loss: 32.5393\n",
      "  ğŸ” Batch 85/90  |  Loss: 38.1276\n",
      "  ğŸ” Batch 86/90  |  Loss: 58.8885\n",
      "  ğŸ” Batch 87/90  |  Loss: 19.2400\n",
      "  ğŸ” Batch 88/90  |  Loss: 27.8376\n",
      "  ğŸ” Batch 89/90  |  Loss: 19.5956\n",
      "  ğŸ” Batch 90/90  |  Loss: 29.2710\n",
      "\n",
      "ğŸ“Š Epoch 14 Summary:\n",
      "   âœ… Accuracy: 0.2069\n",
      "   ğŸ” Recall:   0.2069\n",
      "   â­ F1 Score: 0.2070\n",
      "\n",
      "ğŸŒ€ Epoch 15/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 26.7618\n",
      "  ğŸ” Batch 2/90  |  Loss: 21.9239\n",
      "  ğŸ” Batch 3/90  |  Loss: 21.4034\n",
      "  ğŸ” Batch 4/90  |  Loss: 16.2781\n",
      "  ğŸ” Batch 5/90  |  Loss: 48.0513\n",
      "  ğŸ” Batch 6/90  |  Loss: 31.1797\n",
      "  ğŸ” Batch 7/90  |  Loss: 24.5594\n",
      "  ğŸ” Batch 8/90  |  Loss: 25.8691\n",
      "  ğŸ” Batch 9/90  |  Loss: 28.3668\n",
      "  ğŸ” Batch 10/90  |  Loss: 25.4807\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.1132\n",
      "  ğŸ” Batch 12/90  |  Loss: 26.2356\n",
      "  ğŸ” Batch 13/90  |  Loss: 42.8009\n",
      "  ğŸ” Batch 14/90  |  Loss: 15.8836\n",
      "  ğŸ” Batch 15/90  |  Loss: 19.0005\n",
      "  ğŸ” Batch 16/90  |  Loss: 33.8429\n",
      "  ğŸ” Batch 17/90  |  Loss: 45.0498\n",
      "  ğŸ” Batch 18/90  |  Loss: 21.4307\n",
      "  ğŸ” Batch 19/90  |  Loss: 12.9182\n",
      "  ğŸ” Batch 20/90  |  Loss: 7.2130\n",
      "  ğŸ” Batch 21/90  |  Loss: 34.1214\n",
      "  ğŸ” Batch 22/90  |  Loss: 16.6941\n",
      "  ğŸ” Batch 23/90  |  Loss: 15.4596\n",
      "  ğŸ” Batch 24/90  |  Loss: 66.1007\n",
      "  ğŸ” Batch 25/90  |  Loss: 24.2035\n",
      "  ğŸ” Batch 26/90  |  Loss: 36.3922\n",
      "  ğŸ” Batch 27/90  |  Loss: 17.7327\n",
      "  ğŸ” Batch 28/90  |  Loss: 47.0827\n",
      "  ğŸ” Batch 29/90  |  Loss: 31.8856\n",
      "  ğŸ” Batch 30/90  |  Loss: 48.8773\n",
      "  ğŸ” Batch 31/90  |  Loss: 60.7011\n",
      "  ğŸ” Batch 32/90  |  Loss: 33.5510\n",
      "  ğŸ” Batch 33/90  |  Loss: 42.3856\n",
      "  ğŸ” Batch 34/90  |  Loss: 37.5201\n",
      "  ğŸ” Batch 35/90  |  Loss: 38.6424\n",
      "  ğŸ” Batch 36/90  |  Loss: 32.8682\n",
      "  ğŸ” Batch 37/90  |  Loss: 28.5522\n",
      "  ğŸ” Batch 38/90  |  Loss: 51.6838\n",
      "  ğŸ” Batch 39/90  |  Loss: 19.0125\n",
      "  ğŸ” Batch 40/90  |  Loss: 24.5867\n",
      "  ğŸ” Batch 41/90  |  Loss: 21.1777\n",
      "  ğŸ” Batch 42/90  |  Loss: 46.1679\n",
      "  ğŸ” Batch 43/90  |  Loss: 21.7480\n",
      "  ğŸ” Batch 44/90  |  Loss: 55.3799\n",
      "  ğŸ” Batch 45/90  |  Loss: 61.3319\n",
      "  ğŸ” Batch 46/90  |  Loss: 36.3434\n",
      "  ğŸ” Batch 47/90  |  Loss: 30.3861\n",
      "  ğŸ” Batch 48/90  |  Loss: 36.7059\n",
      "  ğŸ” Batch 49/90  |  Loss: 43.6245\n",
      "  ğŸ” Batch 50/90  |  Loss: 72.3180\n",
      "  ğŸ” Batch 51/90  |  Loss: 79.8274\n",
      "  ğŸ” Batch 52/90  |  Loss: 55.2595\n",
      "  ğŸ” Batch 53/90  |  Loss: 27.2142\n",
      "  ğŸ” Batch 54/90  |  Loss: 32.9750\n",
      "  ğŸ” Batch 55/90  |  Loss: 10.3899\n",
      "  ğŸ” Batch 56/90  |  Loss: 65.1700\n",
      "  ğŸ” Batch 57/90  |  Loss: 26.6069\n",
      "  ğŸ” Batch 58/90  |  Loss: 38.4198\n",
      "  ğŸ” Batch 59/90  |  Loss: 36.5801\n",
      "  ğŸ” Batch 60/90  |  Loss: 26.0979\n",
      "  ğŸ” Batch 61/90  |  Loss: 62.6663\n",
      "  ğŸ” Batch 62/90  |  Loss: 45.9532\n",
      "  ğŸ” Batch 63/90  |  Loss: 57.4675\n",
      "  ğŸ” Batch 64/90  |  Loss: 56.0941\n",
      "  ğŸ” Batch 65/90  |  Loss: 35.7401\n",
      "  ğŸ” Batch 66/90  |  Loss: 40.7110\n",
      "  ğŸ” Batch 67/90  |  Loss: 46.4960\n",
      "  ğŸ” Batch 68/90  |  Loss: 48.1924\n",
      "  ğŸ” Batch 69/90  |  Loss: 14.8714\n",
      "  ğŸ” Batch 70/90  |  Loss: 18.8692\n",
      "  ğŸ” Batch 71/90  |  Loss: 32.7612\n",
      "  ğŸ” Batch 72/90  |  Loss: 20.1546\n",
      "  ğŸ” Batch 73/90  |  Loss: 42.8720\n",
      "  ğŸ” Batch 74/90  |  Loss: 24.5714\n",
      "  ğŸ” Batch 75/90  |  Loss: 29.2146\n",
      "  ğŸ” Batch 76/90  |  Loss: 18.1036\n",
      "  ğŸ” Batch 77/90  |  Loss: 33.4290\n",
      "  ğŸ” Batch 78/90  |  Loss: 48.2659\n",
      "  ğŸ” Batch 79/90  |  Loss: 12.3044\n",
      "  ğŸ” Batch 80/90  |  Loss: 20.7215\n",
      "  ğŸ” Batch 81/90  |  Loss: 38.9334\n",
      "  ğŸ” Batch 82/90  |  Loss: 21.9700\n",
      "  ğŸ” Batch 83/90  |  Loss: 29.9232\n",
      "  ğŸ” Batch 84/90  |  Loss: 42.5655\n",
      "  ğŸ” Batch 85/90  |  Loss: 9.0418\n",
      "  ğŸ” Batch 86/90  |  Loss: 29.5719\n",
      "  ğŸ” Batch 87/90  |  Loss: 38.4968\n",
      "  ğŸ” Batch 88/90  |  Loss: 36.5510\n",
      "  ğŸ” Batch 89/90  |  Loss: 13.9218\n",
      "  ğŸ” Batch 90/90  |  Loss: 52.0600\n",
      "\n",
      "ğŸ“Š Epoch 15 Summary:\n",
      "   âœ… Accuracy: 0.2431\n",
      "   ğŸ” Recall:   0.2431\n",
      "   â­ F1 Score: 0.2438\n",
      "\n",
      "ğŸŒ€ Epoch 16/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 19.5484\n",
      "  ğŸ” Batch 2/90  |  Loss: 33.6204\n",
      "  ğŸ” Batch 3/90  |  Loss: 21.5345\n",
      "  ğŸ” Batch 4/90  |  Loss: 28.7965\n",
      "  ğŸ” Batch 5/90  |  Loss: 48.7933\n",
      "  ğŸ” Batch 6/90  |  Loss: 9.8471\n",
      "  ğŸ” Batch 7/90  |  Loss: 25.0213\n",
      "  ğŸ” Batch 8/90  |  Loss: 35.7483\n",
      "  ğŸ” Batch 9/90  |  Loss: 19.4323\n",
      "  ğŸ” Batch 10/90  |  Loss: 29.5958\n",
      "  ğŸ” Batch 11/90  |  Loss: 24.5789\n",
      "  ğŸ” Batch 12/90  |  Loss: 45.0448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 13/90  |  Loss: 21.9039\n",
      "  ğŸ” Batch 14/90  |  Loss: 35.1570\n",
      "  ğŸ” Batch 15/90  |  Loss: 30.6098\n",
      "  ğŸ” Batch 16/90  |  Loss: 39.5227\n",
      "  ğŸ” Batch 17/90  |  Loss: 12.3182\n",
      "  ğŸ” Batch 18/90  |  Loss: 40.3541\n",
      "  ğŸ” Batch 19/90  |  Loss: 23.7025\n",
      "  ğŸ” Batch 20/90  |  Loss: 31.4844\n",
      "  ğŸ” Batch 21/90  |  Loss: 12.6268\n",
      "  ğŸ” Batch 22/90  |  Loss: 63.7304\n",
      "  ğŸ” Batch 23/90  |  Loss: 42.2768\n",
      "  ğŸ” Batch 24/90  |  Loss: 18.4876\n",
      "  ğŸ” Batch 25/90  |  Loss: 28.0309\n",
      "  ğŸ” Batch 26/90  |  Loss: 23.3461\n",
      "  ğŸ” Batch 27/90  |  Loss: 23.1551\n",
      "  ğŸ” Batch 28/90  |  Loss: 29.5478\n",
      "  ğŸ” Batch 29/90  |  Loss: 22.8931\n",
      "  ğŸ” Batch 30/90  |  Loss: 28.5156\n",
      "  ğŸ” Batch 31/90  |  Loss: 17.8662\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.7179\n",
      "  ğŸ” Batch 33/90  |  Loss: 19.9414\n",
      "  ğŸ” Batch 34/90  |  Loss: 50.6094\n",
      "  ğŸ” Batch 35/90  |  Loss: 40.3331\n",
      "  ğŸ” Batch 36/90  |  Loss: 11.3217\n",
      "  ğŸ” Batch 37/90  |  Loss: 12.4698\n",
      "  ğŸ” Batch 38/90  |  Loss: 29.2636\n",
      "  ğŸ” Batch 39/90  |  Loss: 10.5036\n",
      "  ğŸ” Batch 40/90  |  Loss: 48.2983\n",
      "  ğŸ” Batch 41/90  |  Loss: 58.4774\n",
      "  ğŸ” Batch 42/90  |  Loss: 27.1182\n",
      "  ğŸ” Batch 43/90  |  Loss: 55.2864\n",
      "  ğŸ” Batch 44/90  |  Loss: 18.8475\n",
      "  ğŸ” Batch 45/90  |  Loss: 25.4758\n",
      "  ğŸ” Batch 46/90  |  Loss: 16.7014\n",
      "  ğŸ” Batch 47/90  |  Loss: 46.7891\n",
      "  ğŸ” Batch 48/90  |  Loss: 22.8909\n",
      "  ğŸ” Batch 49/90  |  Loss: 39.8869\n",
      "  ğŸ” Batch 50/90  |  Loss: 42.8556\n",
      "  ğŸ” Batch 51/90  |  Loss: 56.2597\n",
      "  ğŸ” Batch 52/90  |  Loss: 58.2803\n",
      "  ğŸ” Batch 53/90  |  Loss: 26.1545\n",
      "  ğŸ” Batch 54/90  |  Loss: 14.5721\n",
      "  ğŸ” Batch 55/90  |  Loss: 29.6939\n",
      "  ğŸ” Batch 56/90  |  Loss: 27.0137\n",
      "  ğŸ” Batch 57/90  |  Loss: 12.6943\n",
      "  ğŸ” Batch 58/90  |  Loss: 19.2211\n",
      "  ğŸ” Batch 59/90  |  Loss: 33.8100\n",
      "  ğŸ” Batch 60/90  |  Loss: 49.4384\n",
      "  ğŸ” Batch 61/90  |  Loss: 48.4933\n",
      "  ğŸ” Batch 62/90  |  Loss: 34.9635\n",
      "  ğŸ” Batch 63/90  |  Loss: 27.5586\n",
      "  ğŸ” Batch 64/90  |  Loss: 32.3185\n",
      "  ğŸ” Batch 65/90  |  Loss: 24.5609\n",
      "  ğŸ” Batch 66/90  |  Loss: 34.4495\n",
      "  ğŸ” Batch 67/90  |  Loss: 18.3615\n",
      "  ğŸ” Batch 68/90  |  Loss: 14.5298\n",
      "  ğŸ” Batch 69/90  |  Loss: 59.4099\n",
      "  ğŸ” Batch 70/90  |  Loss: 9.6676\n",
      "  ğŸ” Batch 71/90  |  Loss: 36.2279\n",
      "  ğŸ” Batch 72/90  |  Loss: 33.2459\n",
      "  ğŸ” Batch 73/90  |  Loss: 8.7665\n",
      "  ğŸ” Batch 74/90  |  Loss: 34.4517\n",
      "  ğŸ” Batch 75/90  |  Loss: 28.2797\n",
      "  ğŸ” Batch 76/90  |  Loss: 28.9061\n",
      "  ğŸ” Batch 77/90  |  Loss: 29.9306\n",
      "  ğŸ” Batch 78/90  |  Loss: 24.6727\n",
      "  ğŸ” Batch 79/90  |  Loss: 54.3385\n",
      "  ğŸ” Batch 80/90  |  Loss: 33.6452\n",
      "  ğŸ” Batch 81/90  |  Loss: 30.1003\n",
      "  ğŸ” Batch 82/90  |  Loss: 59.4702\n",
      "  ğŸ” Batch 83/90  |  Loss: 83.9131\n",
      "  ğŸ” Batch 84/90  |  Loss: 36.1788\n",
      "  ğŸ” Batch 85/90  |  Loss: 74.9807\n",
      "  ğŸ” Batch 86/90  |  Loss: 75.8998\n",
      "  ğŸ” Batch 87/90  |  Loss: 36.4337\n",
      "  ğŸ” Batch 88/90  |  Loss: 34.5170\n",
      "  ğŸ” Batch 89/90  |  Loss: 9.5478\n",
      "  ğŸ” Batch 90/90  |  Loss: 34.1657\n",
      "\n",
      "ğŸ“Š Epoch 16 Summary:\n",
      "   âœ… Accuracy: 0.2444\n",
      "   ğŸ” Recall:   0.2444\n",
      "   â­ F1 Score: 0.2426\n",
      "\n",
      "ğŸŒ€ Epoch 17/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 44.9770\n",
      "  ğŸ” Batch 2/90  |  Loss: 44.4669\n",
      "  ğŸ” Batch 3/90  |  Loss: 40.8833\n",
      "  ğŸ” Batch 4/90  |  Loss: 23.9779\n",
      "  ğŸ” Batch 5/90  |  Loss: 30.4125\n",
      "  ğŸ” Batch 6/90  |  Loss: 15.2799\n",
      "  ğŸ” Batch 7/90  |  Loss: 55.3138\n",
      "  ğŸ” Batch 8/90  |  Loss: 26.9888\n",
      "  ğŸ” Batch 9/90  |  Loss: 57.5782\n",
      "  ğŸ” Batch 10/90  |  Loss: 28.1561\n",
      "  ğŸ” Batch 11/90  |  Loss: 38.8268\n",
      "  ğŸ” Batch 12/90  |  Loss: 35.5087\n",
      "  ğŸ” Batch 13/90  |  Loss: 18.4159\n",
      "  ğŸ” Batch 14/90  |  Loss: 26.6643\n",
      "  ğŸ” Batch 15/90  |  Loss: 50.3169\n",
      "  ğŸ” Batch 16/90  |  Loss: 35.1251\n",
      "  ğŸ” Batch 17/90  |  Loss: 14.3915\n",
      "  ğŸ” Batch 18/90  |  Loss: 23.0043\n",
      "  ğŸ” Batch 19/90  |  Loss: 48.0370\n",
      "  ğŸ” Batch 20/90  |  Loss: 53.1961\n",
      "  ğŸ” Batch 21/90  |  Loss: 44.7891\n",
      "  ğŸ” Batch 22/90  |  Loss: 18.3014\n",
      "  ğŸ” Batch 23/90  |  Loss: 20.2933\n",
      "  ğŸ” Batch 24/90  |  Loss: 57.0396\n",
      "  ğŸ” Batch 25/90  |  Loss: 18.9403\n",
      "  ğŸ” Batch 26/90  |  Loss: 13.6704\n",
      "  ğŸ” Batch 27/90  |  Loss: 43.0244\n",
      "  ğŸ” Batch 28/90  |  Loss: 22.0173\n",
      "  ğŸ” Batch 29/90  |  Loss: 35.7889\n",
      "  ğŸ” Batch 30/90  |  Loss: 23.7878\n",
      "  ğŸ” Batch 31/90  |  Loss: 35.0248\n",
      "  ğŸ” Batch 32/90  |  Loss: 24.0432\n",
      "  ğŸ” Batch 33/90  |  Loss: 19.8727\n",
      "  ğŸ” Batch 34/90  |  Loss: 40.4005\n",
      "  ğŸ” Batch 35/90  |  Loss: 23.7594\n",
      "  ğŸ” Batch 36/90  |  Loss: 26.6484\n",
      "  ğŸ” Batch 37/90  |  Loss: 32.6500\n",
      "  ğŸ” Batch 38/90  |  Loss: 26.5927\n",
      "  ğŸ” Batch 39/90  |  Loss: 67.1317\n",
      "  ğŸ” Batch 40/90  |  Loss: 36.0097\n",
      "  ğŸ” Batch 41/90  |  Loss: 32.8361\n",
      "  ğŸ” Batch 42/90  |  Loss: 41.9920\n",
      "  ğŸ” Batch 43/90  |  Loss: 29.7407\n",
      "  ğŸ” Batch 44/90  |  Loss: 22.5971\n",
      "  ğŸ” Batch 45/90  |  Loss: 53.7871\n",
      "  ğŸ” Batch 46/90  |  Loss: 14.2135\n",
      "  ğŸ” Batch 47/90  |  Loss: 29.9442\n",
      "  ğŸ” Batch 48/90  |  Loss: 36.7004\n",
      "  ğŸ” Batch 49/90  |  Loss: 17.5859\n",
      "  ğŸ” Batch 50/90  |  Loss: 12.9192\n",
      "  ğŸ” Batch 51/90  |  Loss: 37.0594\n",
      "  ğŸ” Batch 52/90  |  Loss: 28.3448\n",
      "  ğŸ” Batch 53/90  |  Loss: 43.6818\n",
      "  ğŸ” Batch 54/90  |  Loss: 26.5063\n",
      "  ğŸ” Batch 55/90  |  Loss: 27.8030\n",
      "  ğŸ” Batch 56/90  |  Loss: 19.9105\n",
      "  ğŸ” Batch 57/90  |  Loss: 34.3259\n",
      "  ğŸ” Batch 58/90  |  Loss: 29.6437\n",
      "  ğŸ” Batch 59/90  |  Loss: 20.3581\n",
      "  ğŸ” Batch 60/90  |  Loss: 33.0006\n",
      "  ğŸ” Batch 61/90  |  Loss: 42.5722\n",
      "  ğŸ” Batch 62/90  |  Loss: 29.6642\n",
      "  ğŸ” Batch 63/90  |  Loss: 20.6501\n",
      "  ğŸ” Batch 64/90  |  Loss: 29.6762\n",
      "  ğŸ” Batch 65/90  |  Loss: 24.1615\n",
      "  ğŸ” Batch 66/90  |  Loss: 34.5472\n",
      "  ğŸ” Batch 67/90  |  Loss: 13.0238\n",
      "  ğŸ” Batch 68/90  |  Loss: 30.6459\n",
      "  ğŸ” Batch 69/90  |  Loss: 32.0876\n",
      "  ğŸ” Batch 70/90  |  Loss: 33.4225\n",
      "  ğŸ” Batch 71/90  |  Loss: 13.8172\n",
      "  ğŸ” Batch 72/90  |  Loss: 25.3047\n",
      "  ğŸ” Batch 73/90  |  Loss: 40.7604\n",
      "  ğŸ” Batch 74/90  |  Loss: 44.9211\n",
      "  ğŸ” Batch 75/90  |  Loss: 26.2611\n",
      "  ğŸ” Batch 76/90  |  Loss: 61.6443\n",
      "  ğŸ” Batch 77/90  |  Loss: 34.3484\n",
      "  ğŸ” Batch 78/90  |  Loss: 17.7994\n",
      "  ğŸ” Batch 79/90  |  Loss: 36.3320\n",
      "  ğŸ” Batch 80/90  |  Loss: 23.2365\n",
      "  ğŸ” Batch 81/90  |  Loss: 10.8549\n",
      "  ğŸ” Batch 82/90  |  Loss: 31.2470\n",
      "  ğŸ” Batch 83/90  |  Loss: 17.4570\n",
      "  ğŸ” Batch 84/90  |  Loss: 32.2205\n",
      "  ğŸ” Batch 85/90  |  Loss: 42.9500\n",
      "  ğŸ” Batch 86/90  |  Loss: 9.0256\n",
      "  ğŸ” Batch 87/90  |  Loss: 38.3436\n",
      "  ğŸ” Batch 88/90  |  Loss: 31.8157\n",
      "  ğŸ” Batch 89/90  |  Loss: 39.7227\n",
      "  ğŸ” Batch 90/90  |  Loss: 28.4920\n",
      "\n",
      "ğŸ“Š Epoch 17 Summary:\n",
      "   âœ… Accuracy: 0.2639\n",
      "   ğŸ” Recall:   0.2639\n",
      "   â­ F1 Score: 0.2641\n",
      "\n",
      "ğŸŒ€ Epoch 18/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 19.6651\n",
      "  ğŸ” Batch 2/90  |  Loss: 23.3822\n",
      "  ğŸ” Batch 3/90  |  Loss: 39.5900\n",
      "  ğŸ” Batch 4/90  |  Loss: 19.9620\n",
      "  ğŸ” Batch 5/90  |  Loss: 15.7790\n",
      "  ğŸ” Batch 6/90  |  Loss: 27.0541\n",
      "  ğŸ” Batch 7/90  |  Loss: 15.4387\n",
      "  ğŸ” Batch 8/90  |  Loss: 21.3875\n",
      "  ğŸ” Batch 9/90  |  Loss: 43.6092\n",
      "  ğŸ” Batch 10/90  |  Loss: 10.7176\n",
      "  ğŸ” Batch 11/90  |  Loss: 22.0250\n",
      "  ğŸ” Batch 12/90  |  Loss: 23.3507\n",
      "  ğŸ” Batch 13/90  |  Loss: 14.5906\n",
      "  ğŸ” Batch 14/90  |  Loss: 38.0814\n",
      "  ğŸ” Batch 15/90  |  Loss: 22.7417\n",
      "  ğŸ” Batch 16/90  |  Loss: 11.1660\n",
      "  ğŸ” Batch 17/90  |  Loss: 32.1564\n",
      "  ğŸ” Batch 18/90  |  Loss: 8.0457\n",
      "  ğŸ” Batch 19/90  |  Loss: 31.5521\n",
      "  ğŸ” Batch 20/90  |  Loss: 16.2290\n",
      "  ğŸ” Batch 21/90  |  Loss: 30.8028\n",
      "  ğŸ” Batch 22/90  |  Loss: 51.3307\n",
      "  ğŸ” Batch 23/90  |  Loss: 24.4124\n",
      "  ğŸ” Batch 24/90  |  Loss: 32.4975\n",
      "  ğŸ” Batch 25/90  |  Loss: 29.4279\n",
      "  ğŸ” Batch 26/90  |  Loss: 57.7010\n",
      "  ğŸ” Batch 27/90  |  Loss: 17.5918\n",
      "  ğŸ” Batch 28/90  |  Loss: 25.0080\n",
      "  ğŸ” Batch 29/90  |  Loss: 14.5855\n",
      "  ğŸ” Batch 30/90  |  Loss: 39.5421\n",
      "  ğŸ” Batch 31/90  |  Loss: 29.1024\n",
      "  ğŸ” Batch 32/90  |  Loss: 16.7424\n",
      "  ğŸ” Batch 33/90  |  Loss: 43.4713\n",
      "  ğŸ” Batch 34/90  |  Loss: 38.9305\n",
      "  ğŸ” Batch 35/90  |  Loss: 28.5738\n",
      "  ğŸ” Batch 36/90  |  Loss: 35.0073\n",
      "  ğŸ” Batch 37/90  |  Loss: 23.0015\n",
      "  ğŸ” Batch 38/90  |  Loss: 26.1005\n",
      "  ğŸ” Batch 39/90  |  Loss: 41.5809\n",
      "  ğŸ” Batch 40/90  |  Loss: 41.6056\n",
      "  ğŸ” Batch 41/90  |  Loss: 18.7654\n",
      "  ğŸ” Batch 42/90  |  Loss: 18.2580\n",
      "  ğŸ” Batch 43/90  |  Loss: 25.3288\n",
      "  ğŸ” Batch 44/90  |  Loss: 16.0247\n",
      "  ğŸ” Batch 45/90  |  Loss: 18.2513\n",
      "  ğŸ” Batch 46/90  |  Loss: 18.7165\n",
      "  ğŸ” Batch 47/90  |  Loss: 39.8671\n",
      "  ğŸ” Batch 48/90  |  Loss: 12.9970\n",
      "  ğŸ” Batch 49/90  |  Loss: 23.2986\n",
      "  ğŸ” Batch 50/90  |  Loss: 44.0077\n",
      "  ğŸ” Batch 51/90  |  Loss: 38.3893\n",
      "  ğŸ” Batch 52/90  |  Loss: 35.3570\n",
      "  ğŸ” Batch 53/90  |  Loss: 21.5904\n",
      "  ğŸ” Batch 54/90  |  Loss: 10.7813\n",
      "  ğŸ” Batch 55/90  |  Loss: 40.2783\n",
      "  ğŸ” Batch 56/90  |  Loss: 59.3906\n",
      "  ğŸ” Batch 57/90  |  Loss: 12.8917\n",
      "  ğŸ” Batch 58/90  |  Loss: 34.3714\n",
      "  ğŸ” Batch 59/90  |  Loss: 51.7954\n",
      "  ğŸ” Batch 60/90  |  Loss: 55.0871\n",
      "  ğŸ” Batch 61/90  |  Loss: 31.0749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 62/90  |  Loss: 38.1948\n",
      "  ğŸ” Batch 63/90  |  Loss: 33.6874\n",
      "  ğŸ” Batch 64/90  |  Loss: 39.6845\n",
      "  ğŸ” Batch 65/90  |  Loss: 29.7078\n",
      "  ğŸ” Batch 66/90  |  Loss: 41.5432\n",
      "  ğŸ” Batch 67/90  |  Loss: 9.9594\n",
      "  ğŸ” Batch 68/90  |  Loss: 37.1939\n",
      "  ğŸ” Batch 69/90  |  Loss: 17.0472\n",
      "  ğŸ” Batch 70/90  |  Loss: 22.9332\n",
      "  ğŸ” Batch 71/90  |  Loss: 21.7360\n",
      "  ğŸ” Batch 72/90  |  Loss: 28.2582\n",
      "  ğŸ” Batch 73/90  |  Loss: 45.3522\n",
      "  ğŸ” Batch 74/90  |  Loss: 15.8845\n",
      "  ğŸ” Batch 75/90  |  Loss: 21.3988\n",
      "  ğŸ” Batch 76/90  |  Loss: 37.2568\n",
      "  ğŸ” Batch 77/90  |  Loss: 48.9695\n",
      "  ğŸ” Batch 78/90  |  Loss: 18.8109\n",
      "  ğŸ” Batch 79/90  |  Loss: 22.8842\n",
      "  ğŸ” Batch 80/90  |  Loss: 35.6639\n",
      "  ğŸ” Batch 81/90  |  Loss: 38.8289\n",
      "  ğŸ” Batch 82/90  |  Loss: 34.6784\n",
      "  ğŸ” Batch 83/90  |  Loss: 47.0806\n",
      "  ğŸ” Batch 84/90  |  Loss: 39.3123\n",
      "  ğŸ” Batch 85/90  |  Loss: 42.1455\n",
      "  ğŸ” Batch 86/90  |  Loss: 34.5125\n",
      "  ğŸ” Batch 87/90  |  Loss: 26.8750\n",
      "  ğŸ” Batch 88/90  |  Loss: 56.6691\n",
      "  ğŸ” Batch 89/90  |  Loss: 26.9749\n",
      "  ğŸ” Batch 90/90  |  Loss: 29.7130\n",
      "\n",
      "ğŸ“Š Epoch 18 Summary:\n",
      "   âœ… Accuracy: 0.2611\n",
      "   ğŸ” Recall:   0.2611\n",
      "   â­ F1 Score: 0.2607\n",
      "\n",
      "ğŸŒ€ Epoch 19/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 23.1296\n",
      "  ğŸ” Batch 2/90  |  Loss: 15.7508\n",
      "  ğŸ” Batch 3/90  |  Loss: 22.8744\n",
      "  ğŸ” Batch 4/90  |  Loss: 40.2916\n",
      "  ğŸ” Batch 5/90  |  Loss: 19.3050\n",
      "  ğŸ” Batch 6/90  |  Loss: 42.7343\n",
      "  ğŸ” Batch 7/90  |  Loss: 36.7540\n",
      "  ğŸ” Batch 8/90  |  Loss: 36.3681\n",
      "  ğŸ” Batch 9/90  |  Loss: 58.3374\n",
      "  ğŸ” Batch 10/90  |  Loss: 51.8979\n",
      "  ğŸ” Batch 11/90  |  Loss: 23.1603\n",
      "  ğŸ” Batch 12/90  |  Loss: 34.6405\n",
      "  ğŸ” Batch 13/90  |  Loss: 37.6556\n",
      "  ğŸ” Batch 14/90  |  Loss: 30.7416\n",
      "  ğŸ” Batch 15/90  |  Loss: 49.1779\n",
      "  ğŸ” Batch 16/90  |  Loss: 17.0431\n",
      "  ğŸ” Batch 17/90  |  Loss: 35.4502\n",
      "  ğŸ” Batch 18/90  |  Loss: 75.4977\n",
      "  ğŸ” Batch 19/90  |  Loss: 21.7479\n",
      "  ğŸ” Batch 20/90  |  Loss: 12.7447\n",
      "  ğŸ” Batch 21/90  |  Loss: 30.5381\n",
      "  ğŸ” Batch 22/90  |  Loss: 49.6733\n",
      "  ğŸ” Batch 23/90  |  Loss: 28.2020\n",
      "  ğŸ” Batch 24/90  |  Loss: 49.1237\n",
      "  ğŸ” Batch 25/90  |  Loss: 31.5971\n",
      "  ğŸ” Batch 26/90  |  Loss: 53.7357\n",
      "  ğŸ” Batch 27/90  |  Loss: 50.9241\n",
      "  ğŸ” Batch 28/90  |  Loss: 37.6418\n",
      "  ğŸ” Batch 29/90  |  Loss: 40.8884\n",
      "  ğŸ” Batch 30/90  |  Loss: 26.8531\n",
      "  ğŸ” Batch 31/90  |  Loss: 21.2618\n",
      "  ğŸ” Batch 32/90  |  Loss: 43.6169\n",
      "  ğŸ” Batch 33/90  |  Loss: 24.8074\n",
      "  ğŸ” Batch 34/90  |  Loss: 23.1071\n",
      "  ğŸ” Batch 35/90  |  Loss: 21.9422\n",
      "  ğŸ” Batch 36/90  |  Loss: 22.7064\n",
      "  ğŸ” Batch 37/90  |  Loss: 51.1582\n",
      "  ğŸ” Batch 38/90  |  Loss: 28.0773\n",
      "  ğŸ” Batch 39/90  |  Loss: 38.3000\n",
      "  ğŸ” Batch 40/90  |  Loss: 41.9156\n",
      "  ğŸ” Batch 41/90  |  Loss: 52.0947\n",
      "  ğŸ” Batch 42/90  |  Loss: 74.2631\n",
      "  ğŸ” Batch 43/90  |  Loss: 16.1128\n",
      "  ğŸ” Batch 44/90  |  Loss: 26.5505\n",
      "  ğŸ” Batch 45/90  |  Loss: 43.2148\n",
      "  ğŸ” Batch 46/90  |  Loss: 20.0328\n",
      "  ğŸ” Batch 47/90  |  Loss: 28.1291\n",
      "  ğŸ” Batch 48/90  |  Loss: 45.1320\n",
      "  ğŸ” Batch 49/90  |  Loss: 28.3220\n",
      "  ğŸ” Batch 50/90  |  Loss: 35.0807\n",
      "  ğŸ” Batch 51/90  |  Loss: 21.5212\n",
      "  ğŸ” Batch 52/90  |  Loss: 20.6137\n",
      "  ğŸ” Batch 53/90  |  Loss: 48.3582\n",
      "  ğŸ” Batch 54/90  |  Loss: 33.1496\n",
      "  ğŸ” Batch 55/90  |  Loss: 14.7000\n",
      "  ğŸ” Batch 56/90  |  Loss: 21.8798\n",
      "  ğŸ” Batch 57/90  |  Loss: 33.2000\n",
      "  ğŸ” Batch 58/90  |  Loss: 24.1559\n",
      "  ğŸ” Batch 59/90  |  Loss: 24.7546\n",
      "  ğŸ” Batch 60/90  |  Loss: 29.6875\n",
      "  ğŸ” Batch 61/90  |  Loss: 20.9874\n",
      "  ğŸ” Batch 62/90  |  Loss: 41.6944\n",
      "  ğŸ” Batch 63/90  |  Loss: 23.4240\n",
      "  ğŸ” Batch 64/90  |  Loss: 15.2689\n",
      "  ğŸ” Batch 65/90  |  Loss: 26.1868\n",
      "  ğŸ” Batch 66/90  |  Loss: 10.0979\n",
      "  ğŸ” Batch 67/90  |  Loss: 20.0605\n",
      "  ğŸ” Batch 68/90  |  Loss: 30.0504\n",
      "  ğŸ” Batch 69/90  |  Loss: 56.9403\n",
      "  ğŸ” Batch 70/90  |  Loss: 50.9994\n",
      "  ğŸ” Batch 71/90  |  Loss: 30.1208\n",
      "  ğŸ” Batch 72/90  |  Loss: 33.4170\n",
      "  ğŸ” Batch 73/90  |  Loss: 26.9764\n",
      "  ğŸ” Batch 74/90  |  Loss: 36.2480\n",
      "  ğŸ” Batch 75/90  |  Loss: 28.5275\n",
      "  ğŸ” Batch 76/90  |  Loss: 22.4030\n",
      "  ğŸ” Batch 77/90  |  Loss: 10.9683\n",
      "  ğŸ” Batch 78/90  |  Loss: 23.0151\n",
      "  ğŸ” Batch 79/90  |  Loss: 16.0218\n",
      "  ğŸ” Batch 80/90  |  Loss: 13.7835\n",
      "  ğŸ” Batch 81/90  |  Loss: 29.7980\n",
      "  ğŸ” Batch 82/90  |  Loss: 21.3945\n",
      "  ğŸ” Batch 83/90  |  Loss: 19.8256\n",
      "  ğŸ” Batch 84/90  |  Loss: 19.5042\n",
      "  ğŸ” Batch 85/90  |  Loss: 10.0234\n",
      "  ğŸ” Batch 86/90  |  Loss: 50.0142\n",
      "  ğŸ” Batch 87/90  |  Loss: 33.4852\n",
      "  ğŸ” Batch 88/90  |  Loss: 22.8090\n",
      "  ğŸ” Batch 89/90  |  Loss: 39.0670\n",
      "  ğŸ” Batch 90/90  |  Loss: 32.3338\n",
      "\n",
      "ğŸ“Š Epoch 19 Summary:\n",
      "   âœ… Accuracy: 0.2819\n",
      "   ğŸ” Recall:   0.2819\n",
      "   â­ F1 Score: 0.2830\n",
      "\n",
      "ğŸŒ€ Epoch 20/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.1408\n",
      "  ğŸ” Batch 2/90  |  Loss: 40.8413\n",
      "  ğŸ” Batch 3/90  |  Loss: 34.9668\n",
      "  ğŸ” Batch 4/90  |  Loss: 22.8799\n",
      "  ğŸ” Batch 5/90  |  Loss: 14.2614\n",
      "  ğŸ” Batch 6/90  |  Loss: 33.1306\n",
      "  ğŸ” Batch 7/90  |  Loss: 24.2417\n",
      "  ğŸ” Batch 8/90  |  Loss: 20.0041\n",
      "  ğŸ” Batch 9/90  |  Loss: 19.1425\n",
      "  ğŸ” Batch 10/90  |  Loss: 11.0832\n",
      "  ğŸ” Batch 11/90  |  Loss: 28.1142\n",
      "  ğŸ” Batch 12/90  |  Loss: 35.9800\n",
      "  ğŸ” Batch 13/90  |  Loss: 26.5622\n",
      "  ğŸ” Batch 14/90  |  Loss: 36.9463\n",
      "  ğŸ” Batch 15/90  |  Loss: 44.1722\n",
      "  ğŸ” Batch 16/90  |  Loss: 33.4237\n",
      "  ğŸ” Batch 17/90  |  Loss: 24.3543\n",
      "  ğŸ” Batch 18/90  |  Loss: 20.5037\n",
      "  ğŸ” Batch 19/90  |  Loss: 24.8373\n"
     ]
    }
   ],
   "source": [
    "from main import init_recognition\n",
    "\n",
    "config_path = 'config/st_gcn/mediapipe-asl.yaml'\n",
    "processor = init_recognition(config_path)\n",
    "\n",
    "# ğŸ“Š Print metrics\n",
    "for entry in processor.training_metrics:\n",
    "    print(f\"ğŸ“Š Epoch {entry['epoch']}: Acc={entry['accuracy']:.4f} | Recall={entry['recall']:.4f} | F1={entry['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1221f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53381dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "587906f0",
   "metadata": {},
   "source": [
    "### REAL ORGINIAL STGN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4d593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58068c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor start\n",
      "Loading data...\n",
      "Loading model...\n",
      "Starting training for 80 epochs\n",
      "\n",
      "ğŸŒ€ Epoch 1/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 6.6327\n",
      "  ğŸ” Batch 2/90  |  Loss: 6.7337\n",
      "  ğŸ” Batch 3/90  |  Loss: 6.4121\n",
      "  ğŸ” Batch 4/90  |  Loss: 6.3994\n",
      "  ğŸ” Batch 5/90  |  Loss: 6.0084\n",
      "  ğŸ” Batch 6/90  |  Loss: 6.3841\n",
      "  ğŸ” Batch 7/90  |  Loss: 6.4240\n",
      "  ğŸ” Batch 8/90  |  Loss: 6.8490\n",
      "  ğŸ” Batch 9/90  |  Loss: 5.9426\n",
      "  ğŸ” Batch 10/90  |  Loss: 6.2820\n",
      "  ğŸ” Batch 11/90  |  Loss: 5.9316\n",
      "  ğŸ” Batch 12/90  |  Loss: 6.1095\n",
      "  ğŸ” Batch 13/90  |  Loss: 6.1956\n",
      "  ğŸ” Batch 14/90  |  Loss: 6.3795\n",
      "  ğŸ” Batch 15/90  |  Loss: 6.4706\n",
      "  ğŸ” Batch 16/90  |  Loss: 5.6350\n",
      "  ğŸ” Batch 17/90  |  Loss: 6.3042\n",
      "  ğŸ” Batch 18/90  |  Loss: 5.2767\n",
      "  ğŸ” Batch 19/90  |  Loss: 6.2758\n",
      "  ğŸ” Batch 20/90  |  Loss: 6.0295\n",
      "  ğŸ” Batch 21/90  |  Loss: 5.5225\n",
      "  ğŸ” Batch 22/90  |  Loss: 5.9760\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.8132\n",
      "  ğŸ” Batch 24/90  |  Loss: 6.4635\n",
      "  ğŸ” Batch 25/90  |  Loss: 5.4566\n",
      "  ğŸ” Batch 26/90  |  Loss: 6.2540\n",
      "  ğŸ” Batch 27/90  |  Loss: 5.9540\n",
      "  ğŸ” Batch 28/90  |  Loss: 5.9413\n",
      "  ğŸ” Batch 29/90  |  Loss: 5.3487\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.6306\n",
      "  ğŸ” Batch 31/90  |  Loss: 5.3529\n",
      "  ğŸ” Batch 32/90  |  Loss: 5.2008\n",
      "  ğŸ” Batch 33/90  |  Loss: 5.7251\n",
      "  ğŸ” Batch 34/90  |  Loss: 5.0172\n",
      "  ğŸ” Batch 35/90  |  Loss: 5.4773\n",
      "  ğŸ” Batch 36/90  |  Loss: 5.5306\n",
      "  ğŸ” Batch 37/90  |  Loss: 5.1123\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.9839\n",
      "  ğŸ” Batch 39/90  |  Loss: 5.2800\n",
      "  ğŸ” Batch 40/90  |  Loss: 5.3466\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.8643\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.5589\n",
      "  ğŸ” Batch 43/90  |  Loss: 5.2100\n",
      "  ğŸ” Batch 44/90  |  Loss: 5.3464\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.6156\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.8513\n",
      "  ğŸ” Batch 47/90  |  Loss: 5.2474\n",
      "  ğŸ” Batch 48/90  |  Loss: 5.2134\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.9217\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.9302\n",
      "  ğŸ” Batch 51/90  |  Loss: 5.7919\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.7937\n",
      "  ğŸ” Batch 53/90  |  Loss: 5.1531\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.6967\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.9543\n",
      "  ğŸ” Batch 56/90  |  Loss: 5.1773\n",
      "  ğŸ” Batch 57/90  |  Loss: 5.0395\n",
      "  ğŸ” Batch 58/90  |  Loss: 5.1518\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.8158\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.9090\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.5561\n",
      "  ğŸ” Batch 62/90  |  Loss: 5.2668\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.7158\n",
      "  ğŸ” Batch 64/90  |  Loss: 5.2835\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.6457\n",
      "  ğŸ” Batch 66/90  |  Loss: 5.2776\n",
      "  ğŸ” Batch 67/90  |  Loss: 5.1865\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.8944\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.7457\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.7496\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.5816\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.8458\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.7321\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.6089\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.9962\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.9647\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.7991\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.7418\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.8343\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.9167\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.6145\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.7352\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.6164\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.7615\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.6361\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.8333\n",
      "  ğŸ” Batch 87/90  |  Loss: 5.0208\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.7360\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.6548\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.8873\n",
      "\n",
      "ğŸ“Š Epoch 1 Summary:\n",
      "   âœ… Accuracy: 0.0042\n",
      "   ğŸ” Recall:   0.0040\n",
      "   â­ F1 Score: 0.0012\n",
      "\n",
      "ğŸŒ€ Epoch 2/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.4626\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.4986\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.5869\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.6506\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.6400\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.6667\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.8357\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.3558\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.5213\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.5142\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.3308\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.6072\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.8752\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.7070\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.4771\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.3763\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.2400\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.4543\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.8636\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.6251\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.5756\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.6793\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.7158\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.5526\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.5489\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.6266\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.6199\n",
      "  ğŸ” Batch 28/90  |  Loss: 5.0345\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.6098\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.5753\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.8394\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.6015\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.5953\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.7353\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.5377\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.4618\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.6629\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.4251\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.4348\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.8563\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.4749\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.5076\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.3643\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.8084\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.9862\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.7943\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.5711\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.6491\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.8355\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.2648\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.6789\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.8407\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.5697\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.8558\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.7527\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.7883\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.6331\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.6267\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.7614\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.7039\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.5458\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.5327\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.8222\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.7533\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.6784\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.6705\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.6313\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.6032\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.5797\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.7074\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.5374\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.7604\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.6873\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.7538\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.5278\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.8360\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.5117\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.6108\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.7576\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.7183\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.5474\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.6902\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.5773\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.5431\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.6830\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.6246\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.6285\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.7969\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.5542\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.7200\n",
      "\n",
      "ğŸ“Š Epoch 2 Summary:\n",
      "   âœ… Accuracy: 0.0083\n",
      "   ğŸ” Recall:   0.0083\n",
      "   â­ F1 Score: 0.0027\n",
      "\n",
      "ğŸŒ€ Epoch 3/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.4968\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.4673\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.5473\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.5702\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.3678\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.4736\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.6507\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.6479\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.3996\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.4164\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.6542\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.3375\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.7381\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.6749\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.4666\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.4565\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.5167\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.5501\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.4851\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.7394\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.6516\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.5305\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.4958\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.5477\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.6464\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.3218\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.3474\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.7121\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.6287\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.2494\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.6991\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.6844\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.7473\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.7651\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.7085\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.4342\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.6274\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.8035\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.7082\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.4341\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.4257\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.6109\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.7374\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.6588\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.5164\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.6031\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.6080\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.6876\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.5875\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.3190\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.7125\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.5882\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.5916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 54/90  |  Loss: 4.6649\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.3848\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.4954\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.2850\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.4456\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.4779\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.4895\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.5969\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.4156\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.8709\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.6605\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.1784\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.5203\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.7884\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.7486\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.7084\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.5725\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.3767\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.6029\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.5191\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.4338\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.5729\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.8178\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.6069\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.5656\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.6096\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.6051\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.7005\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.7436\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.5199\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.6828\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.4147\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.4749\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.3880\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.5833\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.4852\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.5317\n",
      "\n",
      "ğŸ“Š Epoch 3 Summary:\n",
      "   âœ… Accuracy: 0.0111\n",
      "   ğŸ” Recall:   0.0111\n",
      "   â­ F1 Score: 0.0055\n",
      "\n",
      "ğŸŒ€ Epoch 4/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.7062\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.2845\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.4685\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.4849\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.4506\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.5646\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.7117\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.3509\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.4922\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.5167\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.3975\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.5076\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.4515\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.1739\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.5976\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.7938\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.5517\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.4023\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.4194\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.4131\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.3744\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.4954\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.7255\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.4214\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.3888\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.3935\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.3744\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.5710\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.5616\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.2930\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.3838\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.3545\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.3161\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.1062\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.6940\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.4498\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.9710\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.6207\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.3221\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.4087\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.2869\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.4385\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.7352\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.7306\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.4691\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.5905\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.5462\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.1947\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.4671\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.5331\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.5500\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.8555\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.4518\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.2960\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.7226\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.4751\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.6871\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.5869\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.7558\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.3973\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.6668\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.4654\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.5437\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.6332\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.8182\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.4476\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.6437\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.6147\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.3531\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.5157\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.5582\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.1779\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.5570\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.5121\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.6031\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.4300\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.3540\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.5527\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.4971\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.5731\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.3311\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.5008\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.4912\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.4932\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.5334\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.5933\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.3602\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.4401\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.5428\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.2382\n",
      "\n",
      "ğŸ“Š Epoch 4 Summary:\n",
      "   âœ… Accuracy: 0.0194\n",
      "   ğŸ” Recall:   0.0194\n",
      "   â­ F1 Score: 0.0137\n",
      "\n",
      "ğŸŒ€ Epoch 5/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.3762\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.1855\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.3541\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.3996\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.2763\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.3681\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.3448\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.1193\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.2572\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.6453\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.2859\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.3776\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.2478\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.0230\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.5719\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.2929\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.2032\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.5693\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.4169\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.6464\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.2926\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.3561\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.6332\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.5324\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.6261\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.3081\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.2777\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.7046\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.4890\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.1198\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.4858\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.2596\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.3186\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.2796\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.4032\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.7330\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.2805\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.6410\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.1294\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.4187\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.1229\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.5421\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.1592\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.1328\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.4273\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.6456\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.6333\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.4296\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.5174\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.1403\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.5716\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.4295\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.5192\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.4049\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.1623\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.2172\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.5600\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.2607\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.3929\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.1541\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.4506\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.5603\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.1624\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.1762\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.4305\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.6145\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.4238\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.5593\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.6422\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.5684\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.3884\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.0754\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.3054\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.4894\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.3590\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.2471\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.3732\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.1645\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.0117\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.3762\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.2580\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.6674\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.4934\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.1545\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.3570\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.2962\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.3078\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.4060\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.9788\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.3594\n",
      "\n",
      "ğŸ“Š Epoch 5 Summary:\n",
      "   âœ… Accuracy: 0.0306\n",
      "   ğŸ” Recall:   0.0306\n",
      "   â­ F1 Score: 0.0171\n",
      "\n",
      "ğŸŒ€ Epoch 6/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.3334\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.1660\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.0781\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.7936\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.3584\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.4047\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.2539\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.2523\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.9189\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.3034\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.9831\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.8924\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.1885\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.3120\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.3662\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.5234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 17/90  |  Loss: 4.4851\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.2823\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.3493\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.8349\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.2224\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.1915\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.2348\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.6797\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.4682\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.1471\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.4978\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.1510\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.1360\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.0483\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.0310\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.8896\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.8511\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.2364\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.1991\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.1750\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.3854\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.5349\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.2971\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.5093\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.2121\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.4618\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.2735\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.1906\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.8533\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.0204\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.0007\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.0576\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.5348\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.3525\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.5860\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.1227\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.9657\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.4675\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.8442\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.1138\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.1751\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.1460\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.9444\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.2568\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.4169\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.8827\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.2662\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.2911\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.4164\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.1277\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.2563\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.9304\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.3827\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.9278\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.2970\n",
      "  ğŸ” Batch 72/90  |  Loss: 5.0371\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.3095\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.1604\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.8197\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.3766\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.1227\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.8292\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.2982\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.1051\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.3489\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.2668\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.0465\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.4179\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.2725\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.1133\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.9583\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.2506\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.2094\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.9951\n",
      "\n",
      "ğŸ“Š Epoch 6 Summary:\n",
      "   âœ… Accuracy: 0.0347\n",
      "   ğŸ” Recall:   0.0347\n",
      "   â­ F1 Score: 0.0216\n",
      "\n",
      "ğŸŒ€ Epoch 7/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.9517\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.2174\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.0003\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.9754\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.2053\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.0824\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.9290\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.6550\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.0834\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.1795\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.5738\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.9527\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.6733\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.0954\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.9056\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.2454\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.2087\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.2695\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.8495\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.0376\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.3864\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.2112\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.0869\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.0114\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.2460\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.1935\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.3137\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.2519\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.3264\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.2141\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.2218\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.5021\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.2314\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.9246\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.3419\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.9043\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.0627\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.3127\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.4468\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.2770\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.9836\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.9406\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.1724\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.5376\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.7196\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.9077\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.8295\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.2296\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.4807\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.3682\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.9548\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.3344\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.5099\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.6155\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.1044\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.8265\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.9969\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.2542\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.1984\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.6700\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.8172\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.1136\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.6286\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.8903\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.6115\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.1068\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.3082\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.7530\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.4223\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.0844\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.3379\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.2460\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.2561\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.2455\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.2373\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.9803\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.7668\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.9818\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.0315\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.1789\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.2517\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.1038\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.2986\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.2526\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.7889\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.2637\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.5214\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.1927\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.9257\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.2247\n",
      "\n",
      "ğŸ“Š Epoch 7 Summary:\n",
      "   âœ… Accuracy: 0.0500\n",
      "   ğŸ” Recall:   0.0500\n",
      "   â­ F1 Score: 0.0333\n",
      "\n",
      "ğŸŒ€ Epoch 8/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.9335\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.8723\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.8545\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.9105\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.0910\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.4252\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.9362\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.0385\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.7364\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.2950\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.9195\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.0804\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.1432\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.1436\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.8625\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.1821\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.4257\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.8853\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.1196\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.6377\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.3950\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.0033\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.0049\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.0600\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.6805\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.1483\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.3899\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.6120\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.7126\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.0313\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.0349\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.7607\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.9190\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.9404\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.1194\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.1836\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.2215\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.9682\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.5880\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.9341\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.4168\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.6446\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.5615\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.8043\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.9696\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.6888\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.9913\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.0544\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.9359\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.9890\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.9832\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.2298\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.5878\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.6270\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.9847\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.1012\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.0639\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.9110\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.1962\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.7978\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.8111\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.5619\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.3311\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.6927\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.9952\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.9949\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.0339\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.1075\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.9353\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.2563\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.2663\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.3660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 73/90  |  Loss: 4.3892\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.0486\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.1375\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.9101\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.8712\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.4346\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.8687\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.0201\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.6392\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.7519\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.8235\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.0812\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.0824\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.0361\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.4234\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.6371\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.8222\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.9295\n",
      "\n",
      "ğŸ“Š Epoch 8 Summary:\n",
      "   âœ… Accuracy: 0.0569\n",
      "   ğŸ” Recall:   0.0569\n",
      "   â­ F1 Score: 0.0400\n",
      "\n",
      "ğŸŒ€ Epoch 9/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.7779\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.8957\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.4766\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.0523\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.8155\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.7900\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.6764\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.2741\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.7683\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.9355\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.6183\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.7740\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.5608\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.5878\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.6289\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.6010\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.4472\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.0947\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.9124\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.5079\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.8881\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.5936\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.6986\n",
      "  ğŸ” Batch 24/90  |  Loss: 5.0576\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.8228\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.3239\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.3504\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.1183\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.7113\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.0191\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.9145\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.7613\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.8047\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.2328\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.9715\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.7767\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.8879\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.9569\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.4431\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.7874\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.6058\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.2253\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.8472\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.2803\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.2721\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.6508\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.2451\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.3188\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.6852\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.9387\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.8184\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.8744\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.7302\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.9134\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.8195\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.4880\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.8744\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.6913\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.9325\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.6907\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.4799\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.9377\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.6412\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.2286\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.7973\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.4692\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.1292\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.5609\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.9614\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.0392\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.8366\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.6016\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.5280\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.5976\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.8280\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.4895\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.6242\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.0070\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.5963\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.6540\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.8341\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.0224\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.0421\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.2676\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.1859\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.0635\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.3554\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.0494\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.8289\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.7827\n",
      "\n",
      "ğŸ“Š Epoch 9 Summary:\n",
      "   âœ… Accuracy: 0.0722\n",
      "   ğŸ” Recall:   0.0722\n",
      "   â­ F1 Score: 0.0535\n",
      "\n",
      "ğŸŒ€ Epoch 10/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.4568\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.3273\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.7594\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.6543\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.5532\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.4383\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.6200\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.9901\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.9833\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.9224\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.5830\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.6449\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.1855\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.1264\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.3873\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.0921\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.8294\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.5937\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.6509\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.8415\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.8970\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.2465\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.3642\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.2683\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.6059\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.0972\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.8902\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.9129\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.4527\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.9211\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.1111\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.5939\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.9035\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.5962\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.4794\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.7256\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.7772\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.0700\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.0445\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.9426\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.9888\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.4839\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.8617\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.4755\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.8141\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.0882\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.3809\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.6718\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.9494\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.4589\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.0135\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.9022\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.8679\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.8328\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.1561\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.0491\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.3912\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.7119\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.9135\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.9934\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.6128\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.2485\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.1145\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.9459\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.0741\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.9518\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.7829\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.7510\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.9228\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.5190\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.6836\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.6063\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.7358\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.9675\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.8211\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.2618\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.9165\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.9160\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.0236\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.7659\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.3824\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.8147\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.1516\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.9267\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.1462\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.9407\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.5818\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.2668\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.3995\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.2229\n",
      "\n",
      "ğŸ“Š Epoch 10 Summary:\n",
      "   âœ… Accuracy: 0.0653\n",
      "   ğŸ” Recall:   0.0653\n",
      "   â­ F1 Score: 0.0445\n",
      "\n",
      "ğŸŒ€ Epoch 11/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.3708\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.8158\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.3020\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.8176\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.4404\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.6361\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.6453\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.3949\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.3027\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.8197\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.8546\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.5254\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.6744\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.3326\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.0741\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.9820\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.7066\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.5671\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.1114\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.5827\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.2433\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.2704\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.8016\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.8293\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.1405\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.2615\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.2743\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.0968\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.6431\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.7818\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.5149\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.2860\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.4961\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.7205\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.8665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 36/90  |  Loss: 3.7998\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.4954\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.6209\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.1044\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.5497\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.0107\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.8590\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.4080\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.3169\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.2893\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.4746\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.9148\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.5317\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.5462\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.1188\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.9039\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.5752\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.9177\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.1768\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.0967\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.7992\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.9199\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.5145\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.3732\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.2392\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.9460\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.5543\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.1596\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.9781\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.8210\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.4711\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.8969\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.9579\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.9729\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.3767\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.6683\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.6202\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.7969\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.0000\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.8203\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.0101\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.0444\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.0971\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.7017\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.7391\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.8981\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.5920\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.7822\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.6427\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.4290\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.3672\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.6293\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.6075\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.7545\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.1420\n",
      "\n",
      "ğŸ“Š Epoch 11 Summary:\n",
      "   âœ… Accuracy: 0.0694\n",
      "   ğŸ” Recall:   0.0694\n",
      "   â­ F1 Score: 0.0458\n",
      "\n",
      "ğŸŒ€ Epoch 12/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.8261\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.5373\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.0611\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.3933\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.7932\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.5496\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.1600\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.6825\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.9360\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.6076\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.6617\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.1430\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.9661\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.6140\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.6922\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.3579\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.3694\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.7972\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.7975\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.3554\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.6899\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.0579\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.6585\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.2350\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.9132\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.3421\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.2594\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.7433\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.2014\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.4258\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.0473\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.6645\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.3524\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.3649\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.9500\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.3755\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.4104\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.6236\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.6737\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.7761\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.4380\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.1478\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.6099\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.8683\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.0207\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.5408\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.7812\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.7166\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.7985\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.3098\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.6525\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.3888\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.6023\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.1596\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.3846\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.9482\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.6501\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.1336\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.7255\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.6576\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.3629\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.7492\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.5282\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.9516\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.8456\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.9694\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.7970\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.8996\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.9306\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.8856\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.0077\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.0702\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.8662\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.9187\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.5822\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.7014\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.9997\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.5341\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.4703\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.1217\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.2600\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.8910\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.2751\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.5077\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.8717\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.0307\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.8717\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.7683\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.2864\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.6855\n",
      "\n",
      "ğŸ“Š Epoch 12 Summary:\n",
      "   âœ… Accuracy: 0.0917\n",
      "   ğŸ” Recall:   0.0917\n",
      "   â­ F1 Score: 0.0675\n",
      "\n",
      "ğŸŒ€ Epoch 13/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.9972\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.3689\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.4401\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.4406\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.4032\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.6323\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.7091\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.5003\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.7316\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.6600\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.2324\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.0182\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.8110\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.0110\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.8172\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.3237\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.9239\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.2956\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.7204\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.0738\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.0000\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.9415\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.8620\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.5035\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.5987\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.5087\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.6848\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.6190\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.3891\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.7729\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.9727\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.6940\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.2725\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.1856\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.8360\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.1216\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.4068\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.0328\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.9683\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.3452\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.4109\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.3355\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.6105\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.7463\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.0128\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.8192\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.9960\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.1098\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.5566\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.6531\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.3528\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.8182\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.1403\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.9028\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.5092\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.9162\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.1704\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.4356\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.0997\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.7792\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.4900\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.7600\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.7740\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.9710\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.7883\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.4008\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.7004\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.9338\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.8269\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.7702\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.7278\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.1337\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.5497\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.7050\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.5088\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.5562\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.2214\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.5887\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.7384\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.2275\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.7870\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.6567\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.1345\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.2591\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.8549\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.6385\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.1974\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.1250\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.6440\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.6497\n",
      "\n",
      "ğŸ“Š Epoch 13 Summary:\n",
      "   âœ… Accuracy: 0.1014\n",
      "   ğŸ” Recall:   0.1014\n",
      "   â­ F1 Score: 0.0771\n",
      "\n",
      "ğŸŒ€ Epoch 14/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 1/90  |  Loss: 3.2588\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.1789\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.5408\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.9877\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.6120\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.3183\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.1866\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.6843\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.3819\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.1274\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.8346\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.5351\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.8724\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.4392\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.3627\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.0688\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.4863\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.8091\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.3734\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.8213\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.6415\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.4827\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.5721\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.9109\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.6356\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.8373\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.0517\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.7512\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.0814\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.1560\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.6133\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.5891\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.5576\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.6052\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.0821\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.4212\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.6494\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.5765\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.4961\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.9577\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.1850\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.5185\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.3023\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.3834\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.4904\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.4449\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.0505\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.6811\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.9762\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.7146\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.8765\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.3303\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.2527\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.7060\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.7605\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.7553\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.0415\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.8059\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.0329\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.6621\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.9354\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.9927\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.2306\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.2866\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.5858\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.3173\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.4112\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.8432\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.6709\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.8681\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.3658\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.7237\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.5468\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.7502\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.7655\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.3427\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.9457\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.5520\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.3581\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.2384\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.8701\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.9224\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.7111\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.5382\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.6302\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.5008\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.4322\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.1762\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.1184\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.4612\n",
      "\n",
      "ğŸ“Š Epoch 14 Summary:\n",
      "   âœ… Accuracy: 0.1000\n",
      "   ğŸ” Recall:   0.1000\n",
      "   â­ F1 Score: 0.0692\n",
      "\n",
      "ğŸŒ€ Epoch 15/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.3511\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.2302\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.9101\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.5522\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.7577\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.2278\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.4335\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.4731\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.7602\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.6419\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.2879\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.1024\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.8744\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.4205\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.3142\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.3370\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.4137\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.1502\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.6142\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.0424\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.6240\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.4434\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.5520\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.9152\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.0502\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.1096\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.2179\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.3816\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.4326\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.3182\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.3640\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.4188\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.9174\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.6453\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.8823\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.0066\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.3848\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.8011\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.0614\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.3426\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.4428\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.9759\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.5920\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.6268\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.8397\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.6075\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.4361\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.3170\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.3792\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.9281\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.5143\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.8037\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.1360\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.0748\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.3982\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.4897\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.6198\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.1919\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.4923\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.1364\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.0020\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.6672\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.2672\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.5462\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.3426\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.0706\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.0550\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.6212\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.6538\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.4363\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.1174\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.7532\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.9045\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.3385\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.2770\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.4555\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.7384\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.7241\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.4743\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.4233\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.6536\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.3813\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.7776\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.7523\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.6523\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.5118\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.7974\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.7076\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.6942\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.2211\n",
      "\n",
      "ğŸ“Š Epoch 15 Summary:\n",
      "   âœ… Accuracy: 0.0889\n",
      "   ğŸ” Recall:   0.0889\n",
      "   â­ F1 Score: 0.0657\n",
      "\n",
      "ğŸŒ€ Epoch 16/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.4251\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.0211\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.2066\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.1843\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.3174\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.4460\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.3957\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.3279\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.6225\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.1398\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.1228\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.4882\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.3392\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.5313\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.4112\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.2334\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.6433\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.6115\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.4074\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.0538\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.3199\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.0995\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.3349\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.1474\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.8961\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.5257\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.1859\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.3192\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.1301\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.0817\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.2058\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.9368\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.1226\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.7046\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.9807\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.5091\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.8717\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.8525\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.3332\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.6495\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.4724\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.6224\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.4187\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.9656\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.8834\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.3070\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.5071\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.3049\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.2230\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.7709\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.3262\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.6599\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.6367\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.6857\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.8885\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.1365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 57/90  |  Loss: 3.5515\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.3320\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.2862\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.0568\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.5331\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.9913\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.1923\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.9914\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.9253\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.5612\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.8458\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.4093\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.1682\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.9170\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.3636\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.3729\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.5407\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.7004\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.8681\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.3576\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.5374\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.7647\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.7449\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.3124\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.3431\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.4853\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.6614\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.6673\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.1347\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.5508\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.6976\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.5829\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.2705\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.3082\n",
      "\n",
      "ğŸ“Š Epoch 16 Summary:\n",
      "   âœ… Accuracy: 0.0875\n",
      "   ğŸ” Recall:   0.0875\n",
      "   â­ F1 Score: 0.0660\n",
      "\n",
      "ğŸŒ€ Epoch 17/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.2595\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.3142\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.2775\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.3897\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.7648\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.0297\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.3447\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.2011\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.3839\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.1746\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.2826\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.4733\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.2616\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.1423\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.3268\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.8619\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.4464\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.4519\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.0873\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.6620\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.2368\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.7343\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.7705\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.2955\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.7465\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.1271\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.3101\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.0881\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.8119\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.0127\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.7359\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.9941\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.9447\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.3626\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.2197\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.1087\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.5912\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.8917\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.0396\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.9550\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.0849\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.2505\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.5217\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.8183\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.4033\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.2915\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.8500\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.8971\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.7371\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.5986\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.4927\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.2391\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.5290\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.0768\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.5111\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.4500\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.3193\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.5622\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.2645\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.5097\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.2132\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.9767\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.1933\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.7926\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.7897\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.7113\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.9208\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.7000\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.8909\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.1746\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.7595\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.3473\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.6725\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.5883\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.0700\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.4031\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.3876\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.3157\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.0525\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.6295\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.2183\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.0588\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.6612\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.4626\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.9481\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.5920\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.5840\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.7024\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.2450\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.5195\n",
      "\n",
      "ğŸ“Š Epoch 17 Summary:\n",
      "   âœ… Accuracy: 0.1069\n",
      "   ğŸ” Recall:   0.1069\n",
      "   â­ F1 Score: 0.0798\n",
      "\n",
      "ğŸŒ€ Epoch 18/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.9880\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.9874\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.4642\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.6792\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.8229\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.0828\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.5753\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.0139\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.4293\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.0993\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.3921\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.1118\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.9798\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.2043\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.2285\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.1621\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.2221\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.0281\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.8343\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.0219\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.6336\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.8184\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.6753\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.0202\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.0233\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.3887\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.9305\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.8276\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.4080\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.4997\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.4019\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.2469\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.5154\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.1335\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.9424\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.2386\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.5077\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.5734\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.5036\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.8522\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.3776\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.9598\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.4334\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.0509\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.3538\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.3608\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.7824\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.5398\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.2165\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.3189\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.0387\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.7691\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.8901\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.8396\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.3216\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.3977\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.2837\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.3701\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.8615\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.8240\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.9658\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.3991\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.0556\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.6454\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.1210\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.7268\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.2318\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.3717\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.6823\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.1304\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.9335\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.4114\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.6471\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.3802\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.4162\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.5759\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.3936\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.3529\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.4180\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.6718\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.0481\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.5040\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.5258\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.5383\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.4721\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.2537\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.6108\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.1428\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.2896\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.9188\n",
      "\n",
      "ğŸ“Š Epoch 18 Summary:\n",
      "   âœ… Accuracy: 0.1000\n",
      "   ğŸ” Recall:   0.1000\n",
      "   â­ F1 Score: 0.0763\n",
      "\n",
      "ğŸŒ€ Epoch 19/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.9600\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.3446\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.5609\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.2986\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.6756\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.7724\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.2950\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.0143\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.1026\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.2743\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.0712\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.9838\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.4983\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.9623\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.4659\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.6183\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.1798\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.6410\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.8237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 20/90  |  Loss: 3.6021\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.9407\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.3025\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.5766\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.4475\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.9499\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.7565\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.9695\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.5794\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.6706\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.1443\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.4740\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.5716\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.2818\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.5129\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.2255\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.8437\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.2308\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.9535\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.5405\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.7923\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.6727\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.4927\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.1404\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.9454\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.9789\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.6865\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.1170\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.9463\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.6062\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.9789\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.3480\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.3474\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.1438\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.6083\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.9973\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.0982\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.6337\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.1868\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.7004\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.8479\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.7126\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.7455\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.4963\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.2898\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.3507\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.8409\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.8117\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.0170\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.4628\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.6948\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.9295\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.9753\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.4463\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.4079\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.0340\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.8068\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.3282\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.9256\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.2562\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.1120\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.3689\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.2730\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.2613\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.0196\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.3875\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.0057\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.5968\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.1106\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.8998\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.8494\n",
      "\n",
      "ğŸ“Š Epoch 19 Summary:\n",
      "   âœ… Accuracy: 0.1069\n",
      "   ğŸ” Recall:   0.1069\n",
      "   â­ F1 Score: 0.0921\n",
      "\n",
      "ğŸŒ€ Epoch 20/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.3671\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.4053\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.4150\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.0826\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.1523\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.8679\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.8424\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.8443\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.3064\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.1247\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.0952\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.6314\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.1566\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.2563\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.1564\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.0343\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.8764\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.3237\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.8139\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.2452\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.5600\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.8574\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.9527\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.9908\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.3156\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.9859\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.3531\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.7188\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.4602\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.2536\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.4245\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.4956\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.0896\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.0119\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.5852\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.0336\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.9214\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.9677\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.7633\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.3337\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.2958\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.4656\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.6120\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.2590\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.4160\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.2431\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.9169\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.0130\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.5565\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.9705\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.8470\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.3065\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.5667\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.4617\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.5627\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.1262\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.0900\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.7193\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.8719\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.7552\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.3931\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.2404\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.8634\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.7338\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.2161\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.6487\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.8089\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.9528\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.3021\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.6795\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.1736\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.8680\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.3609\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.2936\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.2923\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.0694\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.7832\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.8458\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.2412\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.1774\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.0683\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.1927\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.5013\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.7656\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.9578\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.4507\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.2217\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.9349\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.3657\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.1401\n",
      "\n",
      "ğŸ“Š Epoch 20 Summary:\n",
      "   âœ… Accuracy: 0.1250\n",
      "   ğŸ” Recall:   0.1250\n",
      "   â­ F1 Score: 0.0990\n",
      "\n",
      "ğŸŒ€ Epoch 21/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.2233\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.5891\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.1432\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.0372\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.1531\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.6317\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.4562\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.4207\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.0042\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.0326\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.1799\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.4056\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.5593\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.3200\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.4094\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.9873\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.9368\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.4425\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.6533\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.3068\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.0026\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.3887\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.0985\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.1531\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.4763\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.0967\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.0615\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.2763\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.8767\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.2568\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.0945\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.9962\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.5407\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.8934\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.0571\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.8618\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.4484\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.1019\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.6024\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.0187\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.4591\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.2773\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.6364\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.9704\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.1443\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.6524\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.6779\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.3994\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.2907\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.3167\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.1529\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.5092\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.5532\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.2964\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.1219\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.3594\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.9288\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.1255\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.3587\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.3692\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.7294\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.4067\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.0337\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.8134\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.0275\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.2282\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.0693\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.9263\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.3486\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.7227\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.9889\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.3039\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.7385\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.5930\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.4340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 76/90  |  Loss: 3.3403\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.2015\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.7940\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.2687\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.4488\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.6909\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.3661\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.5971\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.0920\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.2310\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.8733\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.8655\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.2865\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.4090\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.4774\n",
      "\n",
      "ğŸ“Š Epoch 21 Summary:\n",
      "   âœ… Accuracy: 0.1264\n",
      "   ğŸ” Recall:   0.1264\n",
      "   â­ F1 Score: 0.1030\n",
      "\n",
      "ğŸŒ€ Epoch 22/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.4194\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.0954\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.4634\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.6279\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.4687\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.8749\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.3782\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.6262\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.8935\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.0316\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.1119\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.0094\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.3841\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.0098\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.0409\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.9837\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.1868\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.3348\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.6074\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.0491\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.3278\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.3930\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.8140\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.1742\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.9288\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.2813\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.0896\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.9372\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.8675\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.9827\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.1764\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.7978\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.1974\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.2910\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.4239\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.2172\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.1225\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.5768\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.3377\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.1907\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.1660\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.2613\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.6423\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.7745\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.8569\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.2039\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.2839\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.6576\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.4695\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.5696\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.0944\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.3908\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.4680\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.5874\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.3843\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.7466\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.0786\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.0237\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.7398\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.8507\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.6486\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.1013\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.2468\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.6538\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.5459\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.4948\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.0935\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.2659\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.4965\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.4768\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.6699\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.0248\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.4993\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.1295\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.3417\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.4094\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.7005\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.9981\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.1489\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.8669\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.3382\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.1478\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.2341\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.8655\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.4282\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.1821\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.4867\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.1978\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.3788\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.1150\n",
      "\n",
      "ğŸ“Š Epoch 22 Summary:\n",
      "   âœ… Accuracy: 0.1292\n",
      "   ğŸ” Recall:   0.1292\n",
      "   â­ F1 Score: 0.1033\n",
      "\n",
      "ğŸŒ€ Epoch 23/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.3064\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.0380\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.3295\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.0569\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.3656\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.7167\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.4619\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.5374\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.0684\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.0447\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.3714\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.8926\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.1676\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.7273\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.5669\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.2479\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.8112\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.9856\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.8940\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.5163\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.2356\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.1490\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.3962\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.9451\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.3736\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.3742\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.0605\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.7083\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.2063\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.4543\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.5770\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.0947\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.0677\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.7045\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.9775\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.9840\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.0004\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.7750\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.8127\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.2980\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.9973\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.5275\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.3943\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.0237\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.0438\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.0926\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.7503\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.0439\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.3763\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.6563\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.0023\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.0596\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.1468\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.5153\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.6826\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.6312\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.5551\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.6964\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.3363\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.5950\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.7993\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.3087\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.1786\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.5416\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.9579\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.8050\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.5786\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.6894\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.1733\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.8745\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.2566\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.8513\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.0194\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.5643\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.8454\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.1355\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.5411\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.8725\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.2068\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.0894\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.5941\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.5730\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.9679\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.5221\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.9384\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.3217\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.0272\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.6411\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.6618\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.6964\n",
      "\n",
      "ğŸ“Š Epoch 23 Summary:\n",
      "   âœ… Accuracy: 0.1583\n",
      "   ğŸ” Recall:   0.1583\n",
      "   â­ F1 Score: 0.1297\n",
      "\n",
      "ğŸŒ€ Epoch 24/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.1688\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.2118\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.4615\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.7512\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.8029\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.8204\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.2579\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.0720\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.4566\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.6492\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.5314\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.6710\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.2265\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.7215\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.9263\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.3445\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.7459\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.2055\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.7271\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.2888\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.8518\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.1404\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.4400\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.2079\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.1502\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.6849\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.7390\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.8249\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.5675\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.2190\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.8702\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.3024\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.6212\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.3935\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.1559\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.3015\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.8320\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.0674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 39/90  |  Loss: 2.7861\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.1231\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.9259\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.7380\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.4173\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.8031\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.2069\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.9634\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.7536\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.0049\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.1462\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.4981\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.2749\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.6144\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.6032\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.8196\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.4208\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.7450\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.0297\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.7124\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.2111\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.6273\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.3059\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.9826\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.0294\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.5632\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.7159\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.6964\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.3707\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.5715\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.7864\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.0360\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.6401\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.9140\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.3369\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.1554\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.2184\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.0934\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.9026\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.6910\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.7165\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.4277\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.7436\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.7527\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.0322\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.2905\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.6585\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.2702\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.5352\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.5424\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.6463\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.3855\n",
      "\n",
      "ğŸ“Š Epoch 24 Summary:\n",
      "   âœ… Accuracy: 0.1556\n",
      "   ğŸ” Recall:   0.1556\n",
      "   â­ F1 Score: 0.1365\n",
      "\n",
      "ğŸŒ€ Epoch 25/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.6851\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.1391\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.7071\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.2913\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.8185\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.3067\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.0226\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.5293\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.2645\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.5055\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.7593\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.7829\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.4618\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.6740\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.4492\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.3822\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.6219\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.0476\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.8819\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.1870\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.6793\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.7478\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.0059\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.3305\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.4512\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.1135\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.9377\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.3210\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.3147\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.9326\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.5711\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.0129\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.0815\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.7008\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.4362\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.0469\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.6337\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.2687\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.1104\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.5329\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.6289\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.1428\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.6833\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.8088\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.4101\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.7985\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.3568\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.6263\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.8429\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.0652\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.5880\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.1880\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.3553\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.8587\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.5450\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.4700\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.8254\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.6034\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.3836\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.9931\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.6605\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.6415\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.3464\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.5731\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.8604\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.2168\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.2878\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.7308\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.5105\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.3338\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.8031\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.1973\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.0349\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.9168\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.5023\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.4245\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.1896\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.5301\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.9816\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.2565\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.2099\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.3379\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.8736\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.7175\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.2415\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.9310\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.4989\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.2564\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.5015\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.2164\n",
      "\n",
      "ğŸ“Š Epoch 25 Summary:\n",
      "   âœ… Accuracy: 0.1681\n",
      "   ğŸ” Recall:   0.1681\n",
      "   â­ F1 Score: 0.1451\n",
      "\n",
      "ğŸŒ€ Epoch 26/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.2647\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.2351\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.6035\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.4463\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.9202\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.2727\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.8954\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.0737\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.6645\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.8990\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.1350\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.0376\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.8242\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.4726\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.6031\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.6758\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.9742\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.9628\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.4425\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.3286\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.0265\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.4330\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.8720\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.0862\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.1659\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.7221\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.0141\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.8192\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.1766\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.9982\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.6729\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.3544\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.1918\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.8558\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.9677\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.0344\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.9878\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.9771\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.1570\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.6378\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.5271\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.6976\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.9699\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.6747\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.6874\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.6119\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.1404\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.7711\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.9153\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.6858\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.1143\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.2214\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.9836\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.1979\n",
      "  ğŸ” Batch 55/90  |  Loss: 1.9262\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.7983\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.9135\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.1723\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.9540\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.4939\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.6462\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.7252\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.5322\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.4383\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.3705\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.0368\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.9624\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.2717\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.7034\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.4524\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.9906\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.9200\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.6190\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.5194\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.9404\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.1971\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.7330\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.3823\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.8672\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.2081\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.9961\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.3186\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.3692\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.0228\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.7549\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.1977\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.7901\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.7782\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.4958\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.3031\n",
      "\n",
      "ğŸ“Š Epoch 26 Summary:\n",
      "   âœ… Accuracy: 0.1611\n",
      "   ğŸ” Recall:   0.1611\n",
      "   â­ F1 Score: 0.1389\n",
      "\n",
      "ğŸŒ€ Epoch 27/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.6758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 2/90  |  Loss: 3.0765\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.1511\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.7998\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.6302\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.4190\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.6873\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.1062\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.0634\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.1564\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.5826\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.6637\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.8251\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.8430\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.7146\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.4414\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.7165\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.7217\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.6975\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.0506\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.7087\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.0572\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.6443\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.1180\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.6234\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.5080\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.8704\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.0248\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.7736\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.2288\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.6747\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.7219\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.6353\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.0528\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.8491\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.6224\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.9876\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.4991\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.5300\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.1245\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.8482\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.3567\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.2677\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.3064\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.4483\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.0505\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.7646\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.4524\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.4162\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.4047\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.5234\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.8149\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.0176\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.8689\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.2370\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.1907\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.1893\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.4673\n",
      "  ğŸ” Batch 59/90  |  Loss: 3.5147\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.0143\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.6734\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.3822\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.0817\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.0028\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.5144\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.2295\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.6186\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.1433\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.8133\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.6629\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.8648\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.3175\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.8129\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.4999\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.8372\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.4740\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.7453\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.9388\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.0974\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.0334\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.9690\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.8581\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.0055\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.2097\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.9124\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.0285\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.6236\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.0793\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.2311\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.8874\n",
      "\n",
      "ğŸ“Š Epoch 27 Summary:\n",
      "   âœ… Accuracy: 0.1556\n",
      "   ğŸ” Recall:   0.1556\n",
      "   â­ F1 Score: 0.1350\n",
      "\n",
      "ğŸŒ€ Epoch 28/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.9442\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.2454\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.1985\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.0038\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.4752\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.5839\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.8601\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.7356\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.7780\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.6673\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.6559\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.4104\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.3312\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.0904\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.7926\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.7404\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.6243\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.6523\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.2534\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.4774\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.7591\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.1865\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.7716\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.4411\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.1308\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.9982\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.7924\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.7674\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.1369\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.6990\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.1362\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.4193\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.5180\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.2070\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.6542\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.3134\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.5379\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.1135\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.1037\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.4444\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.3733\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.7547\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.6083\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.7692\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.7651\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.8924\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.2213\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.4578\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.1673\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.5478\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.9224\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.0375\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.8437\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.4273\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.0324\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.3960\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.3449\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.8867\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.3732\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.4599\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.5359\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.6770\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.1053\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.3613\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.7816\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.8465\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.5836\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.7746\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.8137\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.4450\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.6535\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.0356\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.4354\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.0067\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.9533\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.7536\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.2821\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.7999\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.6348\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.2706\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.9560\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.4142\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.4878\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.8310\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.9276\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.2976\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.8827\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.8553\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.6714\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.5665\n",
      "\n",
      "ğŸ“Š Epoch 28 Summary:\n",
      "   âœ… Accuracy: 0.1750\n",
      "   ğŸ” Recall:   0.1750\n",
      "   â­ F1 Score: 0.1506\n",
      "\n",
      "ğŸŒ€ Epoch 29/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 3.1574\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.4434\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.7537\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.4560\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.1477\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.6876\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.4962\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.0407\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.9136\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.4814\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.1052\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.2831\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.7208\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.4101\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.8790\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.2746\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.4307\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.9441\n",
      "  ğŸ” Batch 19/90  |  Loss: 3.2599\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.7209\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.3097\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.1940\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.3278\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.9190\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.1056\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.2736\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.0673\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.1707\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.8351\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.8849\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.3604\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.9418\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.5309\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.1893\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.9033\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.5174\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.9473\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.1688\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.7233\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.5487\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.8240\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.4415\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.5817\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.4424\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.6647\n",
      "  ğŸ” Batch 46/90  |  Loss: 3.3726\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.1994\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.0262\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.5767\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.1223\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.0626\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.0335\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.3302\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.8689\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.2110\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.4137\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.4136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 58/90  |  Loss: 3.3779\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.5007\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.7707\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.6137\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.8672\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.3346\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.4366\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.0438\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.9133\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.1846\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.5957\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.8389\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.4308\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.9094\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.5396\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.3631\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.2329\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.2894\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.9322\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.0164\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.5231\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.5544\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.1669\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.0863\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.0007\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.4120\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.5983\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.6546\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.1976\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.7026\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.7515\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.1563\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.2256\n",
      "\n",
      "ğŸ“Š Epoch 29 Summary:\n",
      "   âœ… Accuracy: 0.1597\n",
      "   ğŸ” Recall:   0.1597\n",
      "   â­ F1 Score: 0.1348\n",
      "\n",
      "ğŸŒ€ Epoch 30/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.4941\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.2521\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.3059\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.6293\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.5820\n",
      "  ğŸ” Batch 6/90  |  Loss: 3.0038\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.3508\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.6237\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.7505\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.4107\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.1282\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.4949\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.5083\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.0076\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.2757\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.1581\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.9279\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.1726\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.9481\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.5264\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.2004\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.1560\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.1490\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.1838\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.1562\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.6409\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.4495\n",
      "  ğŸ” Batch 28/90  |  Loss: 3.1410\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.3033\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.0140\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.8648\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.6835\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.6414\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.3316\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.5630\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.9187\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.9762\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.5278\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.6072\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.7078\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.6705\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.7517\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.5647\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.7199\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.8971\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.5843\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.4845\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.2271\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.7462\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.4788\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.6528\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.9389\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.2650\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.9323\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.4579\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.1136\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.6758\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.6039\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.2230\n",
      "  ğŸ” Batch 60/90  |  Loss: 3.0649\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.5968\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.9025\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.3045\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.2094\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.3984\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.5447\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.1752\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.1308\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.9565\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.9017\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.8895\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.9686\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.0075\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.1993\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.9127\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.3985\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.5911\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.6583\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.8009\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.2061\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.0580\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.8624\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.6370\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.7465\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.8182\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.6447\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.0178\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.0508\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.5589\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.0164\n",
      "\n",
      "ğŸ“Š Epoch 30 Summary:\n",
      "   âœ… Accuracy: 0.1819\n",
      "   ğŸ” Recall:   0.1819\n",
      "   â­ F1 Score: 0.1623\n",
      "\n",
      "ğŸŒ€ Epoch 31/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.5962\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.3667\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.1794\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.7030\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.3475\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.5531\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.7696\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.2965\n",
      "  ğŸ” Batch 9/90  |  Loss: 3.4995\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.4254\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.5962\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.2225\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.7598\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.6813\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.9829\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.5383\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.7098\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.5637\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.7290\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.7889\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.3785\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.3386\n",
      "  ğŸ” Batch 23/90  |  Loss: 3.0055\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.9404\n",
      "  ğŸ” Batch 25/90  |  Loss: 3.1897\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.4432\n",
      "  ğŸ” Batch 27/90  |  Loss: 3.3871\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.6655\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.2764\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.0066\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.9188\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.8782\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.1213\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.6298\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.3354\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.6963\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.4040\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.5590\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.7140\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.7216\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.5268\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.9403\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.1608\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.8845\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.0145\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.7365\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.4729\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.6315\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.4774\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.1186\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.0704\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.6509\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.4448\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.1022\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.7695\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.1131\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.0677\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.5537\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.9523\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.6007\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.8658\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.6992\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.9578\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.1993\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.9811\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.6450\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.4694\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.6122\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.0182\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.0088\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.9229\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.3129\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.5385\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.8801\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.3048\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.2988\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.5574\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.9272\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.0103\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.1722\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.8389\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.3913\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.2265\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.4830\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.2793\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.3717\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.3752\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.7571\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.7250\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.8315\n",
      "\n",
      "ğŸ“Š Epoch 31 Summary:\n",
      "   âœ… Accuracy: 0.1958\n",
      "   ğŸ” Recall:   0.1958\n",
      "   â­ F1 Score: 0.1771\n",
      "\n",
      "ğŸŒ€ Epoch 32/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 1.8411\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.8730\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.7919\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.8485\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.4560\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.6642\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.7249\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.8382\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.3697\n",
      "  ğŸ” Batch 10/90  |  Loss: 3.1819\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.9330\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.1282\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.7114\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.5493\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.2224\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.9522\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.2918\n",
      "  ğŸ” Batch 18/90  |  Loss: 1.9870\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.8596\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.5923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 21/90  |  Loss: 2.8419\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.0234\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.9714\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.4082\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.8659\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.5685\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.6200\n",
      "  ğŸ” Batch 28/90  |  Loss: 1.9735\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.6684\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.8968\n",
      "  ğŸ” Batch 31/90  |  Loss: 3.0457\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.9069\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.4522\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.6295\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.7756\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.1745\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.7613\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.3311\n",
      "  ğŸ” Batch 39/90  |  Loss: 3.4527\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.4878\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.2253\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.6519\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.6853\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.2552\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.0073\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.6047\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.4052\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.4770\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.2180\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.2767\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.8218\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.9295\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.6435\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.3651\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.8234\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.0999\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.0915\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.8573\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.9808\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.0928\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.5735\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.6259\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.3338\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.6141\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.8081\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.8987\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.7874\n",
      "  ğŸ” Batch 68/90  |  Loss: 3.3763\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.6608\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.1607\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.5451\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.7039\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.7919\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.8900\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.8073\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.8952\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.4734\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.3621\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.3345\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.2816\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.4432\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.5700\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.3283\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.7140\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.6717\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.6788\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.9471\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.4832\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.4905\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.6375\n",
      "\n",
      "ğŸ“Š Epoch 32 Summary:\n",
      "   âœ… Accuracy: 0.2125\n",
      "   ğŸ” Recall:   0.2125\n",
      "   â­ F1 Score: 0.1896\n",
      "\n",
      "ğŸŒ€ Epoch 33/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.4980\n",
      "  ğŸ” Batch 2/90  |  Loss: 3.1301\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.0848\n",
      "  ğŸ” Batch 4/90  |  Loss: 1.6228\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.0086\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.0456\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.7426\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.9243\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.2096\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.3780\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.8897\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.8908\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.5835\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.6545\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.5216\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.7554\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.5087\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.9524\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.5022\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.7408\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.2356\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.7194\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.5791\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.8311\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.3642\n",
      "  ğŸ” Batch 26/90  |  Loss: 3.4206\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.1461\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.4695\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.3110\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.0632\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.1572\n",
      "  ğŸ” Batch 32/90  |  Loss: 3.8729\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.7935\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.2783\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.8862\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.5287\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.4587\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.2579\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.5024\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.5849\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.8088\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.7650\n",
      "  ğŸ” Batch 43/90  |  Loss: 3.2364\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.5036\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.5188\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.6300\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.9408\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.7025\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.5334\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.1037\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.3347\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.2351\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.3311\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.4133\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.1044\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.5622\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.8778\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.3389\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.8291\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.6675\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.9896\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.1362\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.9408\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.7822\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.8940\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.0585\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.4156\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.6934\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.8828\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.2729\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.8687\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.9185\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.0201\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.7198\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.1783\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.3004\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.9884\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.4103\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.7881\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.7181\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.9885\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.2759\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.5689\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.5214\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.9542\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.0400\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.2272\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.7650\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.7157\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.2346\n",
      "\n",
      "ğŸ“Š Epoch 33 Summary:\n",
      "   âœ… Accuracy: 0.2097\n",
      "   ğŸ” Recall:   0.2097\n",
      "   â­ F1 Score: 0.1860\n",
      "\n",
      "ğŸŒ€ Epoch 34/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.8173\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.0445\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.4605\n",
      "  ğŸ” Batch 4/90  |  Loss: 3.1268\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.5360\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.1984\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.3615\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.0473\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.9282\n",
      "  ğŸ” Batch 10/90  |  Loss: 1.7305\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.4512\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.4719\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.4893\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.2964\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.6287\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.2126\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.7081\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.5075\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.1241\n",
      "  ğŸ” Batch 20/90  |  Loss: 3.3922\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.2156\n",
      "  ğŸ” Batch 22/90  |  Loss: 3.0925\n",
      "  ğŸ” Batch 23/90  |  Loss: 1.8730\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.3743\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.9577\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.5770\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.5204\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.8354\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.6178\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.9438\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.7241\n",
      "  ğŸ” Batch 32/90  |  Loss: 1.7757\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.4217\n",
      "  ğŸ” Batch 34/90  |  Loss: 3.2146\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.9532\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.7446\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.9283\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.4739\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.4678\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.1212\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.5436\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.2621\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.8964\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.0030\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.1413\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.5280\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.8801\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.2420\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.4547\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.3778\n",
      "  ğŸ” Batch 51/90  |  Loss: 3.2564\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.3898\n",
      "  ğŸ” Batch 53/90  |  Loss: 3.2780\n",
      "  ğŸ” Batch 54/90  |  Loss: 3.1131\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.8274\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.1502\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.8235\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.8798\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.5999\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.9739\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.9218\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.8526\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.5327\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.4828\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.8376\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.5411\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.5677\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.7392\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.7167\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.4844\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.6977\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.6227\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.2194\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.0116\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.1515\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 77/90  |  Loss: 2.1259\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.8937\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.9406\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.9469\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.1139\n",
      "  ğŸ” Batch 82/90  |  Loss: 3.1063\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.8787\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.7340\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.6798\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.1421\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.8471\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.9086\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.0898\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.9140\n",
      "\n",
      "ğŸ“Š Epoch 34 Summary:\n",
      "   âœ… Accuracy: 0.2083\n",
      "   ğŸ” Recall:   0.2083\n",
      "   â­ F1 Score: 0.1921\n",
      "\n",
      "ğŸŒ€ Epoch 35/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.4757\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.8797\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.5858\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.2908\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.6952\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.8380\n",
      "  ğŸ” Batch 7/90  |  Loss: 3.6668\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.2864\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.0127\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.4900\n",
      "  ğŸ” Batch 11/90  |  Loss: 1.9218\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.0171\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.9388\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.5299\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.0638\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.2582\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.2561\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.4279\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.5669\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.9780\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.4063\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.6120\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.0674\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.5305\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.3476\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.5191\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.8054\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.6463\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.6194\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.4720\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.8125\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.2645\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.0327\n",
      "  ğŸ” Batch 34/90  |  Loss: 1.8919\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.1081\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.6879\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.0156\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.2562\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.7298\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.6551\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.4919\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.7040\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.4633\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.8736\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.9702\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.3683\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.4639\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.8466\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.4169\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.4548\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.5471\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.4479\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.7579\n",
      "  ğŸ” Batch 54/90  |  Loss: 1.9724\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.7086\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.0762\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.0935\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.1053\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.9572\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.9929\n",
      "  ğŸ” Batch 61/90  |  Loss: 3.0490\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.5453\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.9746\n",
      "  ğŸ” Batch 64/90  |  Loss: 1.6369\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.6498\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.7688\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.6733\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.6386\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.7342\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.5891\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.3067\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.8746\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.5980\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.7855\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.2897\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.3751\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.7511\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.0137\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.4155\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.2021\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.8540\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.7205\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.5359\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.1016\n",
      "  ğŸ” Batch 85/90  |  Loss: 1.8731\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.8813\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.5652\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.8637\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.6741\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.9208\n",
      "\n",
      "ğŸ“Š Epoch 35 Summary:\n",
      "   âœ… Accuracy: 0.2556\n",
      "   ğŸ” Recall:   0.2556\n",
      "   â­ F1 Score: 0.2321\n",
      "\n",
      "ğŸŒ€ Epoch 36/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.2001\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.7075\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.1858\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.1467\n",
      "  ğŸ” Batch 5/90  |  Loss: 3.0183\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.4301\n",
      "  ğŸ” Batch 7/90  |  Loss: 1.7653\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.5234\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.2600\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.0776\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.6695\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.9344\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.6257\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.3817\n",
      "  ğŸ” Batch 15/90  |  Loss: 3.2749\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.2051\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.6320\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.1937\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.8052\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.9087\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.5766\n",
      "  ğŸ” Batch 22/90  |  Loss: 1.6217\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.6437\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.0883\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.2150\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.6758\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.4015\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.9643\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.9669\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.6090\n",
      "  ğŸ” Batch 31/90  |  Loss: 1.4996\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.5043\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.2329\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.1889\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.5650\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.4806\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.3614\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.7407\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.4785\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.7607\n",
      "  ğŸ” Batch 41/90  |  Loss: 3.1667\n",
      "  ğŸ” Batch 42/90  |  Loss: 3.3857\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.0202\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.5071\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.4266\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.7057\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.6887\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.8314\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.3421\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.9150\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.7805\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.5108\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.5656\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.6371\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.2316\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.2214\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.6349\n",
      "  ğŸ” Batch 58/90  |  Loss: 3.1774\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.3498\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.3572\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.9666\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.8894\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.7275\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.7990\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.1404\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.4288\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.2069\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.2600\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.4906\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.7431\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.6282\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.4902\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.5614\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.7853\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.8458\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.4196\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.8717\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.2172\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.6586\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.9884\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.8633\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.6841\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.5185\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.8561\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.0473\n",
      "  ğŸ” Batch 86/90  |  Loss: 3.0634\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.3998\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.1078\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.9110\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.5624\n",
      "\n",
      "ğŸ“Š Epoch 36 Summary:\n",
      "   âœ… Accuracy: 0.2250\n",
      "   ğŸ” Recall:   0.2250\n",
      "   â­ F1 Score: 0.2120\n",
      "\n",
      "ğŸŒ€ Epoch 37/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.1651\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.6755\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.6623\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.9609\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.1682\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.9681\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.7367\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.7034\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.5850\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.4048\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.7571\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.6231\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.7165\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.1094\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.8457\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.2567\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.5666\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.2603\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.8569\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.6237\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.3048\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.3718\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.6186\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.5665\n",
      "  ğŸ” Batch 25/90  |  Loss: 1.8669\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.7450\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.5911\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.8376\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.7857\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.4220\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.1979\n",
      "  ğŸ” Batch 32/90  |  Loss: 1.7918\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.2484\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.5653\n",
      "  ğŸ” Batch 35/90  |  Loss: 3.2075\n",
      "  ğŸ” Batch 36/90  |  Loss: 3.3108\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.2454\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.3785\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.2307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 40/90  |  Loss: 3.1834\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.1490\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.9624\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.8417\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.6962\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.9028\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.7344\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.8616\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.0834\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.0901\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.9343\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.4927\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.0127\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.7074\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.7823\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.7602\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.5306\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.2073\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.2681\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.7954\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.4036\n",
      "  ğŸ” Batch 61/90  |  Loss: 1.8179\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.0420\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.8091\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.5572\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.9243\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.8699\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.7560\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.8852\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.2088\n",
      "  ğŸ” Batch 70/90  |  Loss: 3.1851\n",
      "  ğŸ” Batch 71/90  |  Loss: 3.0797\n",
      "  ğŸ” Batch 72/90  |  Loss: 1.8961\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.9585\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.8682\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.4524\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.4762\n",
      "  ğŸ” Batch 77/90  |  Loss: 3.0539\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.7790\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.7011\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.0781\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.0382\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.6192\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.9224\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.0788\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.7294\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.8224\n",
      "  ğŸ” Batch 87/90  |  Loss: 3.8722\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.0378\n",
      "  ğŸ” Batch 89/90  |  Loss: 3.0909\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.0759\n",
      "\n",
      "ğŸ“Š Epoch 37 Summary:\n",
      "   âœ… Accuracy: 0.2431\n",
      "   ğŸ” Recall:   0.2431\n",
      "   â­ F1 Score: 0.2233\n",
      "\n",
      "ğŸŒ€ Epoch 38/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.6880\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.6309\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.0283\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.5467\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.5115\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.1256\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.2570\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.9242\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.7360\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.5026\n",
      "  ğŸ” Batch 11/90  |  Loss: 3.1540\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.2350\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.6486\n",
      "  ğŸ” Batch 14/90  |  Loss: 3.2921\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.6269\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.6065\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.2007\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.9081\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.5100\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.0115\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.7970\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.6894\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.8797\n",
      "  ğŸ” Batch 24/90  |  Loss: 3.0621\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.6901\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.5795\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.3264\n",
      "  ğŸ” Batch 28/90  |  Loss: 1.8794\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.0753\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.7785\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.7830\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.0334\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.2955\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.5940\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.7963\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.5328\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.6181\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.2433\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.5123\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.0118\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.4571\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.7414\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.7735\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.4605\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.1644\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.6516\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.9318\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.0935\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.3110\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.5743\n",
      "  ğŸ” Batch 51/90  |  Loss: 1.7789\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.5571\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.7597\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.5596\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.0699\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.9936\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.9789\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.1934\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.8732\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.6840\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.7840\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.3460\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.1702\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.2412\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.4064\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.8016\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.5157\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.1318\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.8704\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.6075\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.6074\n",
      "  ğŸ” Batch 72/90  |  Loss: 3.0381\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.9000\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.3660\n",
      "  ğŸ” Batch 75/90  |  Loss: 3.8667\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.8051\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.8401\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.5527\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.8591\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.8368\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.7130\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.3932\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.2107\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.4659\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.1534\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.6507\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.5127\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.4859\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.7625\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.6464\n",
      "\n",
      "ğŸ“Š Epoch 38 Summary:\n",
      "   âœ… Accuracy: 0.2639\n",
      "   ğŸ” Recall:   0.2639\n",
      "   â­ F1 Score: 0.2397\n",
      "\n",
      "ğŸŒ€ Epoch 39/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.4649\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.5976\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.5658\n",
      "  ğŸ” Batch 4/90  |  Loss: 1.9163\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.7923\n",
      "  ğŸ” Batch 6/90  |  Loss: 1.9940\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.2820\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.0533\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.0685\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.8399\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.2998\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.8310\n",
      "  ğŸ” Batch 13/90  |  Loss: 3.0426\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.2781\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.2857\n",
      "  ğŸ” Batch 16/90  |  Loss: 3.2522\n",
      "  ğŸ” Batch 17/90  |  Loss: 1.9307\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.7624\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.3940\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.1426\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.1377\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.4856\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.9781\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.2717\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.9469\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.7528\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.5511\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.7170\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.2502\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.3807\n",
      "  ğŸ” Batch 31/90  |  Loss: 1.9625\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.4923\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.3477\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.8142\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.1976\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.4850\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.0326\n",
      "  ğŸ” Batch 38/90  |  Loss: 3.0132\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.4566\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.2133\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.5274\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.5198\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.2700\n",
      "  ğŸ” Batch 44/90  |  Loss: 3.0100\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.2556\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.2358\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.3702\n",
      "  ğŸ” Batch 48/90  |  Loss: 3.0115\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.1360\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.1488\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.9439\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.5582\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.8373\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.0080\n",
      "  ğŸ” Batch 55/90  |  Loss: 1.8940\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.2822\n",
      "  ğŸ” Batch 57/90  |  Loss: 3.2862\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.9905\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.0003\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.7054\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.0801\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.9685\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.3392\n",
      "  ğŸ” Batch 64/90  |  Loss: 3.0726\n",
      "  ğŸ” Batch 65/90  |  Loss: 3.0474\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.0960\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.5966\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.7256\n",
      "  ğŸ” Batch 69/90  |  Loss: 3.3843\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.4646\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.4467\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.7024\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.3752\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.1701\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.3811\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.3068\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.9994\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.9867\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.3939\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.0218\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.5662\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.4419\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.6446\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.0230\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.9337\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.4327\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.8899\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.5926\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.5438\n",
      "  ğŸ” Batch 90/90  |  Loss: 3.6253\n",
      "\n",
      "ğŸ“Š Epoch 39 Summary:\n",
      "   âœ… Accuracy: 0.2833\n",
      "   ğŸ” Recall:   0.2833\n",
      "   â­ F1 Score: 0.2681\n",
      "\n",
      "ğŸŒ€ Epoch 40/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.2906\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.5247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 3/90  |  Loss: 3.2386\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.0449\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.2500\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.7107\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.4407\n",
      "  ğŸ” Batch 8/90  |  Loss: 1.7089\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.8017\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.4885\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.0817\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.3374\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.4729\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.3568\n",
      "  ğŸ” Batch 15/90  |  Loss: 1.8851\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.1466\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.6135\n",
      "  ğŸ” Batch 18/90  |  Loss: 3.6008\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.9426\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.2622\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.2046\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.0455\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.4437\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.6677\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.3630\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.7695\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.0877\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.1934\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.6714\n",
      "  ğŸ” Batch 30/90  |  Loss: 1.2402\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.4363\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.4485\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.5607\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.7302\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.6831\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.5116\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.6249\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.3181\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.3400\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.3242\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.7913\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.9356\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.7714\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.7907\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.9959\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.5339\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.3494\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.0915\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.2794\n",
      "  ğŸ” Batch 50/90  |  Loss: 3.0946\n",
      "  ğŸ” Batch 51/90  |  Loss: 1.8481\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.5383\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.7934\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.0688\n",
      "  ğŸ” Batch 55/90  |  Loss: 3.3237\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.5275\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.5775\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.6189\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.3138\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.5596\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.0496\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.5984\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.4296\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.3546\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.4025\n",
      "  ğŸ” Batch 66/90  |  Loss: 3.0032\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.9776\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.7040\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.5943\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.8411\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.7836\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.2628\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.5706\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.8056\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.8419\n",
      "  ğŸ” Batch 76/90  |  Loss: 3.3750\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.1638\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.1690\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.5395\n",
      "  ğŸ” Batch 80/90  |  Loss: 3.2660\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.8561\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.8631\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.6059\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.3563\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.8737\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.6142\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.9165\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.3253\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.4472\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.4969\n",
      "\n",
      "ğŸ“Š Epoch 40 Summary:\n",
      "   âœ… Accuracy: 0.2444\n",
      "   ğŸ” Recall:   0.2444\n",
      "   â­ F1 Score: 0.2290\n",
      "\n",
      "ğŸŒ€ Epoch 41/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.3548\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.5806\n",
      "  ğŸ” Batch 3/90  |  Loss: 1.8205\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.0782\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.3447\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.2513\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.7872\n",
      "  ğŸ” Batch 8/90  |  Loss: 3.3170\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.3551\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.3721\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.7888\n",
      "  ğŸ” Batch 12/90  |  Loss: 3.1269\n",
      "  ğŸ” Batch 13/90  |  Loss: 1.8942\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.1221\n",
      "  ğŸ” Batch 15/90  |  Loss: 2.6981\n",
      "  ğŸ” Batch 16/90  |  Loss: 1.6085\n",
      "  ğŸ” Batch 17/90  |  Loss: 2.4129\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.6339\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.8587\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.6603\n",
      "  ğŸ” Batch 21/90  |  Loss: 3.1949\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.0704\n",
      "  ğŸ” Batch 23/90  |  Loss: 1.9149\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.7267\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.6128\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.8491\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.2381\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.1712\n",
      "  ğŸ” Batch 29/90  |  Loss: 3.1623\n",
      "  ğŸ” Batch 30/90  |  Loss: 2.5495\n",
      "  ğŸ” Batch 31/90  |  Loss: 2.8118\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.2846\n",
      "  ğŸ” Batch 33/90  |  Loss: 2.6976\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.4291\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.4576\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.1266\n",
      "  ğŸ” Batch 37/90  |  Loss: 2.3549\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.1531\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.6715\n",
      "  ğŸ” Batch 40/90  |  Loss: 2.6837\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.3502\n",
      "  ğŸ” Batch 42/90  |  Loss: 1.8367\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.4434\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.4572\n",
      "  ğŸ” Batch 45/90  |  Loss: 3.2261\n",
      "  ğŸ” Batch 46/90  |  Loss: 1.4313\n",
      "  ğŸ” Batch 47/90  |  Loss: 2.7499\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.5437\n",
      "  ğŸ” Batch 49/90  |  Loss: 3.2528\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.2112\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.7691\n",
      "  ğŸ” Batch 52/90  |  Loss: 3.1819\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.6680\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.7206\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.4937\n",
      "  ğŸ” Batch 56/90  |  Loss: 3.1102\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.5315\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.1654\n",
      "  ğŸ” Batch 59/90  |  Loss: 2.2367\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.3762\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.9678\n",
      "  ğŸ” Batch 62/90  |  Loss: 3.0600\n",
      "  ğŸ” Batch 63/90  |  Loss: 3.2657\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.5880\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.8182\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.5223\n",
      "  ğŸ” Batch 67/90  |  Loss: 2.0345\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.9166\n",
      "  ğŸ” Batch 69/90  |  Loss: 1.9824\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.2794\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.5619\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.9178\n",
      "  ğŸ” Batch 73/90  |  Loss: 2.6967\n",
      "  ğŸ” Batch 74/90  |  Loss: 2.7969\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.9118\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.6401\n",
      "  ğŸ” Batch 77/90  |  Loss: 2.1644\n",
      "  ğŸ” Batch 78/90  |  Loss: 2.2438\n",
      "  ğŸ” Batch 79/90  |  Loss: 3.0990\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.0776\n",
      "  ğŸ” Batch 81/90  |  Loss: 2.7887\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.5777\n",
      "  ğŸ” Batch 83/90  |  Loss: 3.5512\n",
      "  ğŸ” Batch 84/90  |  Loss: 3.0790\n",
      "  ğŸ” Batch 85/90  |  Loss: 3.4096\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.9438\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.3409\n",
      "  ğŸ” Batch 88/90  |  Loss: 3.1135\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.5502\n",
      "  ğŸ” Batch 90/90  |  Loss: 2.0606\n",
      "\n",
      "ğŸ“Š Epoch 41 Summary:\n",
      "   âœ… Accuracy: 0.2694\n",
      "   ğŸ” Recall:   0.2694\n",
      "   â­ F1 Score: 0.2551\n",
      "\n",
      "ğŸŒ€ Epoch 42/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 1.8017\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.7347\n",
      "  ğŸ” Batch 3/90  |  Loss: 3.2867\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.6591\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.7562\n",
      "  ğŸ” Batch 6/90  |  Loss: 1.6430\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.0761\n",
      "  ğŸ” Batch 8/90  |  Loss: 2.2073\n",
      "  ğŸ” Batch 9/90  |  Loss: 2.5021\n",
      "  ğŸ” Batch 10/90  |  Loss: 2.4401\n",
      "  ğŸ” Batch 11/90  |  Loss: 2.6793\n",
      "  ğŸ” Batch 12/90  |  Loss: 2.0528\n",
      "  ğŸ” Batch 13/90  |  Loss: 2.1625\n",
      "  ğŸ” Batch 14/90  |  Loss: 2.8173\n",
      "  ğŸ” Batch 15/90  |  Loss: 1.8560\n",
      "  ğŸ” Batch 16/90  |  Loss: 2.3247\n",
      "  ğŸ” Batch 17/90  |  Loss: 3.0308\n",
      "  ğŸ” Batch 18/90  |  Loss: 2.4578\n",
      "  ğŸ” Batch 19/90  |  Loss: 2.1074\n",
      "  ğŸ” Batch 20/90  |  Loss: 2.6077\n",
      "  ğŸ” Batch 21/90  |  Loss: 2.4072\n",
      "  ğŸ” Batch 22/90  |  Loss: 2.1297\n",
      "  ğŸ” Batch 23/90  |  Loss: 2.3307\n",
      "  ğŸ” Batch 24/90  |  Loss: 2.7791\n",
      "  ğŸ” Batch 25/90  |  Loss: 2.4466\n",
      "  ğŸ” Batch 26/90  |  Loss: 2.7672\n",
      "  ğŸ” Batch 27/90  |  Loss: 2.5127\n",
      "  ğŸ” Batch 28/90  |  Loss: 2.5029\n",
      "  ğŸ” Batch 29/90  |  Loss: 2.2695\n",
      "  ğŸ” Batch 30/90  |  Loss: 3.5161\n",
      "  ğŸ” Batch 31/90  |  Loss: 1.7720\n",
      "  ğŸ” Batch 32/90  |  Loss: 2.7226\n",
      "  ğŸ” Batch 33/90  |  Loss: 3.0442\n",
      "  ğŸ” Batch 34/90  |  Loss: 2.3600\n",
      "  ğŸ” Batch 35/90  |  Loss: 2.5174\n",
      "  ğŸ” Batch 36/90  |  Loss: 2.4344\n",
      "  ğŸ” Batch 37/90  |  Loss: 3.1523\n",
      "  ğŸ” Batch 38/90  |  Loss: 2.4742\n",
      "  ğŸ” Batch 39/90  |  Loss: 2.1597\n",
      "  ğŸ” Batch 40/90  |  Loss: 3.6341\n",
      "  ğŸ” Batch 41/90  |  Loss: 2.2779\n",
      "  ğŸ” Batch 42/90  |  Loss: 2.5888\n",
      "  ğŸ” Batch 43/90  |  Loss: 2.7367\n",
      "  ğŸ” Batch 44/90  |  Loss: 2.1403\n",
      "  ğŸ” Batch 45/90  |  Loss: 2.1036\n",
      "  ğŸ” Batch 46/90  |  Loss: 2.0530\n",
      "  ğŸ” Batch 47/90  |  Loss: 3.8788\n",
      "  ğŸ” Batch 48/90  |  Loss: 2.0480\n",
      "  ğŸ” Batch 49/90  |  Loss: 2.6003\n",
      "  ğŸ” Batch 50/90  |  Loss: 2.5252\n",
      "  ğŸ” Batch 51/90  |  Loss: 2.0077\n",
      "  ğŸ” Batch 52/90  |  Loss: 2.6391\n",
      "  ğŸ” Batch 53/90  |  Loss: 2.4756\n",
      "  ğŸ” Batch 54/90  |  Loss: 2.2359\n",
      "  ğŸ” Batch 55/90  |  Loss: 2.2270\n",
      "  ğŸ” Batch 56/90  |  Loss: 2.7149\n",
      "  ğŸ” Batch 57/90  |  Loss: 2.2441\n",
      "  ğŸ” Batch 58/90  |  Loss: 2.7574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Batch 59/90  |  Loss: 2.4873\n",
      "  ğŸ” Batch 60/90  |  Loss: 2.2136\n",
      "  ğŸ” Batch 61/90  |  Loss: 2.3197\n",
      "  ğŸ” Batch 62/90  |  Loss: 2.5140\n",
      "  ğŸ” Batch 63/90  |  Loss: 2.5215\n",
      "  ğŸ” Batch 64/90  |  Loss: 2.0580\n",
      "  ğŸ” Batch 65/90  |  Loss: 2.6364\n",
      "  ğŸ” Batch 66/90  |  Loss: 2.5526\n",
      "  ğŸ” Batch 67/90  |  Loss: 3.0237\n",
      "  ğŸ” Batch 68/90  |  Loss: 2.8661\n",
      "  ğŸ” Batch 69/90  |  Loss: 2.7289\n",
      "  ğŸ” Batch 70/90  |  Loss: 2.1549\n",
      "  ğŸ” Batch 71/90  |  Loss: 2.0478\n",
      "  ğŸ” Batch 72/90  |  Loss: 2.3634\n",
      "  ğŸ” Batch 73/90  |  Loss: 3.4076\n",
      "  ğŸ” Batch 74/90  |  Loss: 3.2115\n",
      "  ğŸ” Batch 75/90  |  Loss: 2.0512\n",
      "  ğŸ” Batch 76/90  |  Loss: 2.6228\n",
      "  ğŸ” Batch 77/90  |  Loss: 1.5404\n",
      "  ğŸ” Batch 78/90  |  Loss: 3.0534\n",
      "  ğŸ” Batch 79/90  |  Loss: 2.5375\n",
      "  ğŸ” Batch 80/90  |  Loss: 2.4173\n",
      "  ğŸ” Batch 81/90  |  Loss: 3.3321\n",
      "  ğŸ” Batch 82/90  |  Loss: 2.7545\n",
      "  ğŸ” Batch 83/90  |  Loss: 2.3841\n",
      "  ğŸ” Batch 84/90  |  Loss: 2.3176\n",
      "  ğŸ” Batch 85/90  |  Loss: 2.4755\n",
      "  ğŸ” Batch 86/90  |  Loss: 2.2217\n",
      "  ğŸ” Batch 87/90  |  Loss: 2.4439\n",
      "  ğŸ” Batch 88/90  |  Loss: 2.1206\n",
      "  ğŸ” Batch 89/90  |  Loss: 2.6178\n",
      "  ğŸ” Batch 90/90  |  Loss: 1.8643\n",
      "\n",
      "ğŸ“Š Epoch 42 Summary:\n",
      "   âœ… Accuracy: 0.2819\n",
      "   ğŸ” Recall:   0.2819\n",
      "   â­ F1 Score: 0.2673\n",
      "\n",
      "ğŸŒ€ Epoch 43/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 2.4059\n",
      "  ğŸ” Batch 2/90  |  Loss: 2.0437\n",
      "  ğŸ” Batch 3/90  |  Loss: 2.3560\n",
      "  ğŸ” Batch 4/90  |  Loss: 2.5137\n",
      "  ğŸ” Batch 5/90  |  Loss: 2.7042\n",
      "  ğŸ” Batch 6/90  |  Loss: 2.4150\n",
      "  ğŸ” Batch 7/90  |  Loss: 2.2792\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_recognition\n\u001b[0;32m      3\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/st_gcn/mediapipe-asl.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43minit_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ğŸ“Š Print metrics\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m processor\u001b[38;5;241m.\u001b[39mtraining_metrics:\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\main.py:50\u001b[0m, in \u001b[0;36minit_recognition\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m p \u001b[38;5;241m=\u001b[39m tool_module\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mparse_args([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m'\u001b[39m, config_path])\n\u001b[0;32m     49\u001b[0m processor \u001b[38;5;241m=\u001b[39m recog\u001b[38;5;241m.\u001b[39mProcessor(p)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processor\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\tools\\recognition.py:77\u001b[0m, in \u001b[0;36mProcessor.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     75\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\model\\st_gcn.py:78\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# x shape: [N, C, T, V]\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m gcn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst_gcn_networks:\n\u001b[1;32m---> 78\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)  \u001b[38;5;66;03m# [N, 256, 1, 1]\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\model\\st_gcn.py:43\u001b[0m, in \u001b[0;36mSTGCNBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Padding before temporal conv\u001b[39;00m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcn_padding[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcn_padding[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# pad time dimension\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtcn(x)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Handle shape mismatch just in case\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from main import init_recognition\n",
    "\n",
    "config_path = 'config/st_gcn/mediapipe-asl.yaml'\n",
    "processor = init_recognition(config_path)\n",
    "\n",
    "# ğŸ“Š Print metrics\n",
    "for entry in processor.training_metrics:\n",
    "    print(f\"ğŸ“Š Epoch {entry['epoch']}: Acc={entry['accuracy']:.4f} | Recall={entry['recall']:.4f} | F1={entry['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480a120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03cb2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f80e9510",
   "metadata": {},
   "source": [
    "### ST-GCN-SL training (Aafter adjusting the configuration**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92cb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Number of unique classes: 90\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to your training label file\n",
    "label_path = './data/mediapipe_asl/train_label.pkl'\n",
    "\n",
    "# Load the labels\n",
    "with open(label_path, 'rb') as f:\n",
    "    sample_names, labels = pickle.load(f)\n",
    "\n",
    "# Check number of unique classes\n",
    "unique_classes = len(set(labels))\n",
    "print(f\"âœ… Number of unique classes: {unique_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dac67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor start\n",
      "Loading data...\n",
      "Loading model...\n",
      "Starting training for 80 epochs\n",
      "\n",
      "ğŸŒ€ Epoch 1/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.6055\n",
      "  ğŸ” Batch 2/90  |  Loss: 5.0111\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.5954\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.5447\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.7115\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.8965\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.7383\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.5696\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.4575\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.3408\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.5501\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.6809\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.8151\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.5479\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.5336\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.5558\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.7105\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.7532\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.6908\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.6580\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.8142\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.8380\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.5386\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.7402\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.5855\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.6775\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.4151\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.7843\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.5547\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.6355\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.6745\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.6970\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.6021\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.6174\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.6748\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.5147\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.3686\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.5866\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.4742\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.6237\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.7031\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.5899\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.5559\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.5423\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.6855\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.5796\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.4638\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.7086\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.4751\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.6151\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.4983\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.5694\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.5202\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.6037\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.5743\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.4091\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.4744\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.5025\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.5743\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.5558\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.6216\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.5339\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.6410\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.5662\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.6669\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.5576\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.5437\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.5516\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.5835\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.4280\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.5429\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.5592\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.4562\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.5757\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.5387\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.4257\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.5425\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.6434\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.5016\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.4903\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.6033\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.5153\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.5338\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.6252\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.5859\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.5131\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.5240\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.4994\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.5143\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.5256\n",
      "\n",
      "ğŸ“Š Epoch 1 Summary:\n",
      "   âœ… Accuracy: 0.0056\n",
      "   ğŸ” Recall:   0.0056\n",
      "   â­ F1 Score: 0.0007\n",
      "\n",
      "ğŸŒ€ Epoch 2/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.4578\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.4448\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.5267\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.4593\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.4690\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.4989\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.5534\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.4677\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.4744\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.4683\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.4647\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.4637\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.3936\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.5102\n",
      "  ğŸ” Batch 15/90  |  Loss: 4.5408\n",
      "  ğŸ” Batch 16/90  |  Loss: 4.4894\n",
      "  ğŸ” Batch 17/90  |  Loss: 4.5164\n",
      "  ğŸ” Batch 18/90  |  Loss: 4.5082\n",
      "  ğŸ” Batch 19/90  |  Loss: 4.4732\n",
      "  ğŸ” Batch 20/90  |  Loss: 4.4148\n",
      "  ğŸ” Batch 21/90  |  Loss: 4.4718\n",
      "  ğŸ” Batch 22/90  |  Loss: 4.4826\n",
      "  ğŸ” Batch 23/90  |  Loss: 4.5413\n",
      "  ğŸ” Batch 24/90  |  Loss: 4.5200\n",
      "  ğŸ” Batch 25/90  |  Loss: 4.5382\n",
      "  ğŸ” Batch 26/90  |  Loss: 4.4076\n",
      "  ğŸ” Batch 27/90  |  Loss: 4.5766\n",
      "  ğŸ” Batch 28/90  |  Loss: 4.3999\n",
      "  ğŸ” Batch 29/90  |  Loss: 4.4557\n",
      "  ğŸ” Batch 30/90  |  Loss: 4.4347\n",
      "  ğŸ” Batch 31/90  |  Loss: 4.4603\n",
      "  ğŸ” Batch 32/90  |  Loss: 4.5429\n",
      "  ğŸ” Batch 33/90  |  Loss: 4.5003\n",
      "  ğŸ” Batch 34/90  |  Loss: 4.4747\n",
      "  ğŸ” Batch 35/90  |  Loss: 4.4593\n",
      "  ğŸ” Batch 36/90  |  Loss: 4.5038\n",
      "  ğŸ” Batch 37/90  |  Loss: 4.4815\n",
      "  ğŸ” Batch 38/90  |  Loss: 4.4223\n",
      "  ğŸ” Batch 39/90  |  Loss: 4.5402\n",
      "  ğŸ” Batch 40/90  |  Loss: 4.5935\n",
      "  ğŸ” Batch 41/90  |  Loss: 4.5109\n",
      "  ğŸ” Batch 42/90  |  Loss: 4.4326\n",
      "  ğŸ” Batch 43/90  |  Loss: 4.5626\n",
      "  ğŸ” Batch 44/90  |  Loss: 4.5718\n",
      "  ğŸ” Batch 45/90  |  Loss: 4.4443\n",
      "  ğŸ” Batch 46/90  |  Loss: 4.5761\n",
      "  ğŸ” Batch 47/90  |  Loss: 4.3843\n",
      "  ğŸ” Batch 48/90  |  Loss: 4.4684\n",
      "  ğŸ” Batch 49/90  |  Loss: 4.5008\n",
      "  ğŸ” Batch 50/90  |  Loss: 4.4044\n",
      "  ğŸ” Batch 51/90  |  Loss: 4.4979\n",
      "  ğŸ” Batch 52/90  |  Loss: 4.4433\n",
      "  ğŸ” Batch 53/90  |  Loss: 4.5141\n",
      "  ğŸ” Batch 54/90  |  Loss: 4.6265\n",
      "  ğŸ” Batch 55/90  |  Loss: 4.5557\n",
      "  ğŸ” Batch 56/90  |  Loss: 4.5742\n",
      "  ğŸ” Batch 57/90  |  Loss: 4.3936\n",
      "  ğŸ” Batch 58/90  |  Loss: 4.6688\n",
      "  ğŸ” Batch 59/90  |  Loss: 4.5289\n",
      "  ğŸ” Batch 60/90  |  Loss: 4.3978\n",
      "  ğŸ” Batch 61/90  |  Loss: 4.5005\n",
      "  ğŸ” Batch 62/90  |  Loss: 4.6606\n",
      "  ğŸ” Batch 63/90  |  Loss: 4.4991\n",
      "  ğŸ” Batch 64/90  |  Loss: 4.5694\n",
      "  ğŸ” Batch 65/90  |  Loss: 4.4141\n",
      "  ğŸ” Batch 66/90  |  Loss: 4.5118\n",
      "  ğŸ” Batch 67/90  |  Loss: 4.5793\n",
      "  ğŸ” Batch 68/90  |  Loss: 4.4868\n",
      "  ğŸ” Batch 69/90  |  Loss: 4.3662\n",
      "  ğŸ” Batch 70/90  |  Loss: 4.4948\n",
      "  ğŸ” Batch 71/90  |  Loss: 4.3961\n",
      "  ğŸ” Batch 72/90  |  Loss: 4.5281\n",
      "  ğŸ” Batch 73/90  |  Loss: 4.5021\n",
      "  ğŸ” Batch 74/90  |  Loss: 4.5642\n",
      "  ğŸ” Batch 75/90  |  Loss: 4.5793\n",
      "  ğŸ” Batch 76/90  |  Loss: 4.4379\n",
      "  ğŸ” Batch 77/90  |  Loss: 4.5277\n",
      "  ğŸ” Batch 78/90  |  Loss: 4.5116\n",
      "  ğŸ” Batch 79/90  |  Loss: 4.4807\n",
      "  ğŸ” Batch 80/90  |  Loss: 4.4002\n",
      "  ğŸ” Batch 81/90  |  Loss: 4.4348\n",
      "  ğŸ” Batch 82/90  |  Loss: 4.5067\n",
      "  ğŸ” Batch 83/90  |  Loss: 4.5831\n",
      "  ğŸ” Batch 84/90  |  Loss: 4.4138\n",
      "  ğŸ” Batch 85/90  |  Loss: 4.5463\n",
      "  ğŸ” Batch 86/90  |  Loss: 4.5352\n",
      "  ğŸ” Batch 87/90  |  Loss: 4.5216\n",
      "  ğŸ” Batch 88/90  |  Loss: 4.3303\n",
      "  ğŸ” Batch 89/90  |  Loss: 4.5738\n",
      "  ğŸ” Batch 90/90  |  Loss: 4.5205\n",
      "\n",
      "ğŸ“Š Epoch 2 Summary:\n",
      "   âœ… Accuracy: 0.0167\n",
      "   ğŸ” Recall:   0.0167\n",
      "   â­ F1 Score: 0.0032\n",
      "\n",
      "ğŸŒ€ Epoch 3/80\n",
      "  ğŸ” Batch 1/90  |  Loss: 4.4932\n",
      "  ğŸ” Batch 2/90  |  Loss: 4.4452\n",
      "  ğŸ” Batch 3/90  |  Loss: 4.3962\n",
      "  ğŸ” Batch 4/90  |  Loss: 4.3923\n",
      "  ğŸ” Batch 5/90  |  Loss: 4.4868\n",
      "  ğŸ” Batch 6/90  |  Loss: 4.4704\n",
      "  ğŸ” Batch 7/90  |  Loss: 4.5036\n",
      "  ğŸ” Batch 8/90  |  Loss: 4.5572\n",
      "  ğŸ” Batch 9/90  |  Loss: 4.3629\n",
      "  ğŸ” Batch 10/90  |  Loss: 4.4578\n",
      "  ğŸ” Batch 11/90  |  Loss: 4.4470\n",
      "  ğŸ” Batch 12/90  |  Loss: 4.6505\n",
      "  ğŸ” Batch 13/90  |  Loss: 4.5192\n",
      "  ğŸ” Batch 14/90  |  Loss: 4.4803\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_recognition\n\u001b[0;32m      3\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/st_gcn/mediapipe-asl.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43minit_recognition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ğŸ“Š Print metrics\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m processor\u001b[38;5;241m.\u001b[39mtraining_metrics:\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\main.py:50\u001b[0m, in \u001b[0;36minit_recognition\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m p \u001b[38;5;241m=\u001b[39m tool_module\u001b[38;5;241m.\u001b[39mget_parser()\u001b[38;5;241m.\u001b[39mparse_args([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m'\u001b[39m, config_path])\n\u001b[0;32m     49\u001b[0m processor \u001b[38;5;241m=\u001b[39m recog\u001b[38;5;241m.\u001b[39mProcessor(p)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processor\n",
      "File \u001b[1;32m~\\Videos\\st-gcn\\tools\\recognition.py:95\u001b[0m, in \u001b[0;36mProcessor.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(output, label)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 95\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ğŸ” Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  |  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stgcn-env\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from main import init_recognition\n",
    "\n",
    "config_path = 'config/st_gcn/mediapipe-asl.yaml'\n",
    "processor = init_recognition(config_path)\n",
    "\n",
    "# ğŸ“Š Print metrics\n",
    "for entry in processor.training_metrics:\n",
    "    print(f\"ğŸ“Š Epoch {entry['epoch']}: Acc={entry['accuracy']:.4f} | Recall={entry['recall']:.4f} | F1={entry['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949ab3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d24084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stgcn-env)",
   "language": "python",
   "name": "stgcn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
