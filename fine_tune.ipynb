{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699ceeef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(0, 55, 207); padding: 30px; border-radius: 20px; box-shadow: 0 4px 15px rgba(105, 195, 255, 0.3); color:rgb(187, 201, 248); font-family: 'Times New Roman', serif;\">\n",
    "\n",
    "<h1 style=\"text-align: center; font-size: 38px; color: white; font-weight: bold;\">Fine Tuning St-GCN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a8e6e",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from feeder.feeder import Feeder\n",
    "from main import init_recognition\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3638112",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Create Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('config/st_gcn/mediapipe-asl.yaml', 'r', encoding='utf-8') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Create dataset\n",
    "dataset = Feeder(**cfg['train_feeder_args'])\n",
    "loader = DataLoader(dataset, batch_size=cfg['train_batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"üß™ Dataset contains {len(dataset)} samples\")\n",
    "print(f\"üì¶ Each batch contains {cfg['train_batch_size']} samples\")\n",
    "print(f\"üîÅ Total batches = {len(loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ac561",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Fine Tuning the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cad31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'config/st_gcn/mediapipe-asl.yaml'\n",
    "processor = init_recognition(config_path)\n",
    "\n",
    "# Save your fine-tuned model\n",
    "torch.save(processor.model.state_dict(), \"./weights/fine_tuned_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def96b9b",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Model Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Set paths ===\n",
    "STGCN_PATH = r\"YOUR_MAIN_DIRECTORY\"\n",
    "MODEL_PATH = \"./weights/fine_tuned_model.pt\"\n",
    "VAL_DATA_PATH = './data/mediapipe_asl/val_data.npy'\n",
    "VAL_LABEL_PATH = './data/mediapipe_asl/val_label.pkl' \n",
    "EXCEL_PATH = r\"LABELS.xlsx\"\n",
    "MAP_LABELS_PATH = \"./data/mediapipe_asl/label_mapping.pkl\"\n",
    "\n",
    "# === 2. Load class names and video_id to gloss mapping ===\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "video_id_to_gloss = {str(row['video_id']).zfill(5): row['gloss'] for _, row in df.iterrows()}\n",
    "CLASS_NAMES = sorted(df['gloss'].unique())\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f\"‚úÖ Loaded {NUM_CLASSES} classes.\")\n",
    "\n",
    "# === 3. Set device ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "\n",
    "# === 4. Import model ===\n",
    "sys.path.append(STGCN_PATH)\n",
    "from model.st_gcn import Model\n",
    "\n",
    "model = Model(\n",
    "    in_channels=6,\n",
    "    num_class=NUM_CLASSES,\n",
    "    num_point=21,\n",
    "    num_person=1,\n",
    "    graph=\"graph.mediapipe_asl.Graph\",\n",
    "    graph_args={\"layout\": \"mediapipe_asl\", \"strategy\": \"spatial\"}\n",
    ")\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded successfully.\")\n",
    "\n",
    "# === 5. Load validation data and labels ===\n",
    "val_data = np.load(VAL_DATA_PATH)\n",
    "\n",
    "with open(VAL_LABEL_PATH, 'rb') as f:\n",
    "    val_label_data = pickle.load(f)\n",
    "\n",
    "if isinstance(val_label_data, tuple):\n",
    "    label_list = val_label_data[0]\n",
    "else:\n",
    "    label_list = val_label_data\n",
    "\n",
    "assert len(val_data) == len(label_list), \"Mismatch between val_data and label_list lengths\"\n",
    "print(f\"‚úÖ Loaded {len(label_list)} validation samples.\")\n",
    "\n",
    "# === 6. Predict all samples ===\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"Predicting...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(val_data)):\n",
    "        data_sample = val_data[idx]\n",
    "        data_sample = np.squeeze(data_sample, axis=-1)  # (3, T, V)\n",
    "\n",
    "        # Create velocity\n",
    "        if data_sample.shape[1] > 1:\n",
    "            velocity = data_sample[:, 1:, :] - data_sample[:, :-1, :]\n",
    "            position = data_sample[:, :-1, :]\n",
    "            data_sample = np.concatenate((position, velocity), axis=0)\n",
    "        else:\n",
    "            velocity = np.zeros_like(data_sample)\n",
    "            data_sample = np.concatenate((data_sample, velocity), axis=0)\n",
    "\n",
    "        data_sample = torch.tensor(data_sample, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        filename = label_list[idx]             # e.g., '69206.npy'\n",
    "        video_id = filename.replace('.npy', '')  # '69206'\n",
    "        true_gloss = video_id_to_gloss[video_id]  # lookup correct gloss\n",
    "        true_index = CLASS_NAMES.index(true_gloss)  # map gloss to index\n",
    "\n",
    "        output = model(data_sample)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "        y_true.append(true_index)\n",
    "        y_pred.append(predicted.item())\n",
    "\n",
    "print(\"‚úÖ Prediction complete.\")\n",
    "\n",
    "# === 7. Plot confusion matrix ===\n",
    "print(\"Plotting confusion matrix...\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 18))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "disp.plot(ax=ax, cmap='Blues', xticks_rotation=90)\n",
    "plt.title('Confusion Matrix with Class Names')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"‚úÖ Confusion matrix plotted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Calculate metrics ===\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='weighted')\n",
    "rec = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# === Print metrics ===\n",
    "print(f\"‚úÖ Test Accuracy : {acc*100:.2f}%\")\n",
    "print(f\"‚úÖ Test Precision: {prec*100:.2f}%\")\n",
    "print(f\"‚úÖ Test Recall   : {rec*100:.2f}%\")\n",
    "print(f\"‚úÖ Test F1 Score : {f1*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
