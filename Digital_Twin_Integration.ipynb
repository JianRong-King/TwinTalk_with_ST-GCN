{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6cfeb2a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(0, 55, 207); padding: 30px; border-radius: 20px; box-shadow: 0 4px 15px rgba(105, 195, 255, 0.3); color:rgb(187, 201, 248); font-family: 'Times New Roman', serif;\">\n",
    "\n",
    "<h1 style=\"text-align: center; font-size: 38px; color: white; font-weight: bold;\">Digital Twin Integration</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e2348",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f5dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa3a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingj\\anaconda3\\envs\\ST-GCN\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import requests\n",
    "import time\n",
    "from model.st_gcn import Model\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93af5d",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Configuration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a39faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS = './Best_fine_tuned_model/st-gcn.pt' # or the model you have trained\n",
    "NUM_CLASSES = 90    # Adjust to your number of classes\n",
    "# Adjust to your class names\n",
    "CLASS_NAMES = [\n",
    "    'all', 'almost', 'approve', 'before', 'boss', 'break', 'business', 'busy', 'but', 'buy', 'can', \n",
    "    'change', 'clock', 'computer', 'deaf', 'decide', 'delay', 'different', 'discuss', 'drink', \n",
    "    'eat', 'email', 'evaluate', 'explain', 'family', 'fine', 'finish', 'forget', 'full', 'give', \n",
    "    'goal', 'have', 'hearing', 'help', 'how', 'idea', 'improve', 'inform', 'last', 'later', 'leader', \n",
    "    'like', 'manager', 'many', 'meet', 'meeting', 'money', 'month', 'need', 'no', 'now', 'office', \n",
    "    'paper', 'plan', 'policy', 'presentation', 'problem', 'professional', 'provide', 'responsibility', \n",
    "    'result', 'role', 'same', 'schedule', 'secretary', 'sell', 'show', 'sorry', 'study', 'support', \n",
    "    'table', 'take', 'team', 'time', 'trade', 'understand', 'vacation', 'visit', 'wait', 'want', \n",
    "    'week', 'what', 'who', 'why', 'with', 'work', 'workshop', 'year', 'yes', 'yesterday'\n",
    "]\n",
    "\n",
    "# MediaPipe setup for hand landmarks\n",
    "mp_hands = mp.solutions.hands\n",
    "hand_detector = mp_hands.Hands(static_image_mode=False, \n",
    "                               max_num_hands=2, \n",
    "                               model_complexity=1, \n",
    "                               min_detection_confidence=0.5, \n",
    "                               min_tracking_confidence=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3334dd4",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Model-GPU</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e48ee06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (st_gcn_networks): ModuleList(\n",
       "    (0): STGCNBlock(\n",
       "      (gcn): Conv2d(6, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (tcn): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1))\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1-2): 2 x STGCNBlock(\n",
       "      (gcn): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (tcn): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1))\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (residual): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): STGCNBlock(\n",
       "      (gcn): Conv2d(64, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (tcn): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(9, 1), stride=(2, 1))\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): STGCNBlock(\n",
       "      (gcn): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (tcn): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(9, 1), stride=(1, 1))\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (residual): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): STGCNBlock(\n",
       "      (gcn): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (tcn): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(9, 1), stride=(2, 1))\n",
       "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): STGCNBlock(\n",
       "      (gcn): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (tcn): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(9, 1), stride=(1, 1))\n",
       "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (residual): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=256, out_features=90, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model\n",
    "model = Model(in_channels=6, num_class=NUM_CLASSES, num_point=21, num_person=1, \n",
    "              graph=\"graph.mediapipe_asl.Graph\", graph_args={\"layout\":\"mediapipe_asl\", \"strategy\":\"spatial\"})\n",
    "\n",
    "# Move model to GPU\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb028ea",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Open Web Cam</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b992e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_sign_from_camera():\n",
    "    \"\"\"Capture a sign sequence from the webcam. Returns a list of frames (BGR images).\"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # open default camera \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot access camera.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Press 's' to start recording the sign, 'e' to end recording, or 'q' to quit.\")\n",
    "    recorded_frames = []\n",
    "    recording = False\n",
    "    start_time = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # camera read error\n",
    "        # Optionally, flip the frame horizontally for a mirror-view (if needed)\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # If currently recording, save frames to list\n",
    "        if recording:\n",
    "            recorded_frames.append(frame.copy())\n",
    "            # Draw a recording indicator on frame\n",
    "            cv2.circle(frame, (30, 30), 10, (0,0,255), -1)  # red dot\n",
    "            cv2.putText(frame, \"Recording...\", (50, 35), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.8, (0,0,255), 2)\n",
    "            # Auto-stop recording after a certain time to avoid infinite recording \n",
    "            if time.time() - start_time > 5:  # 5 seconds max for a sign \n",
    "                recording = False\n",
    "                print(\"Auto-stopped recording after 5 seconds.\")\n",
    "                # break out to process the recorded frames\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            # Not recording yet: overlay instructions\n",
    "            cv2.putText(frame, \"Press 's' to start recording a sign\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "            cv2.putText(frame, \"Press 'q' to quit\", (10,60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "        \n",
    "        # Show the camera feed\n",
    "        cv2.imshow(\"Camera\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF  # read keyboard input\n",
    "        if key == ord('q'):\n",
    "            # Quit entire program\n",
    "            cap.release()\n",
    "            cv2.destroyWindow(\"Camera\")\n",
    "            return None\n",
    "        if key == ord('s') and not recording:\n",
    "            # Start recording\n",
    "            print(\"Recording started. Perform the sign and press 'e' when done.\")\n",
    "            recording = True\n",
    "            start_time = time.time()\n",
    "        if key == ord('e') and recording:\n",
    "            # End recording\n",
    "            recording = False\n",
    "            print(\"Recording stopped.\")\n",
    "            break\n",
    "    \n",
    "    # Cleanup camera resources\n",
    "    cap.release()\n",
    "    cv2.destroyWindow(\"Camera\")\n",
    "    return recorded_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec5c67",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Translation (sign to text)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f69166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_sign(frames):\n",
    "    \"\"\"Given a list of frames (BGR images) for a sign, return the predicted text.\"\"\"\n",
    "    if not frames:\n",
    "        return None\n",
    "    # Extract hand keypoints from each frame using MediaPipe\n",
    "    all_landmarks = []  # will be list of 21 (x,y,z) for each frame\n",
    "    for frame in frames:\n",
    "        # Convert BGR frame to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hand_detector.process(rgb_frame)\n",
    "        if results.multi_hand_landmarks:\n",
    "            # If a hand is detected, take the first hand's landmarks\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            # Normalize or scale coordinates:\n",
    "            # MediaPipe provides x, y, z normalized relative to the image and metric depth.\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append([lm.x, lm.y, lm.z])\n",
    "            # landmarks is a list of 21 [x,y,z]\n",
    "        else:\n",
    "            # If no hand detected in this frame, use zeros (or could skip frame)\n",
    "            landmarks = [[0.0, 0.0, 0.0]] * 21\n",
    "        all_landmarks.append(landmarks)\n",
    "    # Convert to NumPy array for model input\n",
    "    all_landmarks = np.array(all_landmarks)  # shape (T, 21, 3)\n",
    "    T = all_landmarks.shape[0]\n",
    "    # Rearrange to shape (3, T, 21)\n",
    "    all_landmarks = np.transpose(all_landmarks, (2, 0, 1))  # (3, T, 21)\n",
    "    # Compute velocity (difference between consecutive frames)\n",
    "    if T > 1:\n",
    "        velocity = all_landmarks[:, 1:, :] - all_landmarks[:, :-1, :]\n",
    "    else:\n",
    "        # If only 1 frame, velocity can be zeros\n",
    "        velocity = np.zeros_like(all_landmarks)\n",
    "        velocity = velocity[:, :-1, :]  # will be shape (3, 0, 21), effectively no time steps\n",
    "    # Concatenate original coords (excluding last frame to match velocity length) with velocity\n",
    "    if velocity.shape[1] > 0:\n",
    "        data = np.concatenate((all_landmarks[:, :-1, :], velocity), axis=0)  # shape (6, T-1, 21)\n",
    "    else:\n",
    "        # In case of single-frame (which is rare for a sign), handle separately\n",
    "        data = np.concatenate((all_landmarks, np.zeros_like(all_landmarks)), axis=0)  # (6, T, 21) with velocity zero\n",
    "    # Add batch dimension and person dimension if needed\n",
    "    data = np.expand_dims(data, axis=0)  # shape (1, 6, T-1, 21)\n",
    "    # Convert to torch tensor\n",
    "    data_tensor = torch.from_numpy(data).float().to(device)\n",
    "\n",
    "    data_tensor = data_tensor.to(device)\n",
    "    # model is already on appropriate device\n",
    "    with torch.no_grad():\n",
    "        output = model(data_tensor)  # shape (1, num_class)\n",
    "        _, pred_class = torch.max(output, dim=1)  # predicted class index\n",
    "        pred_class = int(pred_class.item())\n",
    "    # Map class index to text\n",
    "    if pred_class < len(CLASS_NAMES):\n",
    "        predicted_text = CLASS_NAMES[pred_class]\n",
    "    else:\n",
    "        # If for some reason class is out of range, just return the index as string\n",
    "        predicted_text = str(pred_class)\n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e5f90",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Words to Sentence (GenAI)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465a5750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kingj\\anaconda3\\envs\\ST-GCN\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load text generation pipeline\n",
    "nlp = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "def generate_sentence_better(accepted_words):\n",
    "    input_prompt = f\"Create a meaningful English sentence using the following words: {', '.join(accepted_words)}.\"\n",
    "    \n",
    "    output = nlp(input_prompt, max_length=50, do_sample=False)[0]['generated_text']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436be7e",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Avatar API call</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e07401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def generate_avatar_video(text):\n",
    "#     \"\"\"Call the Digital Twin API to generate the avatar video. Returns the video URL.\"\"\"\n",
    "#     if text is None:\n",
    "#         return None\n",
    "#     print(f\"Sending text to Digital Twin API: '{text}'\")\n",
    "#     try:\n",
    "#         # Call local Node API\n",
    "#         response = requests.post(\"h\n",
    "# ttp://localhost:3000/generate\", json={\"text\": text}, timeout=60)\n",
    "#         response.raise_for_status()\n",
    "#     except requests.RequestException as e:\n",
    "#         print(\"Error communicating with Digital Twin API:\", e)\n",
    "#         return None\n",
    "#     data = response.json()\n",
    "#     video_url = data.get(\"videoUrl\") or data.get(\"url\")\n",
    "#     if video_url:\n",
    "#         print(\"Received video URL:\", video_url)\n",
    "#     else:\n",
    "#         print(\"No video URL received. Response:\", data)\n",
    "#     return video_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110a00c",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Show results</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a4b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play_video_from_url(video_url):\n",
    "#     if not video_url:\n",
    "#         return\n",
    "\n",
    "#     # === Download and save the video ===\n",
    "#     try:\n",
    "#         print(\"Downloading avatar video...\")\n",
    "#         video_data = requests.get(video_url, timeout=60).content\n",
    "#         video_path = \"avatar_output.mp4\"\n",
    "#         with open(video_path, \"wb\") as f:\n",
    "#             f.write(video_data)\n",
    "#         print(f\"Video saved as {video_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(\"Failed to download video:\", e)\n",
    "#         return\n",
    "\n",
    "#     while True:\n",
    "#         # === Play the saved video with VLC ===\n",
    "#         print(\"Opening avatar video with VLC...\")\n",
    "#         os.system(f'start vlc --play-and-exit {video_path}')\n",
    "\n",
    "#         # === Ask user if they want to replay ===\n",
    "#         print(\"Press 'r' to replay the avatar, any other key to continue...\")\n",
    "#         key = input().strip().lower()\n",
    "#         if key == 'r':\n",
    "#             continue  # Replay again\n",
    "#         else:\n",
    "#             break  # Exit playing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f63fc",
   "metadata": {},
   "source": [
    "<h3 style=\"font-size: 22px; color: white; font-weight: bold;\">Main Code</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756149cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sign Language Translation with Digital Twin Integration...\n",
      "Press 's' to start recording the sign, 'e' to end recording, or 'q' to quit.\n",
      "Recording started. Perform the sign and press 'e' when done.\n",
      "Auto-stopped recording after 5 seconds.\n",
      "Predicted sign text: help\n"
     ]
    }
   ],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Sign Language Translation with Digital Twin Integration...\")\n",
    "    while True:\n",
    "        accepted_words = []\n",
    "        while True:\n",
    "            frames = capture_sign_from_camera()\n",
    "            if frames is None or len(frames) == 0:\n",
    "                print(\"No frames captured or quit requested.\")\n",
    "                break\n",
    "            \n",
    "            text = predict_sign(frames)\n",
    "\n",
    "        \n",
    "            \n",
    "            print(f\"Predicted sign text: {text}\")   # 🔥 PRINT IMMEDIATELY\n",
    "            decision = input(\"Accept word? [y = yes, n = no, f = finish]: \").strip().lower()\n",
    "\n",
    "            if decision == 'y':\n",
    "                accepted_words.append(text)\n",
    "                print(f\"✅ Added '{text}'.\")\n",
    "            elif decision == 'n':\n",
    "                print(\"🔁 Retry recording.\")\n",
    "                continue\n",
    "            elif decision == 'f':\n",
    "                finish_decision = input(f\"Do you want to add '{text}' before finishing? [y/n]: \").strip().lower()\n",
    "                if finish_decision == 'y':\n",
    "                    accepted_words.append(text)\n",
    "                    print(f\"✅ Added '{text}' before finishing.\")\n",
    "                else:\n",
    "                    print(\"⏩ Word skipped before finishing.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"❓ Invalid input. Press 'y', 'n', or 'f'.\")\n",
    "                continue\n",
    "        if not accepted_words:\n",
    "            print(\"Prediction failed.\")\n",
    "            continue\n",
    "\n",
    "        generated_sentence = generate_sentence_better(accepted_words)  # using better sentence generator\n",
    "        print(f\"📝 Generated sentence: {generated_sentence}\")\n",
    "\n",
    "        # video_url = generate_avatar_video(text)\n",
    "        # if video_url:\n",
    "        #     play_video_from_url(video_url)\n",
    "        # another = input(\"Start another translation? [y/n]: \").strip().lower()\n",
    "        # if another != 'y':\n",
    "        #     print(\"👋 Exiting. Goodbye!\")\n",
    "        #     break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST-GCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
